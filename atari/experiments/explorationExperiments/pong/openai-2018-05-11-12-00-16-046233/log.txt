Logging to /tmp/openai-2018-05-11-12-00-16-046233
---------------------------------
| avg reward         | -0.41    |
| epsilonValue       | 100      |
| explained_variance | -564     |
| fps                | 47       |
| learning rate      | 0.001    |
| nupdates           | 1        |
| policy_entropy     | 1.79     |
| policy_loss        | -0.076   |
| total_timesteps    | 80       |
| value_loss         | 0.0184   |
---------------------------------
---------------------------------
| avg reward         | -1.02    |
| epsilonValue       | 98.5     |
| explained_variance | -0.22    |
| fps                | 1481     |
| learning rate      | 0.001    |
| nupdates           | 100      |
| policy_entropy     | 1.79     |
| policy_loss        | 0.0529   |
| total_timesteps    | 8000     |
| value_loss         | 0.000759 |
---------------------------------
---------------------------------
| avg reward         | -1.48    |
| epsilonValue       | 97.1     |
| explained_variance | -0.0133  |
| fps                | 1660     |
| learning rate      | 0.001    |
| nupdates           | 200      |
| policy_entropy     | 1.79     |
| policy_loss        | -0.0799  |
| total_timesteps    | 16000    |
| value_loss         | 0.0405   |
---------------------------------
---------------------------------
| avg reward         | -1.99    |
| epsilonValue       | 95.6     |
| explained_variance | -0.0183  |
| fps                | 1748     |
| learning rate      | 0.001    |
| nupdates           | 300      |
| policy_entropy     | 1.79     |
| policy_loss        | 0.0343   |
| total_timesteps    | 24000    |
| value_loss         | 0.0186   |
---------------------------------
---------------------------------
| avg reward         | -1.36    |
| epsilonValue       | 94.2     |
| explained_variance | 0.0107   |
| fps                | 1806     |
| learning rate      | 0.001    |
| nupdates           | 400      |
| policy_entropy     | 1.79     |
| policy_loss        | 0.00746  |
| total_timesteps    | 32000    |
| value_loss         | 0.0169   |
---------------------------------
---------------------------------
| avg reward         | -2.47    |
| epsilonValue       | 92.7     |
| explained_variance | 0.0287   |
| fps                | 1863     |
| learning rate      | 0.001    |
| nupdates           | 500      |
| policy_entropy     | 1.78     |
| policy_loss        | -0.143   |
| total_timesteps    | 40000    |
| value_loss         | 0.0678   |
---------------------------------
---------------------------------
| avg reward         | -1.82    |
| epsilonValue       | 91.3     |
| explained_variance | -0.0264  |
| fps                | 1879     |
| learning rate      | 0.001    |
| nupdates           | 600      |
| policy_entropy     | 1.77     |
| policy_loss        | -0.0471  |
| total_timesteps    | 48000    |
| value_loss         | 0.0333   |
---------------------------------
---------------------------------
| avg reward         | -1.67    |
| epsilonValue       | 89.8     |
| explained_variance | 0.0199   |
| fps                | 1900     |
| learning rate      | 0.001    |
| nupdates           | 700      |
| policy_entropy     | 1.78     |
| policy_loss        | -0.295   |
| total_timesteps    | 56000    |
| value_loss         | 0.0932   |
---------------------------------
---------------------------------
| avg reward         | -2.74    |
| epsilonValue       | 88.4     |
| explained_variance | -0.00729 |
| fps                | 1909     |
| learning rate      | 0.001    |
| nupdates           | 800      |
| policy_entropy     | 1.76     |
| policy_loss        | -0.0233  |
| total_timesteps    | 64000    |
| value_loss         | 0.0439   |
---------------------------------
---------------------------------
| avg reward         | -1.33    |
| epsilonValue       | 86.9     |
| explained_variance | 0.0281   |
| fps                | 1929     |
| learning rate      | 0.001    |
| nupdates           | 900      |
| policy_entropy     | 1.79     |
| policy_loss        | -0.225   |
| total_timesteps    | 72000    |
| value_loss         | 0.072    |
---------------------------------
---------------------------------
| avg reward         | -1.25    |
| epsilonValue       | 85.5     |
| explained_variance | 0.0112   |
| fps                | 1948     |
| learning rate      | 0.001    |
| nupdates           | 1000     |
| policy_entropy     | 1.79     |
| policy_loss        | -0.153   |
| total_timesteps    | 80000    |
| value_loss         | 0.0556   |
---------------------------------
----------------------------------
| avg reward         | -1.54     |
| epsilonValue       | 84        |
| explained_variance | -0.000319 |
| fps                | 1948      |
| learning rate      | 0.001     |
| nupdates           | 1100      |
| policy_entropy     | 1.77      |
| policy_loss        | -0.0603   |
| total_timesteps    | 88000     |
| value_loss         | 0.0339    |
----------------------------------
---------------------------------
| avg reward         | -2.18    |
| epsilonValue       | 82.5     |
| explained_variance | 0.00503  |
| fps                | 1957     |
| learning rate      | 0.001    |
| nupdates           | 1200     |
| policy_entropy     | 1.76     |
| policy_loss        | 0.122    |
| total_timesteps    | 96000    |
| value_loss         | 0.00293  |
---------------------------------
---------------------------------
| avg reward         | -2.24    |
| epsilonValue       | 81.1     |
| explained_variance | 0.0788   |
| fps                | 1970     |
| learning rate      | 0.001    |
| nupdates           | 1300     |
| policy_entropy     | 1.76     |
| policy_loss        | -0.0986  |
| total_timesteps    | 104000   |
| value_loss         | 0.0504   |
---------------------------------
---------------------------------
| avg reward         | -1.38    |
| epsilonValue       | 79.6     |
| explained_variance | 0.0685   |
| fps                | 1975     |
| learning rate      | 0.001    |
| nupdates           | 1400     |
| policy_entropy     | 1.79     |
| policy_loss        | -0.192   |
| total_timesteps    | 112000   |
| value_loss         | 0.063    |
---------------------------------
---------------------------------
| avg reward         | -1.75    |
| epsilonValue       | 78.2     |
| explained_variance | 0.134    |
| fps                | 1980     |
| learning rate      | 0.001    |
| nupdates           | 1500     |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0924   |
| total_timesteps    | 120000   |
| value_loss         | 0.00182  |
---------------------------------
---------------------------------
| avg reward         | -2.26    |
| epsilonValue       | 76.7     |
| explained_variance | 0.107    |
| fps                | 1991     |
| learning rate      | 0.001    |
| nupdates           | 1600     |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0177   |
| total_timesteps    | 128000   |
| value_loss         | 0.0213   |
---------------------------------
---------------------------------
| avg reward         | -1.17    |
| epsilonValue       | 75.3     |
| explained_variance | 0.141    |
| fps                | 1993     |
| learning rate      | 0.001    |
| nupdates           | 1700     |
| policy_entropy     | 1.78     |
| policy_loss        | 0.00409  |
| total_timesteps    | 136000   |
| value_loss         | 0.0159   |
---------------------------------
---------------------------------
| avg reward         | -2.25    |
| epsilonValue       | 73.8     |
| explained_variance | 0.172    |
| fps                | 2000     |
| learning rate      | 0.001    |
| nupdates           | 1800     |
| policy_entropy     | 1.77     |
| policy_loss        | 0.0392   |
| total_timesteps    | 144000   |
| value_loss         | 0.0407   |
---------------------------------
---------------------------------
| avg reward         | -1.38    |
| epsilonValue       | 72.4     |
| explained_variance | 0.278    |
| fps                | 2006     |
| learning rate      | 0.001    |
| nupdates           | 1900     |
| policy_entropy     | 1.77     |
| policy_loss        | 0.0189   |
| total_timesteps    | 152000   |
| value_loss         | 0.0166   |
---------------------------------
---------------------------------
| avg reward         | -2.15    |
| epsilonValue       | 70.9     |
| explained_variance | 0.345    |
| fps                | 2009     |
| learning rate      | 0.001    |
| nupdates           | 2000     |
| policy_entropy     | 1.77     |
| policy_loss        | -0.0567  |
| total_timesteps    | 160000   |
| value_loss         | 0.0401   |
---------------------------------
---------------------------------
| avg reward         | -1.96    |
| epsilonValue       | 69.5     |
| explained_variance | 0.542    |
| fps                | 2010     |
| learning rate      | 0.001    |
| nupdates           | 2100     |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0337   |
| total_timesteps    | 168000   |
| value_loss         | 0.0153   |
---------------------------------
---------------------------------
| avg reward         | -1.32    |
| epsilonValue       | 68       |
| explained_variance | 0.0335   |
| fps                | 2013     |
| learning rate      | 0.001    |
| nupdates           | 2200     |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0849   |
| total_timesteps    | 176000   |
| value_loss         | 0.0143   |
---------------------------------
---------------------------------
| avg reward         | -0.939   |
| epsilonValue       | 66.5     |
| explained_variance | -0.312   |
| fps                | 2016     |
| learning rate      | 0.001    |
| nupdates           | 2300     |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0224   |
| total_timesteps    | 184000   |
| value_loss         | 0.00504  |
---------------------------------
---------------------------------
| avg reward         | -2.25    |
| epsilonValue       | 65.1     |
| explained_variance | 0.481    |
| fps                | 2015     |
| learning rate      | 0.001    |
| nupdates           | 2400     |
| policy_entropy     | 1.77     |
| policy_loss        | 0.0915   |
| total_timesteps    | 192000   |
| value_loss         | 0.0186   |
---------------------------------
---------------------------------
| avg reward         | -2.01    |
| epsilonValue       | 63.6     |
| explained_variance | 0.625    |
| fps                | 2017     |
| learning rate      | 0.001    |
| nupdates           | 2500     |
| policy_entropy     | 1.77     |
| policy_loss        | -0.0444  |
| total_timesteps    | 200000   |
| value_loss         | 0.0279   |
---------------------------------
---------------------------------
| avg reward         | -2.42    |
| epsilonValue       | 62.2     |
| explained_variance | 0.266    |
| fps                | 2020     |
| learning rate      | 0.001    |
| nupdates           | 2600     |
| policy_entropy     | 1.74     |
| policy_loss        | -0.113   |
| total_timesteps    | 208000   |
| value_loss         | 0.0492   |
---------------------------------
---------------------------------
| avg reward         | -1.62    |
| epsilonValue       | 60.7     |
| explained_variance | 0.807    |
| fps                | 2022     |
| learning rate      | 0.001    |
| nupdates           | 2700     |
| policy_entropy     | 1.77     |
| policy_loss        | 0.0399   |
| total_timesteps    | 216000   |
| value_loss         | 0.0075   |
---------------------------------
---------------------------------
| avg reward         | -1.67    |
| epsilonValue       | 59.3     |
| explained_variance | 0.532    |
| fps                | 2022     |
| learning rate      | 0.001    |
| nupdates           | 2800     |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0367   |
| total_timesteps    | 224000   |
| value_loss         | 0.0163   |
---------------------------------
---------------------------------
| avg reward         | -1.94    |
| epsilonValue       | 57.8     |
| explained_variance | 0.385    |
| fps                | 2019     |
| learning rate      | 0.001    |
| nupdates           | 2900     |
| policy_entropy     | 1.77     |
| policy_loss        | 0.0989   |
| total_timesteps    | 232000   |
| value_loss         | 0.0337   |
---------------------------------
---------------------------------
| avg reward         | -1.93    |
| epsilonValue       | 56.4     |
| explained_variance | 0.834    |
| fps                | 2022     |
| learning rate      | 0.001    |
| nupdates           | 3000     |
| policy_entropy     | 1.77     |
| policy_loss        | 0.00639  |
| total_timesteps    | 240000   |
| value_loss         | 0.0039   |
---------------------------------
---------------------------------
| avg reward         | -1.89    |
| epsilonValue       | 54.9     |
| explained_variance | 0.768    |
| fps                | 2025     |
| learning rate      | 0.001    |
| nupdates           | 3100     |
| policy_entropy     | 1.77     |
| policy_loss        | -0.131   |
| total_timesteps    | 248000   |
| value_loss         | 0.0153   |
---------------------------------
---------------------------------
| avg reward         | -2.01    |
| epsilonValue       | 53.5     |
| explained_variance | 0.842    |
| fps                | 2026     |
| learning rate      | 0.001    |
| nupdates           | 3200     |
| policy_entropy     | 1.77     |
| policy_loss        | -0.014   |
| total_timesteps    | 256000   |
| value_loss         | 0.00911  |
---------------------------------
---------------------------------
| avg reward         | -2.59    |
| epsilonValue       | 52       |
| explained_variance | 0.496    |
| fps                | 2026     |
| learning rate      | 0.001    |
| nupdates           | 3300     |
| policy_entropy     | 1.77     |
| policy_loss        | 0.0168   |
| total_timesteps    | 264000   |
| value_loss         | 0.0103   |
---------------------------------
---------------------------------
| avg reward         | -1.49    |
| epsilonValue       | 50.5     |
| explained_variance | 0.666    |
| fps                | 2026     |
| learning rate      | 0.001    |
| nupdates           | 3400     |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0812  |
| total_timesteps    | 272000   |
| value_loss         | 0.0259   |
---------------------------------
---------------------------------
| avg reward         | -2.22    |
| epsilonValue       | 49.1     |
| explained_variance | 0.913    |
| fps                | 2027     |
| learning rate      | 0.001    |
| nupdates           | 3500     |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0166  |
| total_timesteps    | 280000   |
| value_loss         | 0.00596  |
---------------------------------
---------------------------------
| avg reward         | -2.22    |
| epsilonValue       | 47.6     |
| explained_variance | 0.876    |
| fps                | 2028     |
| learning rate      | 0.001    |
| nupdates           | 3600     |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0411  |
| total_timesteps    | 288000   |
| value_loss         | 0.00959  |
---------------------------------
---------------------------------
| avg reward         | -1.69    |
| epsilonValue       | 46.2     |
| explained_variance | 0.686    |
| fps                | 2030     |
| learning rate      | 0.001    |
| nupdates           | 3700     |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0326  |
| total_timesteps    | 296000   |
| value_loss         | 0.0196   |
---------------------------------
---------------------------------
| avg reward         | -0.958   |
| epsilonValue       | 44.7     |
| explained_variance | 0.189    |
| fps                | 2028     |
| learning rate      | 0.001    |
| nupdates           | 3800     |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0134  |
| total_timesteps    | 304000   |
| value_loss         | 0.0325   |
---------------------------------
---------------------------------
| avg reward         | -1.82    |
| epsilonValue       | 43.3     |
| explained_variance | 0.745    |
| fps                | 2031     |
| learning rate      | 0.001    |
| nupdates           | 3900     |
| policy_entropy     | 1.77     |
| policy_loss        | -0.109   |
| total_timesteps    | 312000   |
| value_loss         | 0.017    |
---------------------------------
----------------------------------
| avg reward         | -1.52     |
| epsilonValue       | 41.8      |
| explained_variance | 0.821     |
| fps                | 2033      |
| learning rate      | 0.001     |
| nupdates           | 4000      |
| policy_entropy     | 1.77      |
| policy_loss        | -0.000803 |
| total_timesteps    | 320000    |
| value_loss         | 0.00636   |
----------------------------------
---------------------------------
| avg reward         | -1.68    |
| epsilonValue       | 40.4     |
| explained_variance | 0.873    |
| fps                | 2033     |
| learning rate      | 0.001    |
| nupdates           | 4100     |
| policy_entropy     | 1.77     |
| policy_loss        | -0.0558  |
| total_timesteps    | 328000   |
| value_loss         | 0.00845  |
---------------------------------
---------------------------------
| avg reward         | -1.9     |
| epsilonValue       | 38.9     |
| explained_variance | 0.825    |
| fps                | 2033     |
| learning rate      | 0.001    |
| nupdates           | 4200     |
| policy_entropy     | 1.76     |
| policy_loss        | -0.0843  |
| total_timesteps    | 336000   |
| value_loss         | 0.0142   |
---------------------------------
---------------------------------
| avg reward         | -2       |
| epsilonValue       | 37.5     |
| explained_variance | 0.961    |
| fps                | 2033     |
| learning rate      | 0.001    |
| nupdates           | 4300     |
| policy_entropy     | 1.75     |
| policy_loss        | 0.0479   |
| total_timesteps    | 344000   |
| value_loss         | 0.00334  |
---------------------------------
---------------------------------
| avg reward         | -2.11    |
| epsilonValue       | 36       |
| explained_variance | 0.789    |
| fps                | 2032     |
| learning rate      | 0.001    |
| nupdates           | 4400     |
| policy_entropy     | 1.76     |
| policy_loss        | 0.0992   |
| total_timesteps    | 352000   |
| value_loss         | 0.00949  |
---------------------------------
---------------------------------
| avg reward         | -1.94    |
| epsilonValue       | 34.5     |
| explained_variance | 0.543    |
| fps                | 2035     |
| learning rate      | 0.001    |
| nupdates           | 4500     |
| policy_entropy     | 1.75     |
| policy_loss        | 0.0174   |
| total_timesteps    | 360000   |
| value_loss         | 0.0218   |
---------------------------------
---------------------------------
| avg reward         | -1.99    |
| epsilonValue       | 33.1     |
| explained_variance | 0.91     |
| fps                | 2034     |
| learning rate      | 0.001    |
| nupdates           | 4600     |
| policy_entropy     | 1.76     |
| policy_loss        | -0.0938  |
| total_timesteps    | 368000   |
| value_loss         | 0.0105   |
---------------------------------
---------------------------------
| avg reward         | -1.47    |
| epsilonValue       | 31.6     |
| explained_variance | 0.71     |
| fps                | 2034     |
| learning rate      | 0.001    |
| nupdates           | 4700     |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0249   |
| total_timesteps    | 376000   |
| value_loss         | 0.00555  |
---------------------------------
---------------------------------
| avg reward         | -1.96    |
| epsilonValue       | 30.2     |
| explained_variance | 0.93     |
| fps                | 2035     |
| learning rate      | 0.001    |
| nupdates           | 4800     |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0181  |
| total_timesteps    | 384000   |
| value_loss         | 0.00695  |
---------------------------------
---------------------------------
| avg reward         | -2.21    |
| epsilonValue       | 28.7     |
| explained_variance | 0.728    |
| fps                | 2038     |
| learning rate      | 0.001    |
| nupdates           | 4900     |
| policy_entropy     | 1.77     |
| policy_loss        | -0.00621 |
| total_timesteps    | 392000   |
| value_loss         | 0.01     |
---------------------------------
---------------------------------
| avg reward         | -2.15    |
| epsilonValue       | 27.3     |
| explained_variance | 0.708    |
| fps                | 2038     |
| learning rate      | 0.001    |
| nupdates           | 5000     |
| policy_entropy     | 1.77     |
| policy_loss        | 0.131    |
| total_timesteps    | 400000   |
| value_loss         | 0.0105   |
---------------------------------
---------------------------------
| avg reward         | -1.15    |
| epsilonValue       | 25.8     |
| explained_variance | 0.935    |
| fps                | 2038     |
| learning rate      | 0.001    |
| nupdates           | 5100     |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0124  |
| total_timesteps    | 408000   |
| value_loss         | 0.0033   |
---------------------------------
---------------------------------
| avg reward         | -2.27    |
| epsilonValue       | 24.4     |
| explained_variance | 0.87     |
| fps                | 2038     |
| learning rate      | 0.001    |
| nupdates           | 5200     |
| policy_entropy     | 1.75     |
| policy_loss        | -0.0202  |
| total_timesteps    | 416000   |
| value_loss         | 0.00671  |
---------------------------------
---------------------------------
| avg reward         | -2.19    |
| epsilonValue       | 22.9     |
| explained_variance | 0.853    |
| fps                | 2038     |
| learning rate      | 0.001    |
| nupdates           | 5300     |
| policy_entropy     | 1.75     |
| policy_loss        | -0.0223  |
| total_timesteps    | 424000   |
| value_loss         | 0.00878  |
---------------------------------
---------------------------------
| avg reward         | -2.6     |
| epsilonValue       | 21.5     |
| explained_variance | 0.886    |
| fps                | 2040     |
| learning rate      | 0.001    |
| nupdates           | 5400     |
| policy_entropy     | 1.74     |
| policy_loss        | 0.0758   |
| total_timesteps    | 432000   |
| value_loss         | 0.00714  |
---------------------------------
---------------------------------
| avg reward         | -2.13    |
| epsilonValue       | 20       |
| explained_variance | 0.94     |
| fps                | 2041     |
| learning rate      | 0.001    |
| nupdates           | 5500     |
| policy_entropy     | 1.75     |
| policy_loss        | -0.00454 |
| total_timesteps    | 440000   |
| value_loss         | 0.00749  |
---------------------------------
---------------------------------
| avg reward         | -2.04    |
| epsilonValue       | 18.5     |
| explained_variance | 0.88     |
| fps                | 2042     |
| learning rate      | 0.001    |
| nupdates           | 5600     |
| policy_entropy     | 1.75     |
| policy_loss        | -0.0996  |
| total_timesteps    | 448000   |
| value_loss         | 0.00896  |
---------------------------------
---------------------------------
| avg reward         | -2.21    |
| epsilonValue       | 17.1     |
| explained_variance | 0.973    |
| fps                | 2043     |
| learning rate      | 0.001    |
| nupdates           | 5700     |
| policy_entropy     | 1.75     |
| policy_loss        | 0.00583  |
| total_timesteps    | 456000   |
| value_loss         | 0.00374  |
---------------------------------
---------------------------------
| avg reward         | -2.23    |
| epsilonValue       | 15.6     |
| explained_variance | 0.892    |
| fps                | 2044     |
| learning rate      | 0.001    |
| nupdates           | 5800     |
| policy_entropy     | 1.77     |
| policy_loss        | -0.0306  |
| total_timesteps    | 464000   |
| value_loss         | 0.0102   |
---------------------------------
---------------------------------
| avg reward         | -1.37    |
| epsilonValue       | 14.2     |
| explained_variance | 0.475    |
| fps                | 2044     |
| learning rate      | 0.001    |
| nupdates           | 5900     |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0365  |
| total_timesteps    | 472000   |
| value_loss         | 0.0148   |
---------------------------------
---------------------------------
| avg reward         | -2.21    |
| epsilonValue       | 12.7     |
| explained_variance | 0.81     |
| fps                | 2044     |
| learning rate      | 0.001    |
| nupdates           | 6000     |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0671   |
| total_timesteps    | 480000   |
| value_loss         | 0.0181   |
---------------------------------
---------------------------------
| avg reward         | -1.85    |
| epsilonValue       | 11.3     |
| explained_variance | 0.687    |
| fps                | 2044     |
| learning rate      | 0.001    |
| nupdates           | 6100     |
| policy_entropy     | 1.78     |
| policy_loss        | 0.031    |
| total_timesteps    | 488000   |
| value_loss         | 0.0211   |
---------------------------------
---------------------------------
| avg reward         | -1.87    |
| epsilonValue       | 9.82     |
| explained_variance | 0.967    |
| fps                | 2045     |
| learning rate      | 0.001    |
| nupdates           | 6200     |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0877  |
| total_timesteps    | 496000   |
| value_loss         | 0.00587  |
---------------------------------
---------------------------------
| avg reward         | -1.45    |
| epsilonValue       | 8.36     |
| explained_variance | 0.851    |
| fps                | 2046     |
| learning rate      | 0.001    |
| nupdates           | 6300     |
| policy_entropy     | 1.79     |
| policy_loss        | -0.0326  |
| total_timesteps    | 504000   |
| value_loss         | 0.00969  |
---------------------------------
---------------------------------
| avg reward         | -1.92    |
| epsilonValue       | 6.91     |
| explained_variance | 0.89     |
| fps                | 2047     |
| learning rate      | 0.001    |
| nupdates           | 6400     |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0493  |
| total_timesteps    | 512000   |
| value_loss         | 0.0112   |
---------------------------------
---------------------------------
| avg reward         | -1.9     |
| epsilonValue       | 5.46     |
| explained_variance | 0.778    |
| fps                | 2048     |
| learning rate      | 0.001    |
| nupdates           | 6500     |
| policy_entropy     | 1.79     |
| policy_loss        | -0.115   |
| total_timesteps    | 520000   |
| value_loss         | 0.026    |
---------------------------------
---------------------------------
| avg reward         | -1.98    |
| epsilonValue       | 4        |
| explained_variance | 0.915    |
| fps                | 2048     |
| learning rate      | 0.001    |
| nupdates           | 6600     |
| policy_entropy     | 1.78     |
| policy_loss        | -0.00279 |
| total_timesteps    | 528000   |
| value_loss         | 0.00515  |
---------------------------------
---------------------------------
| avg reward         | -1.7     |
| epsilonValue       | 2.55     |
| explained_variance | 0.86     |
| fps                | 2048     |
| learning rate      | 0.001    |
| nupdates           | 6700     |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0808   |
| total_timesteps    | 536000   |
| value_loss         | 0.0146   |
---------------------------------
---------------------------------
| avg reward         | -2.07    |
| epsilonValue       | 1.09     |
| explained_variance | 0.908    |
| fps                | 2049     |
| learning rate      | 0.001    |
| nupdates           | 6800     |
| policy_entropy     | 1.78     |
| policy_loss        | 0.00159  |
| total_timesteps    | 544000   |
| value_loss         | 0.0041   |
---------------------------------
---------------------------------
| avg reward         | -1.73    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.885    |
| fps                | 2049     |
| learning rate      | 0.001    |
| nupdates           | 6900     |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0308   |
| total_timesteps    | 552000   |
| value_loss         | 0.00625  |
---------------------------------
---------------------------------
| avg reward         | -1.93    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.939    |
| fps                | 2050     |
| learning rate      | 0.001    |
| nupdates           | 7000     |
| policy_entropy     | 1.78     |
| policy_loss        | -0.00151 |
| total_timesteps    | 560000   |
| value_loss         | 0.00343  |
---------------------------------
---------------------------------
| avg reward         | -2.69    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.865    |
| fps                | 2050     |
| learning rate      | 0.001    |
| nupdates           | 7100     |
| policy_entropy     | 1.77     |
| policy_loss        | 0.00162  |
| total_timesteps    | 568000   |
| value_loss         | 0.00893  |
---------------------------------
---------------------------------
| avg reward         | -1.65    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.779    |
| fps                | 2051     |
| learning rate      | 0.001    |
| nupdates           | 7200     |
| policy_entropy     | 1.78     |
| policy_loss        | 0.00236  |
| total_timesteps    | 576000   |
| value_loss         | 0.0125   |
---------------------------------
---------------------------------
| avg reward         | -1.5     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.828    |
| fps                | 2052     |
| learning rate      | 0.001    |
| nupdates           | 7300     |
| policy_entropy     | 1.78     |
| policy_loss        | 0.00762  |
| total_timesteps    | 584000   |
| value_loss         | 0.00386  |
---------------------------------
---------------------------------
| avg reward         | -2.37    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.782    |
| fps                | 2053     |
| learning rate      | 0.001    |
| nupdates           | 7400     |
| policy_entropy     | 1.78     |
| policy_loss        | -0.145   |
| total_timesteps    | 592000   |
| value_loss         | 0.0156   |
---------------------------------
---------------------------------
| avg reward         | -1.69    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.973    |
| fps                | 2054     |
| learning rate      | 0.001    |
| nupdates           | 7500     |
| policy_entropy     | 1.78     |
| policy_loss        | -0.026   |
| total_timesteps    | 600000   |
| value_loss         | 0.0039   |
---------------------------------
---------------------------------
| avg reward         | -1.99    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.741    |
| fps                | 2056     |
| learning rate      | 0.001    |
| nupdates           | 7600     |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0454  |
| total_timesteps    | 608000   |
| value_loss         | 0.00908  |
---------------------------------
---------------------------------
| avg reward         | -1.84    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.955    |
| fps                | 2057     |
| learning rate      | 0.001    |
| nupdates           | 7700     |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0542   |
| total_timesteps    | 616000   |
| value_loss         | 0.00698  |
---------------------------------
---------------------------------
| avg reward         | -1.86    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.741    |
| fps                | 2058     |
| learning rate      | 0.001    |
| nupdates           | 7800     |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0735   |
| total_timesteps    | 624000   |
| value_loss         | 0.0206   |
---------------------------------
---------------------------------
| avg reward         | -2.08    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.724    |
| fps                | 2058     |
| learning rate      | 0.001    |
| nupdates           | 7900     |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0407   |
| total_timesteps    | 632000   |
| value_loss         | 0.00871  |
---------------------------------
---------------------------------
| avg reward         | -1.84    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.911    |
| fps                | 2059     |
| learning rate      | 0.001    |
| nupdates           | 8000     |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0474  |
| total_timesteps    | 640000   |
| value_loss         | 0.0123   |
---------------------------------
---------------------------------
| avg reward         | -1.8     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.817    |
| fps                | 2060     |
| learning rate      | 0.001    |
| nupdates           | 8100     |
| policy_entropy     | 1.79     |
| policy_loss        | -0.0718  |
| total_timesteps    | 648000   |
| value_loss         | 0.0108   |
---------------------------------
---------------------------------
| avg reward         | -1.96    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.659    |
| fps                | 2061     |
| learning rate      | 0.001    |
| nupdates           | 8200     |
| policy_entropy     | 1.79     |
| policy_loss        | 0.0299   |
| total_timesteps    | 656000   |
| value_loss         | 0.00854  |
---------------------------------
---------------------------------
| avg reward         | -2.11    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.916    |
| fps                | 2062     |
| learning rate      | 0.001    |
| nupdates           | 8300     |
| policy_entropy     | 1.78     |
| policy_loss        | -0.00662 |
| total_timesteps    | 664000   |
| value_loss         | 0.00773  |
---------------------------------
---------------------------------
| avg reward         | -2.47    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.904    |
| fps                | 2061     |
| learning rate      | 0.001    |
| nupdates           | 8400     |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0111  |
| total_timesteps    | 672000   |
| value_loss         | 0.0073   |
---------------------------------
---------------------------------
| avg reward         | -1.75    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.674    |
| fps                | 2061     |
| learning rate      | 0.001    |
| nupdates           | 8500     |
| policy_entropy     | 1.79     |
| policy_loss        | -0.0456  |
| total_timesteps    | 680000   |
| value_loss         | 0.0151   |
---------------------------------
---------------------------------
| avg reward         | -1.77    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.854    |
| fps                | 2061     |
| learning rate      | 0.001    |
| nupdates           | 8600     |
| policy_entropy     | 1.79     |
| policy_loss        | -0.066   |
| total_timesteps    | 688000   |
| value_loss         | 0.00787  |
---------------------------------
---------------------------------
| avg reward         | -1.82    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.913    |
| fps                | 2062     |
| learning rate      | 0.001    |
| nupdates           | 8700     |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0624  |
| total_timesteps    | 696000   |
| value_loss         | 0.00926  |
---------------------------------
---------------------------------
| avg reward         | -2.01    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.836    |
| fps                | 2061     |
| learning rate      | 0.001    |
| nupdates           | 8800     |
| policy_entropy     | 1.78     |
| policy_loss        | -0.00951 |
| total_timesteps    | 704000   |
| value_loss         | 0.0124   |
---------------------------------
---------------------------------
| avg reward         | -2.24    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.873    |
| fps                | 2062     |
| learning rate      | 0.001    |
| nupdates           | 8900     |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0449  |
| total_timesteps    | 712000   |
| value_loss         | 0.00896  |
---------------------------------
---------------------------------
| avg reward         | -1.85    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.883    |
| fps                | 2061     |
| learning rate      | 0.001    |
| nupdates           | 9000     |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0223   |
| total_timesteps    | 720000   |
| value_loss         | 0.0105   |
---------------------------------
---------------------------------
| avg reward         | -2.52    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.898    |
| fps                | 2061     |
| learning rate      | 0.001    |
| nupdates           | 9100     |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0176  |
| total_timesteps    | 728000   |
| value_loss         | 0.00771  |
---------------------------------
---------------------------------
| avg reward         | -1.81    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.873    |
| fps                | 2061     |
| learning rate      | 0.001    |
| nupdates           | 9200     |
| policy_entropy     | 1.79     |
| policy_loss        | 0.00225  |
| total_timesteps    | 736000   |
| value_loss         | 0.00886  |
---------------------------------
---------------------------------
| avg reward         | -1.85    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.919    |
| fps                | 2062     |
| learning rate      | 0.001    |
| nupdates           | 9300     |
| policy_entropy     | 1.79     |
| policy_loss        | 0.0818   |
| total_timesteps    | 744000   |
| value_loss         | 0.0068   |
---------------------------------
---------------------------------
| avg reward         | -2.18    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.697    |
| fps                | 2063     |
| learning rate      | 0.001    |
| nupdates           | 9400     |
| policy_entropy     | 1.79     |
| policy_loss        | -0.0483  |
| total_timesteps    | 752000   |
| value_loss         | 0.0152   |
---------------------------------
---------------------------------
| avg reward         | -1.78    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.714    |
| fps                | 2063     |
| learning rate      | 0.001    |
| nupdates           | 9500     |
| policy_entropy     | 1.79     |
| policy_loss        | 0.108    |
| total_timesteps    | 760000   |
| value_loss         | 0.0375   |
---------------------------------
----------------------------------
| avg reward         | -1.85     |
| epsilonValue       | 0.000727  |
| explained_variance | 0.984     |
| fps                | 2064      |
| learning rate      | 0.001     |
| nupdates           | 9600      |
| policy_entropy     | 1.79      |
| policy_loss        | -0.000483 |
| total_timesteps    | 768000    |
| value_loss         | 0.00258   |
----------------------------------
---------------------------------
| avg reward         | -1.4     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.948    |
| fps                | 2065     |
| learning rate      | 0.001    |
| nupdates           | 9700     |
| policy_entropy     | 1.79     |
| policy_loss        | -0.00328 |
| total_timesteps    | 776000   |
| value_loss         | 0.00406  |
---------------------------------
---------------------------------
| avg reward         | -1.9     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.944    |
| fps                | 2066     |
| learning rate      | 0.001    |
| nupdates           | 9800     |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0304  |
| total_timesteps    | 784000   |
| value_loss         | 0.00689  |
---------------------------------
----------------------------------
| avg reward         | -1.99     |
| epsilonValue       | 0.000727  |
| explained_variance | 0.944     |
| fps                | 2066      |
| learning rate      | 0.001     |
| nupdates           | 9900      |
| policy_entropy     | 1.79      |
| policy_loss        | -0.000913 |
| total_timesteps    | 792000    |
| value_loss         | 0.00351   |
----------------------------------
---------------------------------
| avg reward         | -1.98    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.895    |
| fps                | 2066     |
| learning rate      | 0.001    |
| nupdates           | 10000    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0125  |
| total_timesteps    | 800000   |
| value_loss         | 0.0129   |
---------------------------------
---------------------------------
| avg reward         | -1.69    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.853    |
| fps                | 2066     |
| learning rate      | 0.001    |
| nupdates           | 10100    |
| policy_entropy     | 1.79     |
| policy_loss        | 0.0315   |
| total_timesteps    | 808000   |
| value_loss         | 0.00703  |
---------------------------------
---------------------------------
| avg reward         | -2.01    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.937    |
| fps                | 2067     |
| learning rate      | 0.001    |
| nupdates           | 10200    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.099   |
| total_timesteps    | 816000   |
| value_loss         | 0.00865  |
---------------------------------
---------------------------------
| avg reward         | -1.46    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.958    |
| fps                | 2067     |
| learning rate      | 0.001    |
| nupdates           | 10300    |
| policy_entropy     | 1.79     |
| policy_loss        | 0.0211   |
| total_timesteps    | 824000   |
| value_loss         | 0.00452  |
---------------------------------
---------------------------------
| avg reward         | -2.12    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.89     |
| fps                | 2067     |
| learning rate      | 0.001    |
| nupdates           | 10400    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0424  |
| total_timesteps    | 832000   |
| value_loss         | 0.0101   |
---------------------------------
---------------------------------
| avg reward         | -1.95    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.899    |
| fps                | 2068     |
| learning rate      | 0.001    |
| nupdates           | 10500    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.00601 |
| total_timesteps    | 840000   |
| value_loss         | 0.0106   |
---------------------------------
---------------------------------
| avg reward         | -2.07    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.965    |
| fps                | 2067     |
| learning rate      | 0.001    |
| nupdates           | 10600    |
| policy_entropy     | 1.79     |
| policy_loss        | 0.0771   |
| total_timesteps    | 848000   |
| value_loss         | 0.00557  |
---------------------------------
---------------------------------
| avg reward         | -2.3     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.626    |
| fps                | 2067     |
| learning rate      | 0.001    |
| nupdates           | 10700    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0318   |
| total_timesteps    | 856000   |
| value_loss         | 0.0233   |
---------------------------------
---------------------------------
| avg reward         | -1.89    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.949    |
| fps                | 2067     |
| learning rate      | 0.001    |
| nupdates           | 10800    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.024   |
| total_timesteps    | 864000   |
| value_loss         | 0.00482  |
---------------------------------
---------------------------------
| avg reward         | -2.01    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.89     |
| fps                | 2068     |
| learning rate      | 0.001    |
| nupdates           | 10900    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0211   |
| total_timesteps    | 872000   |
| value_loss         | 0.0122   |
---------------------------------
---------------------------------
| avg reward         | -2.01    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.886    |
| fps                | 2068     |
| learning rate      | 0.001    |
| nupdates           | 11000    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.000108 |
| total_timesteps    | 880000   |
| value_loss         | 0.00584  |
---------------------------------
---------------------------------
| avg reward         | -2.24    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.973    |
| fps                | 2068     |
| learning rate      | 0.001    |
| nupdates           | 11100    |
| policy_entropy     | 1.77     |
| policy_loss        | -0.0289  |
| total_timesteps    | 888000   |
| value_loss         | 0.00421  |
---------------------------------
---------------------------------
| avg reward         | -2.21    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.908    |
| fps                | 2068     |
| learning rate      | 0.001    |
| nupdates           | 11200    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0246   |
| total_timesteps    | 896000   |
| value_loss         | 0.0073   |
---------------------------------
---------------------------------
| avg reward         | -1.92    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.903    |
| fps                | 2069     |
| learning rate      | 0.001    |
| nupdates           | 11300    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0265   |
| total_timesteps    | 904000   |
| value_loss         | 0.00655  |
---------------------------------
---------------------------------
| avg reward         | -1.48    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.929    |
| fps                | 2068     |
| learning rate      | 0.001    |
| nupdates           | 11400    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.089   |
| total_timesteps    | 912000   |
| value_loss         | 0.0144   |
---------------------------------
---------------------------------
| avg reward         | -1.83    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.739    |
| fps                | 2068     |
| learning rate      | 0.001    |
| nupdates           | 11500    |
| policy_entropy     | 1.79     |
| policy_loss        | 0.054    |
| total_timesteps    | 920000   |
| value_loss         | 0.0264   |
---------------------------------
---------------------------------
| avg reward         | -2.23    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.904    |
| fps                | 2068     |
| learning rate      | 0.001    |
| nupdates           | 11600    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.043    |
| total_timesteps    | 928000   |
| value_loss         | 0.00873  |
---------------------------------
---------------------------------
| avg reward         | -2.1     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.781    |
| fps                | 2068     |
| learning rate      | 0.001    |
| nupdates           | 11700    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.0244  |
| total_timesteps    | 936000   |
| value_loss         | 0.00724  |
---------------------------------
---------------------------------
| avg reward         | -2.29    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.84     |
| fps                | 2068     |
| learning rate      | 0.001    |
| nupdates           | 11800    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0336  |
| total_timesteps    | 944000   |
| value_loss         | 0.00738  |
---------------------------------
---------------------------------
| avg reward         | -2.17    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.937    |
| fps                | 2068     |
| learning rate      | 0.001    |
| nupdates           | 11900    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0284  |
| total_timesteps    | 952000   |
| value_loss         | 0.00518  |
---------------------------------
---------------------------------
| avg reward         | -1.88    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.844    |
| fps                | 2068     |
| learning rate      | 0.001    |
| nupdates           | 12000    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0116   |
| total_timesteps    | 960000   |
| value_loss         | 0.0145   |
---------------------------------
---------------------------------
| avg reward         | -2.04    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.89     |
| fps                | 2069     |
| learning rate      | 0.001    |
| nupdates           | 12100    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0264   |
| total_timesteps    | 968000   |
| value_loss         | 0.0101   |
---------------------------------
---------------------------------
| avg reward         | -2.1     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.853    |
| fps                | 2069     |
| learning rate      | 0.001    |
| nupdates           | 12200    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0425  |
| total_timesteps    | 976000   |
| value_loss         | 0.014    |
---------------------------------
---------------------------------
| avg reward         | -2       |
| epsilonValue       | 0.000727 |
| explained_variance | 0.926    |
| fps                | 2069     |
| learning rate      | 0.001    |
| nupdates           | 12300    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.047   |
| total_timesteps    | 984000   |
| value_loss         | 0.00814  |
---------------------------------
----------------------------------
| avg reward         | -1.83     |
| epsilonValue       | 0.000727  |
| explained_variance | 0.923     |
| fps                | 2069      |
| learning rate      | 0.001     |
| nupdates           | 12400     |
| policy_entropy     | 1.78      |
| policy_loss        | -0.000336 |
| total_timesteps    | 992000    |
| value_loss         | 0.00535   |
----------------------------------
---------------------------------
| avg reward         | -1.95    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.966    |
| fps                | 2069     |
| learning rate      | 0.001    |
| nupdates           | 12500    |
| policy_entropy     | 1.77     |
| policy_loss        | -0.0428  |
| total_timesteps    | 1000000  |
| value_loss         | 0.0048   |
---------------------------------
---------------------------------
| avg reward         | -2.14    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.972    |
| fps                | 2069     |
| learning rate      | 0.001    |
| nupdates           | 12600    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.0456  |
| total_timesteps    | 1008000  |
| value_loss         | 0.0042   |
---------------------------------
---------------------------------
| avg reward         | -1.68    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.897    |
| fps                | 2069     |
| learning rate      | 0.001    |
| nupdates           | 12700    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.0118  |
| total_timesteps    | 1016000  |
| value_loss         | 0.00881  |
---------------------------------
---------------------------------
| avg reward         | -2.34    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.973    |
| fps                | 2069     |
| learning rate      | 0.001    |
| nupdates           | 12800    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.0504  |
| total_timesteps    | 1024000  |
| value_loss         | 0.00397  |
---------------------------------
---------------------------------
| avg reward         | -1.64    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.944    |
| fps                | 2070     |
| learning rate      | 0.001    |
| nupdates           | 12900    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.00536 |
| total_timesteps    | 1032000  |
| value_loss         | 0.00841  |
---------------------------------
---------------------------------
| avg reward         | -2.1     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.548    |
| fps                | 2070     |
| learning rate      | 0.001    |
| nupdates           | 13000    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.00334 |
| total_timesteps    | 1040000  |
| value_loss         | 0.0227   |
---------------------------------
---------------------------------
| avg reward         | -1.25    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.912    |
| fps                | 2070     |
| learning rate      | 0.001    |
| nupdates           | 13100    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.0209  |
| total_timesteps    | 1048000  |
| value_loss         | 0.00534  |
---------------------------------
---------------------------------
| avg reward         | -1.96    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.751    |
| fps                | 2070     |
| learning rate      | 0.001    |
| nupdates           | 13200    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0483  |
| total_timesteps    | 1056000  |
| value_loss         | 0.013    |
---------------------------------
---------------------------------
| avg reward         | -2.24    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.909    |
| fps                | 2070     |
| learning rate      | 0.001    |
| nupdates           | 13300    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.045   |
| total_timesteps    | 1064000  |
| value_loss         | 0.00931  |
---------------------------------
---------------------------------
| avg reward         | -1.9     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.911    |
| fps                | 2070     |
| learning rate      | 0.001    |
| nupdates           | 13400    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.0137  |
| total_timesteps    | 1072000  |
| value_loss         | 0.00549  |
---------------------------------
---------------------------------
| avg reward         | -2.26    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.972    |
| fps                | 2070     |
| learning rate      | 0.001    |
| nupdates           | 13500    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.025   |
| total_timesteps    | 1080000  |
| value_loss         | 0.0036   |
---------------------------------
---------------------------------
| avg reward         | -1.81    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.848    |
| fps                | 2070     |
| learning rate      | 0.001    |
| nupdates           | 13600    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0788  |
| total_timesteps    | 1088000  |
| value_loss         | 0.0175   |
---------------------------------
---------------------------------
| avg reward         | -1.72    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.94     |
| fps                | 2071     |
| learning rate      | 0.001    |
| nupdates           | 13700    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.00886 |
| total_timesteps    | 1096000  |
| value_loss         | 0.00366  |
---------------------------------
---------------------------------
| avg reward         | -2.34    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.91     |
| fps                | 2071     |
| learning rate      | 0.001    |
| nupdates           | 13800    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.00211  |
| total_timesteps    | 1104000  |
| value_loss         | 0.0113   |
---------------------------------
---------------------------------
| avg reward         | -1.82    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.939    |
| fps                | 2071     |
| learning rate      | 0.001    |
| nupdates           | 13900    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0365  |
| total_timesteps    | 1112000  |
| value_loss         | 0.00847  |
---------------------------------
---------------------------------
| avg reward         | -2.04    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.871    |
| fps                | 2071     |
| learning rate      | 0.001    |
| nupdates           | 14000    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0678  |
| total_timesteps    | 1120000  |
| value_loss         | 0.00967  |
---------------------------------
---------------------------------
| avg reward         | -2.01    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.942    |
| fps                | 2071     |
| learning rate      | 0.001    |
| nupdates           | 14100    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0515  |
| total_timesteps    | 1128000  |
| value_loss         | 0.00645  |
---------------------------------
---------------------------------
| avg reward         | -1.98    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.909    |
| fps                | 2071     |
| learning rate      | 0.001    |
| nupdates           | 14200    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0493  |
| total_timesteps    | 1136000  |
| value_loss         | 0.00867  |
---------------------------------
---------------------------------
| avg reward         | -2.18    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.925    |
| fps                | 2071     |
| learning rate      | 0.001    |
| nupdates           | 14300    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0286   |
| total_timesteps    | 1144000  |
| value_loss         | 0.00558  |
---------------------------------
---------------------------------
| avg reward         | -1.62    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.967    |
| fps                | 2071     |
| learning rate      | 0.001    |
| nupdates           | 14400    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0119  |
| total_timesteps    | 1152000  |
| value_loss         | 0.00365  |
---------------------------------
---------------------------------
| avg reward         | -1.61    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.909    |
| fps                | 2071     |
| learning rate      | 0.001    |
| nupdates           | 14500    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0102   |
| total_timesteps    | 1160000  |
| value_loss         | 0.00667  |
---------------------------------
---------------------------------
| avg reward         | -1.92    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.944    |
| fps                | 2071     |
| learning rate      | 0.001    |
| nupdates           | 14600    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0416  |
| total_timesteps    | 1168000  |
| value_loss         | 0.00697  |
---------------------------------
---------------------------------
| avg reward         | -1.86    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.934    |
| fps                | 2071     |
| learning rate      | 0.001    |
| nupdates           | 14700    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.0241  |
| total_timesteps    | 1176000  |
| value_loss         | 0.00769  |
---------------------------------
---------------------------------
| avg reward         | -2.32    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.946    |
| fps                | 2072     |
| learning rate      | 0.001    |
| nupdates           | 14800    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0208   |
| total_timesteps    | 1184000  |
| value_loss         | 0.00775  |
---------------------------------
---------------------------------
| avg reward         | -1.93    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.961    |
| fps                | 2072     |
| learning rate      | 0.001    |
| nupdates           | 14900    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0178  |
| total_timesteps    | 1192000  |
| value_loss         | 0.00395  |
---------------------------------
---------------------------------
| avg reward         | -1.81    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.824    |
| fps                | 2072     |
| learning rate      | 0.001    |
| nupdates           | 15000    |
| policy_entropy     | 1.79     |
| policy_loss        | 0.114    |
| total_timesteps    | 1200000  |
| value_loss         | 0.0325   |
---------------------------------
---------------------------------
| avg reward         | -1.81    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.893    |
| fps                | 2072     |
| learning rate      | 0.001    |
| nupdates           | 15100    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0546  |
| total_timesteps    | 1208000  |
| value_loss         | 0.0129   |
---------------------------------
---------------------------------
| avg reward         | -1.91    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.878    |
| fps                | 2072     |
| learning rate      | 0.001    |
| nupdates           | 15200    |
| policy_entropy     | 1.79     |
| policy_loss        | 0.0923   |
| total_timesteps    | 1216000  |
| value_loss         | 0.0246   |
---------------------------------
---------------------------------
| avg reward         | -2.07    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.881    |
| fps                | 2072     |
| learning rate      | 0.001    |
| nupdates           | 15300    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.0643  |
| total_timesteps    | 1224000  |
| value_loss         | 0.0125   |
---------------------------------
---------------------------------
| avg reward         | -1.98    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.954    |
| fps                | 2072     |
| learning rate      | 0.001    |
| nupdates           | 15400    |
| policy_entropy     | 1.79     |
| policy_loss        | 0.106    |
| total_timesteps    | 1232000  |
| value_loss         | 0.00864  |
---------------------------------
---------------------------------
| avg reward         | -1.93    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.85     |
| fps                | 2073     |
| learning rate      | 0.001    |
| nupdates           | 15500    |
| policy_entropy     | 1.79     |
| policy_loss        | 0.0692   |
| total_timesteps    | 1240000  |
| value_loss         | 0.00952  |
---------------------------------
---------------------------------
| avg reward         | -2.24    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.891    |
| fps                | 2073     |
| learning rate      | 0.001    |
| nupdates           | 15600    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0249   |
| total_timesteps    | 1248000  |
| value_loss         | 0.00479  |
---------------------------------
---------------------------------
| avg reward         | -2.21    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.907    |
| fps                | 2073     |
| learning rate      | 0.001    |
| nupdates           | 15700    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0395   |
| total_timesteps    | 1256000  |
| value_loss         | 0.00569  |
---------------------------------
---------------------------------
| avg reward         | -2.07    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.849    |
| fps                | 2073     |
| learning rate      | 0.001    |
| nupdates           | 15800    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0238  |
| total_timesteps    | 1264000  |
| value_loss         | 0.00756  |
---------------------------------
---------------------------------
| avg reward         | -1.98    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.802    |
| fps                | 2073     |
| learning rate      | 0.001    |
| nupdates           | 15900    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0233  |
| total_timesteps    | 1272000  |
| value_loss         | 0.017    |
---------------------------------
---------------------------------
| avg reward         | -1.99    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.893    |
| fps                | 2074     |
| learning rate      | 0.001    |
| nupdates           | 16000    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0303  |
| total_timesteps    | 1280000  |
| value_loss         | 0.0146   |
---------------------------------
---------------------------------
| avg reward         | -2.33    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.846    |
| fps                | 2074     |
| learning rate      | 0.001    |
| nupdates           | 16100    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0117   |
| total_timesteps    | 1288000  |
| value_loss         | 0.013    |
---------------------------------
---------------------------------
| avg reward         | -1.81    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.93     |
| fps                | 2074     |
| learning rate      | 0.001    |
| nupdates           | 16200    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0502   |
| total_timesteps    | 1296000  |
| value_loss         | 0.0122   |
---------------------------------
---------------------------------
| avg reward         | -1.8     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.912    |
| fps                | 2074     |
| learning rate      | 0.001    |
| nupdates           | 16300    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0669   |
| total_timesteps    | 1304000  |
| value_loss         | 0.0131   |
---------------------------------
---------------------------------
| avg reward         | -1.32    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.727    |
| fps                | 2074     |
| learning rate      | 0.001    |
| nupdates           | 16400    |
| policy_entropy     | 1.79     |
| policy_loss        | 0.00758  |
| total_timesteps    | 1312000  |
| value_loss         | 0.00945  |
---------------------------------
---------------------------------
| avg reward         | -1.99    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.947    |
| fps                | 2074     |
| learning rate      | 0.001    |
| nupdates           | 16500    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0369   |
| total_timesteps    | 1320000  |
| value_loss         | 0.00849  |
---------------------------------
---------------------------------
| avg reward         | -2.01    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.866    |
| fps                | 2074     |
| learning rate      | 0.001    |
| nupdates           | 16600    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0536  |
| total_timesteps    | 1328000  |
| value_loss         | 0.00757  |
---------------------------------
---------------------------------
| avg reward         | -2.17    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.945    |
| fps                | 2075     |
| learning rate      | 0.001    |
| nupdates           | 16700    |
| policy_entropy     | 1.77     |
| policy_loss        | -0.0976  |
| total_timesteps    | 1336000  |
| value_loss         | 0.00924  |
---------------------------------
---------------------------------
| avg reward         | -1.66    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.728    |
| fps                | 2075     |
| learning rate      | 0.001    |
| nupdates           | 16800    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.204    |
| total_timesteps    | 1344000  |
| value_loss         | 0.0495   |
---------------------------------
---------------------------------
| avg reward         | -1.35    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.978    |
| fps                | 2074     |
| learning rate      | 0.001    |
| nupdates           | 16900    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.0077  |
| total_timesteps    | 1352000  |
| value_loss         | 0.00264  |
---------------------------------
---------------------------------
| avg reward         | -2.24    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.783    |
| fps                | 2074     |
| learning rate      | 0.001    |
| nupdates           | 17000    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.125    |
| total_timesteps    | 1360000  |
| value_loss         | 0.028    |
---------------------------------
---------------------------------
| avg reward         | -1.91    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.889    |
| fps                | 2074     |
| learning rate      | 0.001    |
| nupdates           | 17100    |
| policy_entropy     | 1.79     |
| policy_loss        | 0.0686   |
| total_timesteps    | 1368000  |
| value_loss         | 0.0155   |
---------------------------------
---------------------------------
| avg reward         | -2.03    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.882    |
| fps                | 2074     |
| learning rate      | 0.001    |
| nupdates           | 17200    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.0267  |
| total_timesteps    | 1376000  |
| value_loss         | 0.00622  |
---------------------------------
---------------------------------
| avg reward         | -1.8     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.881    |
| fps                | 2074     |
| learning rate      | 0.001    |
| nupdates           | 17300    |
| policy_entropy     | 1.79     |
| policy_loss        | 0.035    |
| total_timesteps    | 1384000  |
| value_loss         | 0.0143   |
---------------------------------
---------------------------------
| avg reward         | -1.66    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.946    |
| fps                | 2074     |
| learning rate      | 0.001    |
| nupdates           | 17400    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.0908  |
| total_timesteps    | 1392000  |
| value_loss         | 0.00735  |
---------------------------------
---------------------------------
| avg reward         | -1.95    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.965    |
| fps                | 2075     |
| learning rate      | 0.001    |
| nupdates           | 17500    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.0538  |
| total_timesteps    | 1400000  |
| value_loss         | 0.00502  |
---------------------------------
---------------------------------
| avg reward         | -2.26    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.887    |
| fps                | 2074     |
| learning rate      | 0.001    |
| nupdates           | 17600    |
| policy_entropy     | 1.79     |
| policy_loss        | 0.0269   |
| total_timesteps    | 1408000  |
| value_loss         | 0.00709  |
---------------------------------
---------------------------------
| avg reward         | -2.18    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.947    |
| fps                | 2074     |
| learning rate      | 0.001    |
| nupdates           | 17700    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.0309  |
| total_timesteps    | 1416000  |
| value_loss         | 0.00966  |
---------------------------------
---------------------------------
| avg reward         | -2.16    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.939    |
| fps                | 2074     |
| learning rate      | 0.001    |
| nupdates           | 17800    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.116   |
| total_timesteps    | 1424000  |
| value_loss         | 0.00699  |
---------------------------------
---------------------------------
| avg reward         | -2.05    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.949    |
| fps                | 2074     |
| learning rate      | 0.001    |
| nupdates           | 17900    |
| policy_entropy     | 1.79     |
| policy_loss        | 0.0148   |
| total_timesteps    | 1432000  |
| value_loss         | 0.00785  |
---------------------------------
---------------------------------
| avg reward         | -2.02    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.96     |
| fps                | 2075     |
| learning rate      | 0.001    |
| nupdates           | 18000    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.025    |
| total_timesteps    | 1440000  |
| value_loss         | 0.0117   |
---------------------------------
---------------------------------
| avg reward         | -2.09    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.935    |
| fps                | 2074     |
| learning rate      | 0.001    |
| nupdates           | 18100    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.0295  |
| total_timesteps    | 1448000  |
| value_loss         | 0.00604  |
---------------------------------
---------------------------------
| avg reward         | -1.89    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.938    |
| fps                | 2075     |
| learning rate      | 0.001    |
| nupdates           | 18200    |
| policy_entropy     | 1.79     |
| policy_loss        | 0.0192   |
| total_timesteps    | 1456000  |
| value_loss         | 0.00905  |
---------------------------------
---------------------------------
| avg reward         | -1.87    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.985    |
| fps                | 2075     |
| learning rate      | 0.001    |
| nupdates           | 18300    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.0401  |
| total_timesteps    | 1464000  |
| value_loss         | 0.00266  |
---------------------------------
---------------------------------
| avg reward         | -2.1     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.887    |
| fps                | 2075     |
| learning rate      | 0.001    |
| nupdates           | 18400    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.0796  |
| total_timesteps    | 1472000  |
| value_loss         | 0.00906  |
---------------------------------
---------------------------------
| avg reward         | -2.17    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.816    |
| fps                | 2075     |
| learning rate      | 0.001    |
| nupdates           | 18500    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0763   |
| total_timesteps    | 1480000  |
| value_loss         | 0.0108   |
---------------------------------
---------------------------------
| avg reward         | -1.97    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.94     |
| fps                | 2075     |
| learning rate      | 0.001    |
| nupdates           | 18600    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.0546  |
| total_timesteps    | 1488000  |
| value_loss         | 0.00746  |
---------------------------------
---------------------------------
| avg reward         | -1.69    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.804    |
| fps                | 2075     |
| learning rate      | 0.001    |
| nupdates           | 18700    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.125   |
| total_timesteps    | 1496000  |
| value_loss         | 0.0304   |
---------------------------------
---------------------------------
| avg reward         | -1.85    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.863    |
| fps                | 2075     |
| learning rate      | 0.001    |
| nupdates           | 18800    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.0221  |
| total_timesteps    | 1504000  |
| value_loss         | 0.0102   |
---------------------------------
---------------------------------
| avg reward         | -1.81    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.882    |
| fps                | 2074     |
| learning rate      | 0.001    |
| nupdates           | 18900    |
| policy_entropy     | 1.79     |
| policy_loss        | 0.049    |
| total_timesteps    | 1512000  |
| value_loss         | 0.0136   |
---------------------------------
---------------------------------
| avg reward         | -2.16    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.948    |
| fps                | 2074     |
| learning rate      | 0.001    |
| nupdates           | 19000    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.00767 |
| total_timesteps    | 1520000  |
| value_loss         | 0.00443  |
---------------------------------
---------------------------------
| avg reward         | -2.11    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.681    |
| fps                | 2075     |
| learning rate      | 0.001    |
| nupdates           | 19100    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.00429  |
| total_timesteps    | 1528000  |
| value_loss         | 0.0116   |
---------------------------------
---------------------------------
| avg reward         | -1.9     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.926    |
| fps                | 2075     |
| learning rate      | 0.001    |
| nupdates           | 19200    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.00801  |
| total_timesteps    | 1536000  |
| value_loss         | 0.0075   |
---------------------------------
---------------------------------
| avg reward         | -2.06    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.948    |
| fps                | 2075     |
| learning rate      | 0.001    |
| nupdates           | 19300    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0698   |
| total_timesteps    | 1544000  |
| value_loss         | 0.00796  |
---------------------------------
---------------------------------
| avg reward         | -2.04    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.734    |
| fps                | 2075     |
| learning rate      | 0.001    |
| nupdates           | 19400    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.14    |
| total_timesteps    | 1552000  |
| value_loss         | 0.0226   |
---------------------------------
---------------------------------
| avg reward         | -1.93    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.786    |
| fps                | 2075     |
| learning rate      | 0.001    |
| nupdates           | 19500    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0376   |
| total_timesteps    | 1560000  |
| value_loss         | 0.0125   |
---------------------------------
---------------------------------
| avg reward         | -1.71    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.957    |
| fps                | 2076     |
| learning rate      | 0.001    |
| nupdates           | 19600    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.00619  |
| total_timesteps    | 1568000  |
| value_loss         | 0.00447  |
---------------------------------
----------------------------------
| avg reward         | -1.96     |
| epsilonValue       | 0.000727  |
| explained_variance | 0.949     |
| fps                | 2076      |
| learning rate      | 0.001     |
| nupdates           | 19700     |
| policy_entropy     | 1.78      |
| policy_loss        | -0.000385 |
| total_timesteps    | 1576000   |
| value_loss         | 0.004     |
----------------------------------
---------------------------------
| avg reward         | -1.76    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.922    |
| fps                | 2076     |
| learning rate      | 0.001    |
| nupdates           | 19800    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.00765  |
| total_timesteps    | 1584000  |
| value_loss         | 0.00561  |
---------------------------------
---------------------------------
| avg reward         | -1.69    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.926    |
| fps                | 2077     |
| learning rate      | 0.001    |
| nupdates           | 19900    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.0433  |
| total_timesteps    | 1592000  |
| value_loss         | 0.00729  |
---------------------------------
---------------------------------
| avg reward         | -1.67    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.875    |
| fps                | 2077     |
| learning rate      | 0.001    |
| nupdates           | 20000    |
| policy_entropy     | 1.79     |
| policy_loss        | 0.000852 |
| total_timesteps    | 1600000  |
| value_loss         | 0.00935  |
---------------------------------
---------------------------------
| avg reward         | -2       |
| epsilonValue       | 0.000727 |
| explained_variance | 0.927    |
| fps                | 2077     |
| learning rate      | 0.001    |
| nupdates           | 20100    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.015    |
| total_timesteps    | 1608000  |
| value_loss         | 0.00436  |
---------------------------------
---------------------------------
| avg reward         | -2.15    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.949    |
| fps                | 2077     |
| learning rate      | 0.001    |
| nupdates           | 20200    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0606  |
| total_timesteps    | 1616000  |
| value_loss         | 0.00854  |
---------------------------------
---------------------------------
| avg reward         | -1.98    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.919    |
| fps                | 2077     |
| learning rate      | 0.001    |
| nupdates           | 20300    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0224   |
| total_timesteps    | 1624000  |
| value_loss         | 0.00438  |
---------------------------------
---------------------------------
| avg reward         | -1.83    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.956    |
| fps                | 2077     |
| learning rate      | 0.001    |
| nupdates           | 20400    |
| policy_entropy     | 1.79     |
| policy_loss        | 0.0576   |
| total_timesteps    | 1632000  |
| value_loss         | 0.00674  |
---------------------------------
---------------------------------
| avg reward         | -1.78    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.976    |
| fps                | 2077     |
| learning rate      | 0.001    |
| nupdates           | 20500    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.0108  |
| total_timesteps    | 1640000  |
| value_loss         | 0.00229  |
---------------------------------
---------------------------------
| avg reward         | -1.61    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.893    |
| fps                | 2078     |
| learning rate      | 0.001    |
| nupdates           | 20600    |
| policy_entropy     | 1.79     |
| policy_loss        | 0.0607   |
| total_timesteps    | 1648000  |
| value_loss         | 0.00702  |
---------------------------------
---------------------------------
| avg reward         | -1.6     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.91     |
| fps                | 2078     |
| learning rate      | 0.001    |
| nupdates           | 20700    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.0171  |
| total_timesteps    | 1656000  |
| value_loss         | 0.013    |
---------------------------------
---------------------------------
| avg reward         | -2.09    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.912    |
| fps                | 2078     |
| learning rate      | 0.001    |
| nupdates           | 20800    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.0105  |
| total_timesteps    | 1664000  |
| value_loss         | 0.00943  |
---------------------------------
---------------------------------
| avg reward         | -1.6     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.944    |
| fps                | 2078     |
| learning rate      | 0.001    |
| nupdates           | 20900    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.0526  |
| total_timesteps    | 1672000  |
| value_loss         | 0.00377  |
---------------------------------
---------------------------------
| avg reward         | -1.59    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.81     |
| fps                | 2078     |
| learning rate      | 0.001    |
| nupdates           | 21000    |
| policy_entropy     | 1.77     |
| policy_loss        | 0.0356   |
| total_timesteps    | 1680000  |
| value_loss         | 0.0065   |
---------------------------------
---------------------------------
| avg reward         | -2.05    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.91     |
| fps                | 2077     |
| learning rate      | 0.001    |
| nupdates           | 21100    |
| policy_entropy     | 1.79     |
| policy_loss        | 0.0269   |
| total_timesteps    | 1688000  |
| value_loss         | 0.0066   |
---------------------------------
---------------------------------
| avg reward         | -1.5     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.895    |
| fps                | 2078     |
| learning rate      | 0.001    |
| nupdates           | 21200    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0276   |
| total_timesteps    | 1696000  |
| value_loss         | 0.0113   |
---------------------------------
---------------------------------
| avg reward         | -2.11    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.93     |
| fps                | 2078     |
| learning rate      | 0.001    |
| nupdates           | 21300    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0313  |
| total_timesteps    | 1704000  |
| value_loss         | 0.00586  |
---------------------------------
---------------------------------
| avg reward         | -1.79    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.868    |
| fps                | 2078     |
| learning rate      | 0.001    |
| nupdates           | 21400    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.055   |
| total_timesteps    | 1712000  |
| value_loss         | 0.00838  |
---------------------------------
---------------------------------
| avg reward         | -2.35    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.924    |
| fps                | 2078     |
| learning rate      | 0.001    |
| nupdates           | 21500    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0381   |
| total_timesteps    | 1720000  |
| value_loss         | 0.0126   |
---------------------------------
---------------------------------
| avg reward         | -1.49    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.822    |
| fps                | 2078     |
| learning rate      | 0.001    |
| nupdates           | 21600    |
| policy_entropy     | 1.79     |
| policy_loss        | 0.0338   |
| total_timesteps    | 1728000  |
| value_loss         | 0.0115   |
---------------------------------
---------------------------------
| avg reward         | -2.48    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.898    |
| fps                | 2078     |
| learning rate      | 0.001    |
| nupdates           | 21700    |
| policy_entropy     | 1.77     |
| policy_loss        | -0.0205  |
| total_timesteps    | 1736000  |
| value_loss         | 0.00874  |
---------------------------------
---------------------------------
| avg reward         | -1.72    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.982    |
| fps                | 2078     |
| learning rate      | 0.001    |
| nupdates           | 21800    |
| policy_entropy     | 1.79     |
| policy_loss        | -0.00575 |
| total_timesteps    | 1744000  |
| value_loss         | 0.00282  |
---------------------------------
---------------------------------
| avg reward         | -1.51    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.92     |
| fps                | 2078     |
| learning rate      | 0.001    |
| nupdates           | 21900    |
| policy_entropy     | 1.79     |
| policy_loss        | 0.0107   |
| total_timesteps    | 1752000  |
| value_loss         | 0.00575  |
---------------------------------
---------------------------------
| avg reward         | -1.59    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.946    |
| fps                | 2078     |
| learning rate      | 0.001    |
| nupdates           | 22000    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0313  |
| total_timesteps    | 1760000  |
| value_loss         | 0.00583  |
---------------------------------
---------------------------------
| avg reward         | -2.08    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.952    |
| fps                | 2079     |
| learning rate      | 0.001    |
| nupdates           | 22100    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0372   |
| total_timesteps    | 1768000  |
| value_loss         | 0.00928  |
---------------------------------
---------------------------------
| avg reward         | -2.2     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.948    |
| fps                | 2079     |
| learning rate      | 0.001    |
| nupdates           | 22200    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0523   |
| total_timesteps    | 1776000  |
| value_loss         | 0.00974  |
---------------------------------
---------------------------------
| avg reward         | -2.05    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.886    |
| fps                | 2079     |
| learning rate      | 0.001    |
| nupdates           | 22300    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0103   |
| total_timesteps    | 1784000  |
| value_loss         | 0.0122   |
---------------------------------
---------------------------------
| avg reward         | -1.63    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.902    |
| fps                | 2079     |
| learning rate      | 0.001    |
| nupdates           | 22400    |
| policy_entropy     | 1.79     |
| policy_loss        | 0.0213   |
| total_timesteps    | 1792000  |
| value_loss         | 0.00593  |
---------------------------------
---------------------------------
| avg reward         | -1.7     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.898    |
| fps                | 2079     |
| learning rate      | 0.001    |
| nupdates           | 22500    |
| policy_entropy     | 1.79     |
| policy_loss        | 0.102    |
| total_timesteps    | 1800000  |
| value_loss         | 0.0118   |
---------------------------------
---------------------------------
| avg reward         | -1.73    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.89     |
| fps                | 2079     |
| learning rate      | 0.001    |
| nupdates           | 22600    |
| policy_entropy     | 1.79     |
| policy_loss        | 0.0762   |
| total_timesteps    | 1808000  |
| value_loss         | 0.00934  |
---------------------------------
---------------------------------
| avg reward         | -2       |
| epsilonValue       | 0.000727 |
| explained_variance | 0.944    |
| fps                | 2080     |
| learning rate      | 0.001    |
| nupdates           | 22700    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0681   |
| total_timesteps    | 1816000  |
| value_loss         | 0.00693  |
---------------------------------
---------------------------------
| avg reward         | -1.66    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.957    |
| fps                | 2080     |
| learning rate      | 0.001    |
| nupdates           | 22800    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.00204 |
| total_timesteps    | 1824000  |
| value_loss         | 0.00467  |
---------------------------------
---------------------------------
| avg reward         | -2.17    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.854    |
| fps                | 2080     |
| learning rate      | 0.001    |
| nupdates           | 22900    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0309  |
| total_timesteps    | 1832000  |
| value_loss         | 0.0107   |
---------------------------------
---------------------------------
| avg reward         | -1.64    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.936    |
| fps                | 2080     |
| learning rate      | 0.001    |
| nupdates           | 23000    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0111  |
| total_timesteps    | 1840000  |
| value_loss         | 0.0052   |
---------------------------------
---------------------------------
| avg reward         | -1.67    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.777    |
| fps                | 2080     |
| learning rate      | 0.001    |
| nupdates           | 23100    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0166   |
| total_timesteps    | 1848000  |
| value_loss         | 0.0227   |
---------------------------------
---------------------------------
| avg reward         | -1.37    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.958    |
| fps                | 2080     |
| learning rate      | 0.001    |
| nupdates           | 23200    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.073   |
| total_timesteps    | 1856000  |
| value_loss         | 0.00742  |
---------------------------------
---------------------------------
| avg reward         | -1.65    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.782    |
| fps                | 2080     |
| learning rate      | 0.001    |
| nupdates           | 23300    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.137    |
| total_timesteps    | 1864000  |
| value_loss         | 0.0213   |
---------------------------------
---------------------------------
| avg reward         | -1.59    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.911    |
| fps                | 2080     |
| learning rate      | 0.001    |
| nupdates           | 23400    |
| policy_entropy     | 1.77     |
| policy_loss        | 0.0152   |
| total_timesteps    | 1872000  |
| value_loss         | 0.00816  |
---------------------------------
---------------------------------
| avg reward         | -1.83    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.811    |
| fps                | 2081     |
| learning rate      | 0.001    |
| nupdates           | 23500    |
| policy_entropy     | 1.77     |
| policy_loss        | -0.0321  |
| total_timesteps    | 1880000  |
| value_loss         | 0.0126   |
---------------------------------
---------------------------------
| avg reward         | -1.92    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.839    |
| fps                | 2081     |
| learning rate      | 0.001    |
| nupdates           | 23600    |
| policy_entropy     | 1.77     |
| policy_loss        | -0.0183  |
| total_timesteps    | 1888000  |
| value_loss         | 0.0112   |
---------------------------------
---------------------------------
| avg reward         | -1.58    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.934    |
| fps                | 2080     |
| learning rate      | 0.001    |
| nupdates           | 23700    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0283  |
| total_timesteps    | 1896000  |
| value_loss         | 0.00527  |
---------------------------------
---------------------------------
| avg reward         | -1.82    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.903    |
| fps                | 2080     |
| learning rate      | 0.001    |
| nupdates           | 23800    |
| policy_entropy     | 1.77     |
| policy_loss        | -0.0708  |
| total_timesteps    | 1904000  |
| value_loss         | 0.0058   |
---------------------------------
---------------------------------
| avg reward         | -1.48    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.889    |
| fps                | 2081     |
| learning rate      | 0.001    |
| nupdates           | 23900    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0248   |
| total_timesteps    | 1912000  |
| value_loss         | 0.00464  |
---------------------------------
---------------------------------
| avg reward         | -1.64    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.909    |
| fps                | 2081     |
| learning rate      | 0.001    |
| nupdates           | 24000    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.134    |
| total_timesteps    | 1920000  |
| value_loss         | 0.00968  |
---------------------------------
---------------------------------
| avg reward         | -1.85    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.793    |
| fps                | 2080     |
| learning rate      | 0.001    |
| nupdates           | 24100    |
| policy_entropy     | 1.77     |
| policy_loss        | 0.0483   |
| total_timesteps    | 1928000  |
| value_loss         | 0.0174   |
---------------------------------
---------------------------------
| avg reward         | -1.85    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.839    |
| fps                | 2080     |
| learning rate      | 0.001    |
| nupdates           | 24200    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0376   |
| total_timesteps    | 1936000  |
| value_loss         | 0.0155   |
---------------------------------
---------------------------------
| avg reward         | -1.7     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.933    |
| fps                | 2080     |
| learning rate      | 0.001    |
| nupdates           | 24300    |
| policy_entropy     | 1.77     |
| policy_loss        | -0.00639 |
| total_timesteps    | 1944000  |
| value_loss         | 0.0057   |
---------------------------------
---------------------------------
| avg reward         | -1.6     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.867    |
| fps                | 2080     |
| learning rate      | 0.001    |
| nupdates           | 24400    |
| policy_entropy     | 1.77     |
| policy_loss        | -0.0445  |
| total_timesteps    | 1952000  |
| value_loss         | 0.0133   |
---------------------------------
---------------------------------
| avg reward         | -1.78    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.95     |
| fps                | 2080     |
| learning rate      | 0.001    |
| nupdates           | 24500    |
| policy_entropy     | 1.77     |
| policy_loss        | -0.0706  |
| total_timesteps    | 1960000  |
| value_loss         | 0.00755  |
---------------------------------
---------------------------------
| avg reward         | -2.09    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.936    |
| fps                | 2081     |
| learning rate      | 0.001    |
| nupdates           | 24600    |
| policy_entropy     | 1.76     |
| policy_loss        | 0.0638   |
| total_timesteps    | 1968000  |
| value_loss         | 0.00481  |
---------------------------------
---------------------------------
| avg reward         | -1.46    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.912    |
| fps                | 2081     |
| learning rate      | 0.001    |
| nupdates           | 24700    |
| policy_entropy     | 1.77     |
| policy_loss        | -0.00102 |
| total_timesteps    | 1976000  |
| value_loss         | 0.0038   |
---------------------------------
---------------------------------
| avg reward         | -1.72    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.889    |
| fps                | 2081     |
| learning rate      | 0.001    |
| nupdates           | 24800    |
| policy_entropy     | 1.76     |
| policy_loss        | 0.142    |
| total_timesteps    | 1984000  |
| value_loss         | 0.0177   |
---------------------------------
---------------------------------
| avg reward         | -1.55    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.847    |
| fps                | 2081     |
| learning rate      | 0.001    |
| nupdates           | 24900    |
| policy_entropy     | 1.76     |
| policy_loss        | 0.00427  |
| total_timesteps    | 1992000  |
| value_loss         | 0.00918  |
---------------------------------
---------------------------------
| avg reward         | -1.57    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.931    |
| fps                | 2081     |
| learning rate      | 0.001    |
| nupdates           | 25000    |
| policy_entropy     | 1.77     |
| policy_loss        | 0.00481  |
| total_timesteps    | 2000000  |
| value_loss         | 0.00475  |
---------------------------------
---------------------------------
| avg reward         | -1.89    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.936    |
| fps                | 2081     |
| learning rate      | 0.001    |
| nupdates           | 25100    |
| policy_entropy     | 1.77     |
| policy_loss        | -0.0143  |
| total_timesteps    | 2008000  |
| value_loss         | 0.00944  |
---------------------------------
---------------------------------
| avg reward         | -2.05    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.852    |
| fps                | 2081     |
| learning rate      | 0.001    |
| nupdates           | 25200    |
| policy_entropy     | 1.76     |
| policy_loss        | -0.0193  |
| total_timesteps    | 2016000  |
| value_loss         | 0.0101   |
---------------------------------
---------------------------------
| avg reward         | -1.79    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.957    |
| fps                | 2081     |
| learning rate      | 0.001    |
| nupdates           | 25300    |
| policy_entropy     | 1.77     |
| policy_loss        | -0.0488  |
| total_timesteps    | 2024000  |
| value_loss         | 0.00534  |
---------------------------------
---------------------------------
| avg reward         | -1.96    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.923    |
| fps                | 2081     |
| learning rate      | 0.001    |
| nupdates           | 25400    |
| policy_entropy     | 1.77     |
| policy_loss        | 0.0503   |
| total_timesteps    | 2032000  |
| value_loss         | 0.0137   |
---------------------------------
---------------------------------
| avg reward         | -2.09    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.74     |
| fps                | 2081     |
| learning rate      | 0.001    |
| nupdates           | 25500    |
| policy_entropy     | 1.75     |
| policy_loss        | 0.0797   |
| total_timesteps    | 2040000  |
| value_loss         | 0.0165   |
---------------------------------
---------------------------------
| avg reward         | -1.77    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.9      |
| fps                | 2082     |
| learning rate      | 0.001    |
| nupdates           | 25600    |
| policy_entropy     | 1.77     |
| policy_loss        | 0.0228   |
| total_timesteps    | 2048000  |
| value_loss         | 0.00807  |
---------------------------------
---------------------------------
| avg reward         | -2.01    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.95     |
| fps                | 2081     |
| learning rate      | 0.001    |
| nupdates           | 25700    |
| policy_entropy     | 1.77     |
| policy_loss        | 0.0281   |
| total_timesteps    | 2056000  |
| value_loss         | 0.00781  |
---------------------------------
---------------------------------
| avg reward         | -2.07    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.974    |
| fps                | 2082     |
| learning rate      | 0.001    |
| nupdates           | 25800    |
| policy_entropy     | 1.77     |
| policy_loss        | -0.0137  |
| total_timesteps    | 2064000  |
| value_loss         | 0.0045   |
---------------------------------
---------------------------------
| avg reward         | -1.7     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.898    |
| fps                | 2082     |
| learning rate      | 0.001    |
| nupdates           | 25900    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.14     |
| total_timesteps    | 2072000  |
| value_loss         | 0.0107   |
---------------------------------
---------------------------------
| avg reward         | -1.74    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.924    |
| fps                | 2082     |
| learning rate      | 0.001    |
| nupdates           | 26000    |
| policy_entropy     | 1.77     |
| policy_loss        | -0.0465  |
| total_timesteps    | 2080000  |
| value_loss         | 0.00602  |
---------------------------------
---------------------------------
| avg reward         | -1.87    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.912    |
| fps                | 2082     |
| learning rate      | 0.001    |
| nupdates           | 26100    |
| policy_entropy     | 1.77     |
| policy_loss        | 0.035    |
| total_timesteps    | 2088000  |
| value_loss         | 0.00759  |
---------------------------------
---------------------------------
| avg reward         | -1.55    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.907    |
| fps                | 2082     |
| learning rate      | 0.001    |
| nupdates           | 26200    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0302  |
| total_timesteps    | 2096000  |
| value_loss         | 0.0162   |
---------------------------------
---------------------------------
| avg reward         | -2.12    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.895    |
| fps                | 2082     |
| learning rate      | 0.001    |
| nupdates           | 26300    |
| policy_entropy     | 1.77     |
| policy_loss        | -0.116   |
| total_timesteps    | 2104000  |
| value_loss         | 0.0164   |
---------------------------------
---------------------------------
| avg reward         | -1.96    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.833    |
| fps                | 2082     |
| learning rate      | 0.001    |
| nupdates           | 26400    |
| policy_entropy     | 1.77     |
| policy_loss        | -0.14    |
| total_timesteps    | 2112000  |
| value_loss         | 0.0181   |
---------------------------------
---------------------------------
| avg reward         | -1.28    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.801    |
| fps                | 2082     |
| learning rate      | 0.001    |
| nupdates           | 26500    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0292   |
| total_timesteps    | 2120000  |
| value_loss         | 0.00776  |
---------------------------------
---------------------------------
| avg reward         | -1.74    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.913    |
| fps                | 2083     |
| learning rate      | 0.001    |
| nupdates           | 26600    |
| policy_entropy     | 1.77     |
| policy_loss        | -0.0761  |
| total_timesteps    | 2128000  |
| value_loss         | 0.0115   |
---------------------------------
---------------------------------
| avg reward         | -1.82    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.902    |
| fps                | 2083     |
| learning rate      | 0.001    |
| nupdates           | 26700    |
| policy_entropy     | 1.77     |
| policy_loss        | 0.0696   |
| total_timesteps    | 2136000  |
| value_loss         | 0.0155   |
---------------------------------
---------------------------------
| avg reward         | -2.05    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.98     |
| fps                | 2083     |
| learning rate      | 0.001    |
| nupdates           | 26800    |
| policy_entropy     | 1.76     |
| policy_loss        | -0.0298  |
| total_timesteps    | 2144000  |
| value_loss         | 0.00346  |
---------------------------------
---------------------------------
| avg reward         | -1.45    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.936    |
| fps                | 2083     |
| learning rate      | 0.001    |
| nupdates           | 26900    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.00106  |
| total_timesteps    | 2152000  |
| value_loss         | 0.00469  |
---------------------------------
---------------------------------
| avg reward         | -1.87    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.897    |
| fps                | 2083     |
| learning rate      | 0.001    |
| nupdates           | 27000    |
| policy_entropy     | 1.77     |
| policy_loss        | 0.0157   |
| total_timesteps    | 2160000  |
| value_loss         | 0.00676  |
---------------------------------
---------------------------------
| avg reward         | -1.45    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.87     |
| fps                | 2083     |
| learning rate      | 0.001    |
| nupdates           | 27100    |
| policy_entropy     | 1.77     |
| policy_loss        | 0.0312   |
| total_timesteps    | 2168000  |
| value_loss         | 0.0256   |
---------------------------------
---------------------------------
| avg reward         | -1.81    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.841    |
| fps                | 2083     |
| learning rate      | 0.001    |
| nupdates           | 27200    |
| policy_entropy     | 1.77     |
| policy_loss        | 0.0695   |
| total_timesteps    | 2176000  |
| value_loss         | 0.00775  |
---------------------------------
---------------------------------
| avg reward         | -1.35    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.917    |
| fps                | 2083     |
| learning rate      | 0.001    |
| nupdates           | 27300    |
| policy_entropy     | 1.76     |
| policy_loss        | -0.0337  |
| total_timesteps    | 2184000  |
| value_loss         | 0.0134   |
---------------------------------
---------------------------------
| avg reward         | -1.72    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.797    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 27400    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0581  |
| total_timesteps    | 2192000  |
| value_loss         | 0.0179   |
---------------------------------
---------------------------------
| avg reward         | -1.67    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.911    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 27500    |
| policy_entropy     | 1.77     |
| policy_loss        | 0.0287   |
| total_timesteps    | 2200000  |
| value_loss         | 0.00946  |
---------------------------------
---------------------------------
| avg reward         | -1.99    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.69     |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 27600    |
| policy_entropy     | 1.75     |
| policy_loss        | -0.081   |
| total_timesteps    | 2208000  |
| value_loss         | 0.0223   |
---------------------------------
---------------------------------
| avg reward         | -1.75    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.948    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 27700    |
| policy_entropy     | 1.76     |
| policy_loss        | -0.0197  |
| total_timesteps    | 2216000  |
| value_loss         | 0.00571  |
---------------------------------
---------------------------------
| avg reward         | -1.84    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.932    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 27800    |
| policy_entropy     | 1.77     |
| policy_loss        | 0.0528   |
| total_timesteps    | 2224000  |
| value_loss         | 0.00901  |
---------------------------------
---------------------------------
| avg reward         | -1.32    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.947    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 27900    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0336   |
| total_timesteps    | 2232000  |
| value_loss         | 0.00372  |
---------------------------------
---------------------------------
| avg reward         | -1.32    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.885    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 28000    |
| policy_entropy     | 1.77     |
| policy_loss        | 0.0326   |
| total_timesteps    | 2240000  |
| value_loss         | 0.0112   |
---------------------------------
---------------------------------
| avg reward         | -1.29    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.837    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 28100    |
| policy_entropy     | 1.78     |
| policy_loss        | 0.107    |
| total_timesteps    | 2248000  |
| value_loss         | 0.0305   |
---------------------------------
---------------------------------
| avg reward         | -1.43    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.825    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 28200    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0959  |
| total_timesteps    | 2256000  |
| value_loss         | 0.0184   |
---------------------------------
---------------------------------
| avg reward         | -2.04    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.871    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 28300    |
| policy_entropy     | 1.77     |
| policy_loss        | 0.0696   |
| total_timesteps    | 2264000  |
| value_loss         | 0.00842  |
---------------------------------
---------------------------------
| avg reward         | -1.65    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.963    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 28400    |
| policy_entropy     | 1.78     |
| policy_loss        | -0.0526  |
| total_timesteps    | 2272000  |
| value_loss         | 0.00454  |
---------------------------------
---------------------------------
| avg reward         | -1.73    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.888    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 28500    |
| policy_entropy     | 1.74     |
| policy_loss        | 0.0305   |
| total_timesteps    | 2280000  |
| value_loss         | 0.0114   |
---------------------------------
---------------------------------
| avg reward         | -1.71    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.903    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 28600    |
| policy_entropy     | 1.77     |
| policy_loss        | -0.00499 |
| total_timesteps    | 2288000  |
| value_loss         | 0.00611  |
---------------------------------
---------------------------------
| avg reward         | -1.5     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.873    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 28700    |
| policy_entropy     | 1.77     |
| policy_loss        | -0.0903  |
| total_timesteps    | 2296000  |
| value_loss         | 0.0128   |
---------------------------------
---------------------------------
| avg reward         | -1.88    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.938    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 28800    |
| policy_entropy     | 1.77     |
| policy_loss        | 0.0204   |
| total_timesteps    | 2304000  |
| value_loss         | 0.00844  |
---------------------------------
---------------------------------
| avg reward         | -1.66    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.859    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 28900    |
| policy_entropy     | 1.76     |
| policy_loss        | -0.0412  |
| total_timesteps    | 2312000  |
| value_loss         | 0.00903  |
---------------------------------
---------------------------------
| avg reward         | -1.17    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.598    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 29000    |
| policy_entropy     | 1.77     |
| policy_loss        | 0.0123   |
| total_timesteps    | 2320000  |
| value_loss         | 0.0142   |
---------------------------------
---------------------------------
| avg reward         | -1.45    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.869    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 29100    |
| policy_entropy     | 1.76     |
| policy_loss        | -0.0429  |
| total_timesteps    | 2328000  |
| value_loss         | 0.00912  |
---------------------------------
---------------------------------
| avg reward         | -1.79    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.796    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 29200    |
| policy_entropy     | 1.75     |
| policy_loss        | -0.0228  |
| total_timesteps    | 2336000  |
| value_loss         | 0.0199   |
---------------------------------
---------------------------------
| avg reward         | -1.55    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.75     |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 29300    |
| policy_entropy     | 1.77     |
| policy_loss        | -0.0102  |
| total_timesteps    | 2344000  |
| value_loss         | 0.0179   |
---------------------------------
---------------------------------
| avg reward         | -1.58    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.873    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 29400    |
| policy_entropy     | 1.76     |
| policy_loss        | 0.0423   |
| total_timesteps    | 2352000  |
| value_loss         | 0.0104   |
---------------------------------
---------------------------------
| avg reward         | -1.42    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.515    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 29500    |
| policy_entropy     | 1.75     |
| policy_loss        | 0.0534   |
| total_timesteps    | 2360000  |
| value_loss         | 0.0188   |
---------------------------------
---------------------------------
| avg reward         | -1.34    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.897    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 29600    |
| policy_entropy     | 1.76     |
| policy_loss        | 0.0253   |
| total_timesteps    | 2368000  |
| value_loss         | 0.00543  |
---------------------------------
---------------------------------
| avg reward         | -1.42    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.927    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 29700    |
| policy_entropy     | 1.75     |
| policy_loss        | -0.0604  |
| total_timesteps    | 2376000  |
| value_loss         | 0.00858  |
---------------------------------
---------------------------------
| avg reward         | -1.29    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.788    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 29800    |
| policy_entropy     | 1.76     |
| policy_loss        | -0.0192  |
| total_timesteps    | 2384000  |
| value_loss         | 0.00979  |
---------------------------------
---------------------------------
| avg reward         | -1.57    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.937    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 29900    |
| policy_entropy     | 1.77     |
| policy_loss        | 0.0149   |
| total_timesteps    | 2392000  |
| value_loss         | 0.00441  |
---------------------------------
---------------------------------
| avg reward         | -1.46    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.823    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 30000    |
| policy_entropy     | 1.74     |
| policy_loss        | 0.0169   |
| total_timesteps    | 2400000  |
| value_loss         | 0.0111   |
---------------------------------
---------------------------------
| avg reward         | -1.49    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.83     |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 30100    |
| policy_entropy     | 1.75     |
| policy_loss        | -0.017   |
| total_timesteps    | 2408000  |
| value_loss         | 0.0166   |
---------------------------------
---------------------------------
| avg reward         | -1.43    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.657    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 30200    |
| policy_entropy     | 1.75     |
| policy_loss        | 0.0329   |
| total_timesteps    | 2416000  |
| value_loss         | 0.0405   |
---------------------------------
---------------------------------
| avg reward         | -1.58    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.924    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 30300    |
| policy_entropy     | 1.75     |
| policy_loss        | -0.0833  |
| total_timesteps    | 2424000  |
| value_loss         | 0.0106   |
---------------------------------
---------------------------------
| avg reward         | -1.76    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.68     |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 30400    |
| policy_entropy     | 1.7      |
| policy_loss        | -0.0877  |
| total_timesteps    | 2432000  |
| value_loss         | 0.0423   |
---------------------------------
---------------------------------
| avg reward         | -2.33    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.767    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 30500    |
| policy_entropy     | 1.68     |
| policy_loss        | 0.0234   |
| total_timesteps    | 2440000  |
| value_loss         | 0.0243   |
---------------------------------
---------------------------------
| avg reward         | -1.45    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.931    |
| fps                | 2083     |
| learning rate      | 0.001    |
| nupdates           | 30600    |
| policy_entropy     | 1.74     |
| policy_loss        | 0.0327   |
| total_timesteps    | 2448000  |
| value_loss         | 0.00615  |
---------------------------------
---------------------------------
| avg reward         | -1.58    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.883    |
| fps                | 2083     |
| learning rate      | 0.001    |
| nupdates           | 30700    |
| policy_entropy     | 1.72     |
| policy_loss        | -0.0214  |
| total_timesteps    | 2456000  |
| value_loss         | 0.00859  |
---------------------------------
---------------------------------
| avg reward         | -1.15    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.885    |
| fps                | 2083     |
| learning rate      | 0.001    |
| nupdates           | 30800    |
| policy_entropy     | 1.74     |
| policy_loss        | -0.0184  |
| total_timesteps    | 2464000  |
| value_loss         | 0.0135   |
---------------------------------
---------------------------------
| avg reward         | -1.43    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.868    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 30900    |
| policy_entropy     | 1.74     |
| policy_loss        | 0.0507   |
| total_timesteps    | 2472000  |
| value_loss         | 0.0054   |
---------------------------------
---------------------------------
| avg reward         | -1.21    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.851    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 31000    |
| policy_entropy     | 1.75     |
| policy_loss        | -0.133   |
| total_timesteps    | 2480000  |
| value_loss         | 0.0136   |
---------------------------------
---------------------------------
| avg reward         | -1.4     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.838    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 31100    |
| policy_entropy     | 1.72     |
| policy_loss        | 0.0255   |
| total_timesteps    | 2488000  |
| value_loss         | 0.0164   |
---------------------------------
---------------------------------
| avg reward         | -1.66    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.775    |
| fps                | 2084     |
| learning rate      | 0.001    |
| nupdates           | 31200    |
| policy_entropy     | 1.7      |
| policy_loss        | 0.0608   |
| total_timesteps    | 2496000  |
| value_loss         | 0.0137   |
---------------------------------
---------------------------------
| avg reward         | -1.23    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.709    |
| fps                | 2085     |
| learning rate      | 0.001    |
| nupdates           | 31300    |
| policy_entropy     | 1.73     |
| policy_loss        | -0.0524  |
| total_timesteps    | 2504000  |
| value_loss         | 0.0196   |
---------------------------------
---------------------------------
| avg reward         | -1.4     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.78     |
| fps                | 2085     |
| learning rate      | 0.001    |
| nupdates           | 31400    |
| policy_entropy     | 1.7      |
| policy_loss        | -0.0501  |
| total_timesteps    | 2512000  |
| value_loss         | 0.0115   |
---------------------------------
---------------------------------
| avg reward         | -1.22    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.867    |
| fps                | 2085     |
| learning rate      | 0.001    |
| nupdates           | 31500    |
| policy_entropy     | 1.77     |
| policy_loss        | -0.00497 |
| total_timesteps    | 2520000  |
| value_loss         | 0.00458  |
---------------------------------
---------------------------------
| avg reward         | -1.19    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.705    |
| fps                | 2085     |
| learning rate      | 0.001    |
| nupdates           | 31600    |
| policy_entropy     | 1.71     |
| policy_loss        | 0.0978   |
| total_timesteps    | 2528000  |
| value_loss         | 0.0194   |
---------------------------------
---------------------------------
| avg reward         | -1.18    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.643    |
| fps                | 2085     |
| learning rate      | 0.001    |
| nupdates           | 31700    |
| policy_entropy     | 1.74     |
| policy_loss        | -0.0651  |
| total_timesteps    | 2536000  |
| value_loss         | 0.0157   |
---------------------------------
---------------------------------
| avg reward         | -1.26    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.877    |
| fps                | 2085     |
| learning rate      | 0.001    |
| nupdates           | 31800    |
| policy_entropy     | 1.74     |
| policy_loss        | -0.105   |
| total_timesteps    | 2544000  |
| value_loss         | 0.0194   |
---------------------------------
---------------------------------
| avg reward         | -0.974   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.781    |
| fps                | 2086     |
| learning rate      | 0.001    |
| nupdates           | 31900    |
| policy_entropy     | 1.74     |
| policy_loss        | 0.0406   |
| total_timesteps    | 2552000  |
| value_loss         | 0.00588  |
---------------------------------
---------------------------------
| avg reward         | -1.18    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.87     |
| fps                | 2086     |
| learning rate      | 0.001    |
| nupdates           | 32000    |
| policy_entropy     | 1.72     |
| policy_loss        | -0.0181  |
| total_timesteps    | 2560000  |
| value_loss         | 0.00972  |
---------------------------------
---------------------------------
| avg reward         | -0.762   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.7      |
| fps                | 2086     |
| learning rate      | 0.001    |
| nupdates           | 32100    |
| policy_entropy     | 1.68     |
| policy_loss        | 0.051    |
| total_timesteps    | 2568000  |
| value_loss         | 0.02     |
---------------------------------
---------------------------------
| avg reward         | -0.994   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.81     |
| fps                | 2086     |
| learning rate      | 0.001    |
| nupdates           | 32200    |
| policy_entropy     | 1.75     |
| policy_loss        | 0.036    |
| total_timesteps    | 2576000  |
| value_loss         | 0.00929  |
---------------------------------
---------------------------------
| avg reward         | -1.22    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.727    |
| fps                | 2086     |
| learning rate      | 0.001    |
| nupdates           | 32300    |
| policy_entropy     | 1.67     |
| policy_loss        | -0.0754  |
| total_timesteps    | 2584000  |
| value_loss         | 0.02     |
---------------------------------
---------------------------------
| avg reward         | -0.882   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.879    |
| fps                | 2087     |
| learning rate      | 0.001    |
| nupdates           | 32400    |
| policy_entropy     | 1.69     |
| policy_loss        | -0.0727  |
| total_timesteps    | 2592000  |
| value_loss         | 0.00725  |
---------------------------------
---------------------------------
| avg reward         | -1.11    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.73     |
| fps                | 2087     |
| learning rate      | 0.001    |
| nupdates           | 32500    |
| policy_entropy     | 1.62     |
| policy_loss        | 0.0394   |
| total_timesteps    | 2600000  |
| value_loss         | 0.0137   |
---------------------------------
---------------------------------
| avg reward         | -1.32    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.553    |
| fps                | 2087     |
| learning rate      | 0.001    |
| nupdates           | 32600    |
| policy_entropy     | 1.71     |
| policy_loss        | -0.181   |
| total_timesteps    | 2608000  |
| value_loss         | 0.0364   |
---------------------------------
---------------------------------
| avg reward         | -1.01    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.634    |
| fps                | 2087     |
| learning rate      | 0.001    |
| nupdates           | 32700    |
| policy_entropy     | 1.72     |
| policy_loss        | 0.0629   |
| total_timesteps    | 2616000  |
| value_loss         | 0.00807  |
---------------------------------
---------------------------------
| avg reward         | -0.875   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.839    |
| fps                | 2087     |
| learning rate      | 0.001    |
| nupdates           | 32800    |
| policy_entropy     | 1.73     |
| policy_loss        | -0.0565  |
| total_timesteps    | 2624000  |
| value_loss         | 0.00814  |
---------------------------------
---------------------------------
| avg reward         | -0.774   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.664    |
| fps                | 2087     |
| learning rate      | 0.001    |
| nupdates           | 32900    |
| policy_entropy     | 1.68     |
| policy_loss        | 0.0878   |
| total_timesteps    | 2632000  |
| value_loss         | 0.0232   |
---------------------------------
---------------------------------
| avg reward         | -1.51    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.884    |
| fps                | 2088     |
| learning rate      | 0.001    |
| nupdates           | 33000    |
| policy_entropy     | 1.6      |
| policy_loss        | -0.00491 |
| total_timesteps    | 2640000  |
| value_loss         | 0.00854  |
---------------------------------
---------------------------------
| avg reward         | -0.855   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.862    |
| fps                | 2088     |
| learning rate      | 0.001    |
| nupdates           | 33100    |
| policy_entropy     | 1.72     |
| policy_loss        | -0.0252  |
| total_timesteps    | 2648000  |
| value_loss         | 0.0115   |
---------------------------------
---------------------------------
| avg reward         | -0.764   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.735    |
| fps                | 2088     |
| learning rate      | 0.001    |
| nupdates           | 33200    |
| policy_entropy     | 1.71     |
| policy_loss        | -0.0141  |
| total_timesteps    | 2656000  |
| value_loss         | 0.00504  |
---------------------------------
----------------------------------
| avg reward         | -0.852    |
| epsilonValue       | 0.000727  |
| explained_variance | 0.803     |
| fps                | 2088      |
| learning rate      | 0.001     |
| nupdates           | 33300     |
| policy_entropy     | 1.68      |
| policy_loss        | -0.000269 |
| total_timesteps    | 2664000   |
| value_loss         | 0.00585   |
----------------------------------
---------------------------------
| avg reward         | -0.882   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.781    |
| fps                | 2089     |
| learning rate      | 0.001    |
| nupdates           | 33400    |
| policy_entropy     | 1.68     |
| policy_loss        | -0.0152  |
| total_timesteps    | 2672000  |
| value_loss         | 0.0133   |
---------------------------------
---------------------------------
| avg reward         | -1.13    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.898    |
| fps                | 2089     |
| learning rate      | 0.001    |
| nupdates           | 33500    |
| policy_entropy     | 1.68     |
| policy_loss        | 0.0514   |
| total_timesteps    | 2680000  |
| value_loss         | 0.00377  |
---------------------------------
---------------------------------
| avg reward         | -1.26    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.836    |
| fps                | 2089     |
| learning rate      | 0.001    |
| nupdates           | 33600    |
| policy_entropy     | 1.66     |
| policy_loss        | -0.0696  |
| total_timesteps    | 2688000  |
| value_loss         | 0.00925  |
---------------------------------
---------------------------------
| avg reward         | -1.18    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.871    |
| fps                | 2089     |
| learning rate      | 0.001    |
| nupdates           | 33700    |
| policy_entropy     | 1.59     |
| policy_loss        | -0.0787  |
| total_timesteps    | 2696000  |
| value_loss         | 0.016    |
---------------------------------
---------------------------------
| avg reward         | -0.802   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.87     |
| fps                | 2089     |
| learning rate      | 0.001    |
| nupdates           | 33800    |
| policy_entropy     | 1.65     |
| policy_loss        | 0.00886  |
| total_timesteps    | 2704000  |
| value_loss         | 0.00624  |
---------------------------------
---------------------------------
| avg reward         | -1.05    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.463    |
| fps                | 2089     |
| learning rate      | 0.001    |
| nupdates           | 33900    |
| policy_entropy     | 1.69     |
| policy_loss        | 0.0955   |
| total_timesteps    | 2712000  |
| value_loss         | 0.0226   |
---------------------------------
---------------------------------
| avg reward         | -1.16    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.844    |
| fps                | 2089     |
| learning rate      | 0.001    |
| nupdates           | 34000    |
| policy_entropy     | 1.66     |
| policy_loss        | -0.0272  |
| total_timesteps    | 2720000  |
| value_loss         | 0.00594  |
---------------------------------
---------------------------------
| avg reward         | -0.932   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.861    |
| fps                | 2089     |
| learning rate      | 0.001    |
| nupdates           | 34100    |
| policy_entropy     | 1.7      |
| policy_loss        | 0.00247  |
| total_timesteps    | 2728000  |
| value_loss         | 0.00561  |
---------------------------------
---------------------------------
| avg reward         | -1.07    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.738    |
| fps                | 2090     |
| learning rate      | 0.001    |
| nupdates           | 34200    |
| policy_entropy     | 1.65     |
| policy_loss        | -0.109   |
| total_timesteps    | 2736000  |
| value_loss         | 0.0216   |
---------------------------------
---------------------------------
| avg reward         | -1.14    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.841    |
| fps                | 2090     |
| learning rate      | 0.001    |
| nupdates           | 34300    |
| policy_entropy     | 1.65     |
| policy_loss        | 0.0227   |
| total_timesteps    | 2744000  |
| value_loss         | 0.00618  |
---------------------------------
---------------------------------
| avg reward         | -1.05    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.813    |
| fps                | 2090     |
| learning rate      | 0.001    |
| nupdates           | 34400    |
| policy_entropy     | 1.65     |
| policy_loss        | -0.0615  |
| total_timesteps    | 2752000  |
| value_loss         | 0.00859  |
---------------------------------
---------------------------------
| avg reward         | -0.972   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.802    |
| fps                | 2090     |
| learning rate      | 0.001    |
| nupdates           | 34500    |
| policy_entropy     | 1.66     |
| policy_loss        | 0.00608  |
| total_timesteps    | 2760000  |
| value_loss         | 0.0153   |
---------------------------------
---------------------------------
| avg reward         | -0.87    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.899    |
| fps                | 2091     |
| learning rate      | 0.001    |
| nupdates           | 34600    |
| policy_entropy     | 1.67     |
| policy_loss        | 0.0303   |
| total_timesteps    | 2768000  |
| value_loss         | 0.00525  |
---------------------------------
---------------------------------
| avg reward         | -0.913   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.92     |
| fps                | 2091     |
| learning rate      | 0.001    |
| nupdates           | 34700    |
| policy_entropy     | 1.59     |
| policy_loss        | 0.061    |
| total_timesteps    | 2776000  |
| value_loss         | 0.00547  |
---------------------------------
---------------------------------
| avg reward         | -0.947   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.719    |
| fps                | 2091     |
| learning rate      | 0.001    |
| nupdates           | 34800    |
| policy_entropy     | 1.72     |
| policy_loss        | 0.0665   |
| total_timesteps    | 2784000  |
| value_loss         | 0.0132   |
---------------------------------
---------------------------------
| avg reward         | -0.92    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.834    |
| fps                | 2091     |
| learning rate      | 0.001    |
| nupdates           | 34900    |
| policy_entropy     | 1.67     |
| policy_loss        | -0.00343 |
| total_timesteps    | 2792000  |
| value_loss         | 0.0124   |
---------------------------------
---------------------------------
| avg reward         | -1.23    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.679    |
| fps                | 2091     |
| learning rate      | 0.001    |
| nupdates           | 35000    |
| policy_entropy     | 1.68     |
| policy_loss        | 0.0632   |
| total_timesteps    | 2800000  |
| value_loss         | 0.00885  |
---------------------------------
---------------------------------
| avg reward         | -1.18    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.783    |
| fps                | 2091     |
| learning rate      | 0.001    |
| nupdates           | 35100    |
| policy_entropy     | 1.63     |
| policy_loss        | -0.0226  |
| total_timesteps    | 2808000  |
| value_loss         | 0.0148   |
---------------------------------
---------------------------------
| avg reward         | -1       |
| epsilonValue       | 0.000727 |
| explained_variance | 0.923    |
| fps                | 2091     |
| learning rate      | 0.001    |
| nupdates           | 35200    |
| policy_entropy     | 1.69     |
| policy_loss        | 0.0649   |
| total_timesteps    | 2816000  |
| value_loss         | 0.00677  |
---------------------------------
---------------------------------
| avg reward         | -0.878   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.75     |
| fps                | 2091     |
| learning rate      | 0.001    |
| nupdates           | 35300    |
| policy_entropy     | 1.65     |
| policy_loss        | 0.0312   |
| total_timesteps    | 2824000  |
| value_loss         | 0.00748  |
---------------------------------
---------------------------------
| avg reward         | -0.599   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.688    |
| fps                | 2092     |
| learning rate      | 0.001    |
| nupdates           | 35400    |
| policy_entropy     | 1.68     |
| policy_loss        | 0.0135   |
| total_timesteps    | 2832000  |
| value_loss         | 0.0088   |
---------------------------------
---------------------------------
| avg reward         | -0.88    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.791    |
| fps                | 2092     |
| learning rate      | 0.001    |
| nupdates           | 35500    |
| policy_entropy     | 1.67     |
| policy_loss        | 0.00211  |
| total_timesteps    | 2840000  |
| value_loss         | 0.0173   |
---------------------------------
---------------------------------
| avg reward         | -0.892   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.893    |
| fps                | 2092     |
| learning rate      | 0.001    |
| nupdates           | 35600    |
| policy_entropy     | 1.7      |
| policy_loss        | -0.018   |
| total_timesteps    | 2848000  |
| value_loss         | 0.00403  |
---------------------------------
---------------------------------
| avg reward         | -0.583   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.521    |
| fps                | 2092     |
| learning rate      | 0.001    |
| nupdates           | 35700    |
| policy_entropy     | 1.69     |
| policy_loss        | 0.104    |
| total_timesteps    | 2856000  |
| value_loss         | 0.0102   |
---------------------------------
---------------------------------
| avg reward         | -0.934   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.811    |
| fps                | 2092     |
| learning rate      | 0.001    |
| nupdates           | 35800    |
| policy_entropy     | 1.7      |
| policy_loss        | 0.0429   |
| total_timesteps    | 2864000  |
| value_loss         | 0.0144   |
---------------------------------
---------------------------------
| avg reward         | -0.841   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.85     |
| fps                | 2092     |
| learning rate      | 0.001    |
| nupdates           | 35900    |
| policy_entropy     | 1.65     |
| policy_loss        | 0.098    |
| total_timesteps    | 2872000  |
| value_loss         | 0.00787  |
---------------------------------
---------------------------------
| avg reward         | -0.866   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.888    |
| fps                | 2092     |
| learning rate      | 0.001    |
| nupdates           | 36000    |
| policy_entropy     | 1.69     |
| policy_loss        | -0.0858  |
| total_timesteps    | 2880000  |
| value_loss         | 0.00552  |
---------------------------------
---------------------------------
| avg reward         | -0.892   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.666    |
| fps                | 2093     |
| learning rate      | 0.001    |
| nupdates           | 36100    |
| policy_entropy     | 1.59     |
| policy_loss        | -0.0105  |
| total_timesteps    | 2888000  |
| value_loss         | 0.0106   |
---------------------------------
---------------------------------
| avg reward         | -0.852   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.558    |
| fps                | 2093     |
| learning rate      | 0.001    |
| nupdates           | 36200    |
| policy_entropy     | 1.68     |
| policy_loss        | -0.032   |
| total_timesteps    | 2896000  |
| value_loss         | 0.0287   |
---------------------------------
---------------------------------
| avg reward         | -0.837   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.836    |
| fps                | 2093     |
| learning rate      | 0.001    |
| nupdates           | 36300    |
| policy_entropy     | 1.7      |
| policy_loss        | -0.0316  |
| total_timesteps    | 2904000  |
| value_loss         | 0.0077   |
---------------------------------
---------------------------------
| avg reward         | -0.792   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.769    |
| fps                | 2093     |
| learning rate      | 0.001    |
| nupdates           | 36400    |
| policy_entropy     | 1.69     |
| policy_loss        | -0.0618  |
| total_timesteps    | 2912000  |
| value_loss         | 0.0151   |
---------------------------------
---------------------------------
| avg reward         | -0.479   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.709    |
| fps                | 2093     |
| learning rate      | 0.001    |
| nupdates           | 36500    |
| policy_entropy     | 1.65     |
| policy_loss        | -0.0199  |
| total_timesteps    | 2920000  |
| value_loss         | 0.0059   |
---------------------------------
---------------------------------
| avg reward         | -0.993   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.837    |
| fps                | 2093     |
| learning rate      | 0.001    |
| nupdates           | 36600    |
| policy_entropy     | 1.65     |
| policy_loss        | -0.0602  |
| total_timesteps    | 2928000  |
| value_loss         | 0.0106   |
---------------------------------
---------------------------------
| avg reward         | -1.05    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.899    |
| fps                | 2093     |
| learning rate      | 0.001    |
| nupdates           | 36700    |
| policy_entropy     | 1.67     |
| policy_loss        | 0.0323   |
| total_timesteps    | 2936000  |
| value_loss         | 0.00687  |
---------------------------------
---------------------------------
| avg reward         | -0.927   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.704    |
| fps                | 2093     |
| learning rate      | 0.001    |
| nupdates           | 36800    |
| policy_entropy     | 1.59     |
| policy_loss        | 0.0197   |
| total_timesteps    | 2944000  |
| value_loss         | 0.00422  |
---------------------------------
---------------------------------
| avg reward         | -0.819   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.778    |
| fps                | 2093     |
| learning rate      | 0.001    |
| nupdates           | 36900    |
| policy_entropy     | 1.61     |
| policy_loss        | 0.0139   |
| total_timesteps    | 2952000  |
| value_loss         | 0.0104   |
---------------------------------
---------------------------------
| avg reward         | -0.974   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.851    |
| fps                | 2094     |
| learning rate      | 0.001    |
| nupdates           | 37000    |
| policy_entropy     | 1.69     |
| policy_loss        | -0.0279  |
| total_timesteps    | 2960000  |
| value_loss         | 0.00812  |
---------------------------------
---------------------------------
| avg reward         | -0.768   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.584    |
| fps                | 2094     |
| learning rate      | 0.001    |
| nupdates           | 37100    |
| policy_entropy     | 1.71     |
| policy_loss        | -0.0514  |
| total_timesteps    | 2968000  |
| value_loss         | 0.0149   |
---------------------------------
---------------------------------
| avg reward         | -0.947   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.689    |
| fps                | 2094     |
| learning rate      | 0.001    |
| nupdates           | 37200    |
| policy_entropy     | 1.63     |
| policy_loss        | -0.0325  |
| total_timesteps    | 2976000  |
| value_loss         | 0.0223   |
---------------------------------
---------------------------------
| avg reward         | -0.901   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.581    |
| fps                | 2094     |
| learning rate      | 0.001    |
| nupdates           | 37300    |
| policy_entropy     | 1.7      |
| policy_loss        | -0.00137 |
| total_timesteps    | 2984000  |
| value_loss         | 0.0233   |
---------------------------------
---------------------------------
| avg reward         | -0.882   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.465    |
| fps                | 2094     |
| learning rate      | 0.001    |
| nupdates           | 37400    |
| policy_entropy     | 1.7      |
| policy_loss        | 0.0974   |
| total_timesteps    | 2992000  |
| value_loss         | 0.0146   |
---------------------------------
---------------------------------
| avg reward         | -0.934   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.818    |
| fps                | 2094     |
| learning rate      | 0.001    |
| nupdates           | 37500    |
| policy_entropy     | 1.7      |
| policy_loss        | 0.042    |
| total_timesteps    | 3000000  |
| value_loss         | 0.0104   |
---------------------------------
---------------------------------
| avg reward         | -0.8     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.741    |
| fps                | 2095     |
| learning rate      | 0.001    |
| nupdates           | 37600    |
| policy_entropy     | 1.7      |
| policy_loss        | -0.1     |
| total_timesteps    | 3008000  |
| value_loss         | 0.011    |
---------------------------------
---------------------------------
| avg reward         | -0.935   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.789    |
| fps                | 2094     |
| learning rate      | 0.001    |
| nupdates           | 37700    |
| policy_entropy     | 1.66     |
| policy_loss        | 0.00306  |
| total_timesteps    | 3016000  |
| value_loss         | 0.00781  |
---------------------------------
---------------------------------
| avg reward         | -0.88    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.682    |
| fps                | 2095     |
| learning rate      | 0.001    |
| nupdates           | 37800    |
| policy_entropy     | 1.64     |
| policy_loss        | 0.00994  |
| total_timesteps    | 3024000  |
| value_loss         | 0.0067   |
---------------------------------
---------------------------------
| avg reward         | -1.09    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.696    |
| fps                | 2095     |
| learning rate      | 0.001    |
| nupdates           | 37900    |
| policy_entropy     | 1.63     |
| policy_loss        | 0.0103   |
| total_timesteps    | 3032000  |
| value_loss         | 0.0144   |
---------------------------------
---------------------------------
| avg reward         | -0.597   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.637    |
| fps                | 2095     |
| learning rate      | 0.001    |
| nupdates           | 38000    |
| policy_entropy     | 1.66     |
| policy_loss        | 0.0176   |
| total_timesteps    | 3040000  |
| value_loss         | 0.00649  |
---------------------------------
---------------------------------
| avg reward         | -0.804   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.909    |
| fps                | 2095     |
| learning rate      | 0.001    |
| nupdates           | 38100    |
| policy_entropy     | 1.58     |
| policy_loss        | 0.0506   |
| total_timesteps    | 3048000  |
| value_loss         | 0.0063   |
---------------------------------
---------------------------------
| avg reward         | -1.1     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.855    |
| fps                | 2095     |
| learning rate      | 0.001    |
| nupdates           | 38200    |
| policy_entropy     | 1.67     |
| policy_loss        | -0.00241 |
| total_timesteps    | 3056000  |
| value_loss         | 0.00593  |
---------------------------------
---------------------------------
| avg reward         | -0.759   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.813    |
| fps                | 2095     |
| learning rate      | 0.001    |
| nupdates           | 38300    |
| policy_entropy     | 1.71     |
| policy_loss        | 0.0268   |
| total_timesteps    | 3064000  |
| value_loss         | 0.00465  |
---------------------------------
---------------------------------
| avg reward         | -1.21    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.777    |
| fps                | 2095     |
| learning rate      | 0.001    |
| nupdates           | 38400    |
| policy_entropy     | 1.59     |
| policy_loss        | -0.0481  |
| total_timesteps    | 3072000  |
| value_loss         | 0.011    |
---------------------------------
---------------------------------
| avg reward         | -0.636   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.73     |
| fps                | 2096     |
| learning rate      | 0.001    |
| nupdates           | 38500    |
| policy_entropy     | 1.69     |
| policy_loss        | 0.00548  |
| total_timesteps    | 3080000  |
| value_loss         | 0.0294   |
---------------------------------
---------------------------------
| avg reward         | -0.426   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.795    |
| fps                | 2096     |
| learning rate      | 0.001    |
| nupdates           | 38600    |
| policy_entropy     | 1.67     |
| policy_loss        | 0.0294   |
| total_timesteps    | 3088000  |
| value_loss         | 0.00743  |
---------------------------------
---------------------------------
| avg reward         | -0.478   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.694    |
| fps                | 2096     |
| learning rate      | 0.001    |
| nupdates           | 38700    |
| policy_entropy     | 1.66     |
| policy_loss        | 0.0132   |
| total_timesteps    | 3096000  |
| value_loss         | 0.00562  |
---------------------------------
---------------------------------
| avg reward         | -0.803   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.814    |
| fps                | 2096     |
| learning rate      | 0.001    |
| nupdates           | 38800    |
| policy_entropy     | 1.64     |
| policy_loss        | -0.114   |
| total_timesteps    | 3104000  |
| value_loss         | 0.0164   |
---------------------------------
---------------------------------
| avg reward         | -0.679   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.886    |
| fps                | 2097     |
| learning rate      | 0.001    |
| nupdates           | 38900    |
| policy_entropy     | 1.65     |
| policy_loss        | 0.0478   |
| total_timesteps    | 3112000  |
| value_loss         | 0.00819  |
---------------------------------
---------------------------------
| avg reward         | -0.76    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.643    |
| fps                | 2097     |
| learning rate      | 0.001    |
| nupdates           | 39000    |
| policy_entropy     | 1.66     |
| policy_loss        | 0.0132   |
| total_timesteps    | 3120000  |
| value_loss         | 0.00881  |
---------------------------------
---------------------------------
| avg reward         | -0.539   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.766    |
| fps                | 2097     |
| learning rate      | 0.001    |
| nupdates           | 39100    |
| policy_entropy     | 1.68     |
| policy_loss        | 0.0907   |
| total_timesteps    | 3128000  |
| value_loss         | 0.0103   |
---------------------------------
---------------------------------
| avg reward         | -0.871   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.714    |
| fps                | 2097     |
| learning rate      | 0.001    |
| nupdates           | 39200    |
| policy_entropy     | 1.7      |
| policy_loss        | 0.166    |
| total_timesteps    | 3136000  |
| value_loss         | 0.0309   |
---------------------------------
---------------------------------
| avg reward         | -0.382   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.298    |
| fps                | 2097     |
| learning rate      | 0.001    |
| nupdates           | 39300    |
| policy_entropy     | 1.66     |
| policy_loss        | -0.0452  |
| total_timesteps    | 3144000  |
| value_loss         | 0.00507  |
---------------------------------
---------------------------------
| avg reward         | -0.669   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.654    |
| fps                | 2097     |
| learning rate      | 0.001    |
| nupdates           | 39400    |
| policy_entropy     | 1.66     |
| policy_loss        | -0.00614 |
| total_timesteps    | 3152000  |
| value_loss         | 0.00327  |
---------------------------------
---------------------------------
| avg reward         | -0.629   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.739    |
| fps                | 2098     |
| learning rate      | 0.001    |
| nupdates           | 39500    |
| policy_entropy     | 1.61     |
| policy_loss        | -0.0504  |
| total_timesteps    | 3160000  |
| value_loss         | 0.011    |
---------------------------------
---------------------------------
| avg reward         | -0.493   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.73     |
| fps                | 2098     |
| learning rate      | 0.001    |
| nupdates           | 39600    |
| policy_entropy     | 1.66     |
| policy_loss        | 0.126    |
| total_timesteps    | 3168000  |
| value_loss         | 0.0205   |
---------------------------------
---------------------------------
| avg reward         | -0.651   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.823    |
| fps                | 2098     |
| learning rate      | 0.001    |
| nupdates           | 39700    |
| policy_entropy     | 1.66     |
| policy_loss        | -0.0264  |
| total_timesteps    | 3176000  |
| value_loss         | 0.00395  |
---------------------------------
---------------------------------
| avg reward         | -0.754   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.847    |
| fps                | 2098     |
| learning rate      | 0.001    |
| nupdates           | 39800    |
| policy_entropy     | 1.64     |
| policy_loss        | 0.0517   |
| total_timesteps    | 3184000  |
| value_loss         | 0.0187   |
---------------------------------
---------------------------------
| avg reward         | -0.952   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.755    |
| fps                | 2099     |
| learning rate      | 0.001    |
| nupdates           | 39900    |
| policy_entropy     | 1.7      |
| policy_loss        | 0.0231   |
| total_timesteps    | 3192000  |
| value_loss         | 0.01     |
---------------------------------
---------------------------------
| avg reward         | -1.06    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.86     |
| fps                | 2099     |
| learning rate      | 0.001    |
| nupdates           | 40000    |
| policy_entropy     | 1.68     |
| policy_loss        | -0.00162 |
| total_timesteps    | 3200000  |
| value_loss         | 0.00504  |
---------------------------------
---------------------------------
| avg reward         | -0.708   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.62     |
| fps                | 2099     |
| learning rate      | 0.001    |
| nupdates           | 40100    |
| policy_entropy     | 1.58     |
| policy_loss        | 0.0741   |
| total_timesteps    | 3208000  |
| value_loss         | 0.0295   |
---------------------------------
---------------------------------
| avg reward         | -0.658   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.779    |
| fps                | 2099     |
| learning rate      | 0.001    |
| nupdates           | 40200    |
| policy_entropy     | 1.66     |
| policy_loss        | 0.00895  |
| total_timesteps    | 3216000  |
| value_loss         | 0.0145   |
---------------------------------
---------------------------------
| avg reward         | -0.527   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.373    |
| fps                | 2099     |
| learning rate      | 0.001    |
| nupdates           | 40300    |
| policy_entropy     | 1.65     |
| policy_loss        | 0.00751  |
| total_timesteps    | 3224000  |
| value_loss         | 0.00941  |
---------------------------------
---------------------------------
| avg reward         | -0.533   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.706    |
| fps                | 2099     |
| learning rate      | 0.001    |
| nupdates           | 40400    |
| policy_entropy     | 1.64     |
| policy_loss        | 0.00641  |
| total_timesteps    | 3232000  |
| value_loss         | 0.00547  |
---------------------------------
---------------------------------
| avg reward         | -0.675   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.875    |
| fps                | 2100     |
| learning rate      | 0.001    |
| nupdates           | 40500    |
| policy_entropy     | 1.68     |
| policy_loss        | -0.00401 |
| total_timesteps    | 3240000  |
| value_loss         | 0.0107   |
---------------------------------
---------------------------------
| avg reward         | -0.559   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.891    |
| fps                | 2100     |
| learning rate      | 0.001    |
| nupdates           | 40600    |
| policy_entropy     | 1.64     |
| policy_loss        | -0.0735  |
| total_timesteps    | 3248000  |
| value_loss         | 0.00765  |
---------------------------------
---------------------------------
| avg reward         | -0.546   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.16     |
| fps                | 2100     |
| learning rate      | 0.001    |
| nupdates           | 40700    |
| policy_entropy     | 1.6      |
| policy_loss        | 0.0785   |
| total_timesteps    | 3256000  |
| value_loss         | 0.0217   |
---------------------------------
---------------------------------
| avg reward         | -0.997   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.765    |
| fps                | 2100     |
| learning rate      | 0.001    |
| nupdates           | 40800    |
| policy_entropy     | 1.62     |
| policy_loss        | -0.0263  |
| total_timesteps    | 3264000  |
| value_loss         | 0.0131   |
---------------------------------
---------------------------------
| avg reward         | -0.918   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.776    |
| fps                | 2100     |
| learning rate      | 0.001    |
| nupdates           | 40900    |
| policy_entropy     | 1.69     |
| policy_loss        | 0.0371   |
| total_timesteps    | 3272000  |
| value_loss         | 0.00854  |
---------------------------------
---------------------------------
| avg reward         | -0.736   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.75     |
| fps                | 2100     |
| learning rate      | 0.001    |
| nupdates           | 41000    |
| policy_entropy     | 1.66     |
| policy_loss        | -0.0722  |
| total_timesteps    | 3280000  |
| value_loss         | 0.014    |
---------------------------------
---------------------------------
| avg reward         | -0.715   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.757    |
| fps                | 2101     |
| learning rate      | 0.001    |
| nupdates           | 41100    |
| policy_entropy     | 1.58     |
| policy_loss        | 0.0104   |
| total_timesteps    | 3288000  |
| value_loss         | 0.00992  |
---------------------------------
---------------------------------
| avg reward         | -0.807   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.798    |
| fps                | 2100     |
| learning rate      | 0.001    |
| nupdates           | 41200    |
| policy_entropy     | 1.6      |
| policy_loss        | -0.0221  |
| total_timesteps    | 3296000  |
| value_loss         | 0.0117   |
---------------------------------
---------------------------------
| avg reward         | -0.827   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.59     |
| fps                | 2100     |
| learning rate      | 0.001    |
| nupdates           | 41300    |
| policy_entropy     | 1.69     |
| policy_loss        | -0.0438  |
| total_timesteps    | 3304000  |
| value_loss         | 0.00632  |
---------------------------------
---------------------------------
| avg reward         | -0.645   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.742    |
| fps                | 2100     |
| learning rate      | 0.001    |
| nupdates           | 41400    |
| policy_entropy     | 1.65     |
| policy_loss        | 0.0513   |
| total_timesteps    | 3312000  |
| value_loss         | 0.00917  |
---------------------------------
---------------------------------
| avg reward         | -0.807   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.791    |
| fps                | 2101     |
| learning rate      | 0.001    |
| nupdates           | 41500    |
| policy_entropy     | 1.68     |
| policy_loss        | -0.0293  |
| total_timesteps    | 3320000  |
| value_loss         | 0.00843  |
---------------------------------
---------------------------------
| avg reward         | -0.584   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.922    |
| fps                | 2101     |
| learning rate      | 0.001    |
| nupdates           | 41600    |
| policy_entropy     | 1.72     |
| policy_loss        | 0.0202   |
| total_timesteps    | 3328000  |
| value_loss         | 0.00583  |
---------------------------------
---------------------------------
| avg reward         | -0.899   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.794    |
| fps                | 2101     |
| learning rate      | 0.001    |
| nupdates           | 41700    |
| policy_entropy     | 1.65     |
| policy_loss        | 0.00972  |
| total_timesteps    | 3336000  |
| value_loss         | 0.00768  |
---------------------------------
---------------------------------
| avg reward         | -0.529   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.663    |
| fps                | 2101     |
| learning rate      | 0.001    |
| nupdates           | 41800    |
| policy_entropy     | 1.61     |
| policy_loss        | -0.0521  |
| total_timesteps    | 3344000  |
| value_loss         | 0.0156   |
---------------------------------
---------------------------------
| avg reward         | -0.67    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.835    |
| fps                | 2101     |
| learning rate      | 0.001    |
| nupdates           | 41900    |
| policy_entropy     | 1.58     |
| policy_loss        | -0.016   |
| total_timesteps    | 3352000  |
| value_loss         | 0.0109   |
---------------------------------
---------------------------------
| avg reward         | -0.542   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.424    |
| fps                | 2101     |
| learning rate      | 0.001    |
| nupdates           | 42000    |
| policy_entropy     | 1.61     |
| policy_loss        | -0.0898  |
| total_timesteps    | 3360000  |
| value_loss         | 0.0137   |
---------------------------------
---------------------------------
| avg reward         | -0.701   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.674    |
| fps                | 2101     |
| learning rate      | 0.001    |
| nupdates           | 42100    |
| policy_entropy     | 1.67     |
| policy_loss        | 0.0747   |
| total_timesteps    | 3368000  |
| value_loss         | 0.0232   |
---------------------------------
---------------------------------
| avg reward         | -0.868   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.916    |
| fps                | 2101     |
| learning rate      | 0.001    |
| nupdates           | 42200    |
| policy_entropy     | 1.65     |
| policy_loss        | -0.0178  |
| total_timesteps    | 3376000  |
| value_loss         | 0.00337  |
---------------------------------
---------------------------------
| avg reward         | -0.925   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.882    |
| fps                | 2102     |
| learning rate      | 0.001    |
| nupdates           | 42300    |
| policy_entropy     | 1.67     |
| policy_loss        | 0.00333  |
| total_timesteps    | 3384000  |
| value_loss         | 0.00666  |
---------------------------------
---------------------------------
| avg reward         | -0.8     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.879    |
| fps                | 2102     |
| learning rate      | 0.001    |
| nupdates           | 42400    |
| policy_entropy     | 1.67     |
| policy_loss        | -0.0455  |
| total_timesteps    | 3392000  |
| value_loss         | 0.00946  |
---------------------------------
---------------------------------
| avg reward         | -0.742   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.711    |
| fps                | 2102     |
| learning rate      | 0.001    |
| nupdates           | 42500    |
| policy_entropy     | 1.69     |
| policy_loss        | -0.0677  |
| total_timesteps    | 3400000  |
| value_loss         | 0.0169   |
---------------------------------
---------------------------------
| avg reward         | -0.479   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.628    |
| fps                | 2102     |
| learning rate      | 0.001    |
| nupdates           | 42600    |
| policy_entropy     | 1.66     |
| policy_loss        | 0.0866   |
| total_timesteps    | 3408000  |
| value_loss         | 0.0169   |
---------------------------------
---------------------------------
| avg reward         | -0.714   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.827    |
| fps                | 2102     |
| learning rate      | 0.001    |
| nupdates           | 42700    |
| policy_entropy     | 1.69     |
| policy_loss        | 0.0185   |
| total_timesteps    | 3416000  |
| value_loss         | 0.00655  |
---------------------------------
---------------------------------
| avg reward         | -1.22    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.781    |
| fps                | 2102     |
| learning rate      | 0.001    |
| nupdates           | 42800    |
| policy_entropy     | 1.6      |
| policy_loss        | -0.116   |
| total_timesteps    | 3424000  |
| value_loss         | 0.0146   |
---------------------------------
---------------------------------
| avg reward         | -0.57    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.557    |
| fps                | 2102     |
| learning rate      | 0.001    |
| nupdates           | 42900    |
| policy_entropy     | 1.7      |
| policy_loss        | -0.0538  |
| total_timesteps    | 3432000  |
| value_loss         | 0.015    |
---------------------------------
---------------------------------
| avg reward         | -0.566   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.358    |
| fps                | 2102     |
| learning rate      | 0.001    |
| nupdates           | 43000    |
| policy_entropy     | 1.61     |
| policy_loss        | -0.0545  |
| total_timesteps    | 3440000  |
| value_loss         | 0.00848  |
---------------------------------
---------------------------------
| avg reward         | -0.432   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.678    |
| fps                | 2102     |
| learning rate      | 0.001    |
| nupdates           | 43100    |
| policy_entropy     | 1.68     |
| policy_loss        | 0.039    |
| total_timesteps    | 3448000  |
| value_loss         | 0.00697  |
---------------------------------
---------------------------------
| avg reward         | -0.769   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.78     |
| fps                | 2103     |
| learning rate      | 0.001    |
| nupdates           | 43200    |
| policy_entropy     | 1.63     |
| policy_loss        | -0.0797  |
| total_timesteps    | 3456000  |
| value_loss         | 0.0195   |
---------------------------------
---------------------------------
| avg reward         | -0.331   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.861    |
| fps                | 2103     |
| learning rate      | 0.001    |
| nupdates           | 43300    |
| policy_entropy     | 1.6      |
| policy_loss        | 0.0166   |
| total_timesteps    | 3464000  |
| value_loss         | 0.0204   |
---------------------------------
---------------------------------
| avg reward         | -0.424   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.872    |
| fps                | 2103     |
| learning rate      | 0.001    |
| nupdates           | 43400    |
| policy_entropy     | 1.67     |
| policy_loss        | 0.0925   |
| total_timesteps    | 3472000  |
| value_loss         | 0.0133   |
---------------------------------
---------------------------------
| avg reward         | -0.801   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.854    |
| fps                | 2103     |
| learning rate      | 0.001    |
| nupdates           | 43500    |
| policy_entropy     | 1.69     |
| policy_loss        | -0.0359  |
| total_timesteps    | 3480000  |
| value_loss         | 0.0116   |
---------------------------------
---------------------------------
| avg reward         | -0.716   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.886    |
| fps                | 2103     |
| learning rate      | 0.001    |
| nupdates           | 43600    |
| policy_entropy     | 1.64     |
| policy_loss        | 0.0834   |
| total_timesteps    | 3488000  |
| value_loss         | 0.0153   |
---------------------------------
----------------------------------
| avg reward         | -0.354    |
| epsilonValue       | 0.000727  |
| explained_variance | 0.893     |
| fps                | 2103      |
| learning rate      | 0.001     |
| nupdates           | 43700     |
| policy_entropy     | 1.69      |
| policy_loss        | -0.000808 |
| total_timesteps    | 3496000   |
| value_loss         | 0.00568   |
----------------------------------
---------------------------------
| avg reward         | -0.42    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.871    |
| fps                | 2103     |
| learning rate      | 0.001    |
| nupdates           | 43800    |
| policy_entropy     | 1.62     |
| policy_loss        | 0.0176   |
| total_timesteps    | 3504000  |
| value_loss         | 0.00936  |
---------------------------------
---------------------------------
| avg reward         | -0.496   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.653    |
| fps                | 2104     |
| learning rate      | 0.001    |
| nupdates           | 43900    |
| policy_entropy     | 1.68     |
| policy_loss        | -0.0335  |
| total_timesteps    | 3512000  |
| value_loss         | 0.00903  |
---------------------------------
---------------------------------
| avg reward         | -0.662   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.915    |
| fps                | 2104     |
| learning rate      | 0.001    |
| nupdates           | 44000    |
| policy_entropy     | 1.55     |
| policy_loss        | -0.0149  |
| total_timesteps    | 3520000  |
| value_loss         | 0.0116   |
---------------------------------
---------------------------------
| avg reward         | -0.41    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.859    |
| fps                | 2104     |
| learning rate      | 0.001    |
| nupdates           | 44100    |
| policy_entropy     | 1.65     |
| policy_loss        | 0.00683  |
| total_timesteps    | 3528000  |
| value_loss         | 0.00674  |
---------------------------------
---------------------------------
| avg reward         | -0.693   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.793    |
| fps                | 2104     |
| learning rate      | 0.001    |
| nupdates           | 44200    |
| policy_entropy     | 1.71     |
| policy_loss        | -0.0128  |
| total_timesteps    | 3536000  |
| value_loss         | 0.00803  |
---------------------------------
---------------------------------
| avg reward         | -0.652   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.895    |
| fps                | 2104     |
| learning rate      | 0.001    |
| nupdates           | 44300    |
| policy_entropy     | 1.66     |
| policy_loss        | 0.061    |
| total_timesteps    | 3544000  |
| value_loss         | 0.0119   |
---------------------------------
---------------------------------
| avg reward         | -0.455   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.799    |
| fps                | 2104     |
| learning rate      | 0.001    |
| nupdates           | 44400    |
| policy_entropy     | 1.42     |
| policy_loss        | -0.0232  |
| total_timesteps    | 3552000  |
| value_loss         | 0.0179   |
---------------------------------
---------------------------------
| avg reward         | -0.271   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.906    |
| fps                | 2104     |
| learning rate      | 0.001    |
| nupdates           | 44500    |
| policy_entropy     | 1.7      |
| policy_loss        | -0.0595  |
| total_timesteps    | 3560000  |
| value_loss         | 0.00658  |
---------------------------------
---------------------------------
| avg reward         | -0.125   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.95     |
| fps                | 2105     |
| learning rate      | 0.001    |
| nupdates           | 44600    |
| policy_entropy     | 1.62     |
| policy_loss        | -0.0471  |
| total_timesteps    | 3568000  |
| value_loss         | 0.00678  |
---------------------------------
---------------------------------
| avg reward         | -0.251   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.748    |
| fps                | 2105     |
| learning rate      | 0.001    |
| nupdates           | 44700    |
| policy_entropy     | 1.63     |
| policy_loss        | 0.00718  |
| total_timesteps    | 3576000  |
| value_loss         | 0.0156   |
---------------------------------
---------------------------------
| avg reward         | -0.34    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.816    |
| fps                | 2105     |
| learning rate      | 0.001    |
| nupdates           | 44800    |
| policy_entropy     | 1.68     |
| policy_loss        | 0.0563   |
| total_timesteps    | 3584000  |
| value_loss         | 0.0129   |
---------------------------------
---------------------------------
| avg reward         | -0.339   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.851    |
| fps                | 2105     |
| learning rate      | 0.001    |
| nupdates           | 44900    |
| policy_entropy     | 1.71     |
| policy_loss        | -0.0182  |
| total_timesteps    | 3592000  |
| value_loss         | 0.0178   |
---------------------------------
---------------------------------
| avg reward         | -0.524   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.865    |
| fps                | 2105     |
| learning rate      | 0.001    |
| nupdates           | 45000    |
| policy_entropy     | 1.68     |
| policy_loss        | -0.0973  |
| total_timesteps    | 3600000  |
| value_loss         | 0.0103   |
---------------------------------
---------------------------------
| avg reward         | -0.357   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.937    |
| fps                | 2105     |
| learning rate      | 0.001    |
| nupdates           | 45100    |
| policy_entropy     | 1.73     |
| policy_loss        | -0.0861  |
| total_timesteps    | 3608000  |
| value_loss         | 0.00994  |
---------------------------------
---------------------------------
| avg reward         | -0.453   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.922    |
| fps                | 2105     |
| learning rate      | 0.001    |
| nupdates           | 45200    |
| policy_entropy     | 1.63     |
| policy_loss        | -0.00247 |
| total_timesteps    | 3616000  |
| value_loss         | 0.00762  |
---------------------------------
---------------------------------
| avg reward         | -0.223   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.571    |
| fps                | 2105     |
| learning rate      | 0.001    |
| nupdates           | 45300    |
| policy_entropy     | 1.64     |
| policy_loss        | 0.0646   |
| total_timesteps    | 3624000  |
| value_loss         | 0.0122   |
---------------------------------
---------------------------------
| avg reward         | -0.336   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.905    |
| fps                | 2105     |
| learning rate      | 0.001    |
| nupdates           | 45400    |
| policy_entropy     | 1.66     |
| policy_loss        | -0.0208  |
| total_timesteps    | 3632000  |
| value_loss         | 0.00955  |
---------------------------------
---------------------------------
| avg reward         | -0.519   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.818    |
| fps                | 2105     |
| learning rate      | 0.001    |
| nupdates           | 45500    |
| policy_entropy     | 1.63     |
| policy_loss        | 0.0304   |
| total_timesteps    | 3640000  |
| value_loss         | 0.0226   |
---------------------------------
---------------------------------
| avg reward         | -0.464   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.917    |
| fps                | 2105     |
| learning rate      | 0.001    |
| nupdates           | 45600    |
| policy_entropy     | 1.55     |
| policy_loss        | -0.0862  |
| total_timesteps    | 3648000  |
| value_loss         | 0.00693  |
---------------------------------
---------------------------------
| avg reward         | -0.262   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.838    |
| fps                | 2106     |
| learning rate      | 0.001    |
| nupdates           | 45700    |
| policy_entropy     | 1.63     |
| policy_loss        | 0.0573   |
| total_timesteps    | 3656000  |
| value_loss         | 0.0117   |
---------------------------------
---------------------------------
| avg reward         | -0.0901  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.914    |
| fps                | 2105     |
| learning rate      | 0.001    |
| nupdates           | 45800    |
| policy_entropy     | 1.59     |
| policy_loss        | -0.0249  |
| total_timesteps    | 3664000  |
| value_loss         | 0.0118   |
---------------------------------
---------------------------------
| avg reward         | -0.327   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.833    |
| fps                | 2106     |
| learning rate      | 0.001    |
| nupdates           | 45900    |
| policy_entropy     | 1.66     |
| policy_loss        | -0.0651  |
| total_timesteps    | 3672000  |
| value_loss         | 0.00884  |
---------------------------------
---------------------------------
| avg reward         | -0.381   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.894    |
| fps                | 2106     |
| learning rate      | 0.001    |
| nupdates           | 46000    |
| policy_entropy     | 1.5      |
| policy_loss        | -0.071   |
| total_timesteps    | 3680000  |
| value_loss         | 0.0118   |
---------------------------------
---------------------------------
| avg reward         | -0.483   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.937    |
| fps                | 2106     |
| learning rate      | 0.001    |
| nupdates           | 46100    |
| policy_entropy     | 1.63     |
| policy_loss        | -0.0126  |
| total_timesteps    | 3688000  |
| value_loss         | 0.00625  |
---------------------------------
---------------------------------
| avg reward         | -0.28    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.835    |
| fps                | 2106     |
| learning rate      | 0.001    |
| nupdates           | 46200    |
| policy_entropy     | 1.71     |
| policy_loss        | 0.0622   |
| total_timesteps    | 3696000  |
| value_loss         | 0.0195   |
---------------------------------
---------------------------------
| avg reward         | -0.281   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.929    |
| fps                | 2106     |
| learning rate      | 0.001    |
| nupdates           | 46300    |
| policy_entropy     | 1.58     |
| policy_loss        | -0.0166  |
| total_timesteps    | 3704000  |
| value_loss         | 0.00821  |
---------------------------------
---------------------------------
| avg reward         | -0.442   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.805    |
| fps                | 2106     |
| learning rate      | 0.001    |
| nupdates           | 46400    |
| policy_entropy     | 1.69     |
| policy_loss        | -0.0253  |
| total_timesteps    | 3712000  |
| value_loss         | 0.00571  |
---------------------------------
---------------------------------
| avg reward         | -0.208   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.88     |
| fps                | 2106     |
| learning rate      | 0.001    |
| nupdates           | 46500    |
| policy_entropy     | 1.56     |
| policy_loss        | -0.0651  |
| total_timesteps    | 3720000  |
| value_loss         | 0.015    |
---------------------------------
---------------------------------
| avg reward         | -0.438   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.726    |
| fps                | 2107     |
| learning rate      | 0.001    |
| nupdates           | 46600    |
| policy_entropy     | 1.64     |
| policy_loss        | -0.111   |
| total_timesteps    | 3728000  |
| value_loss         | 0.0167   |
---------------------------------
---------------------------------
| avg reward         | -0.268   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.76     |
| fps                | 2107     |
| learning rate      | 0.001    |
| nupdates           | 46700    |
| policy_entropy     | 1.71     |
| policy_loss        | 0.065    |
| total_timesteps    | 3736000  |
| value_loss         | 0.0186   |
---------------------------------
---------------------------------
| avg reward         | -0.434   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.877    |
| fps                | 2107     |
| learning rate      | 0.001    |
| nupdates           | 46800    |
| policy_entropy     | 1.54     |
| policy_loss        | 0.00901  |
| total_timesteps    | 3744000  |
| value_loss         | 0.0098   |
---------------------------------
---------------------------------
| avg reward         | -0.491   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.8      |
| fps                | 2107     |
| learning rate      | 0.001    |
| nupdates           | 46900    |
| policy_entropy     | 1.51     |
| policy_loss        | -0.177   |
| total_timesteps    | 3752000  |
| value_loss         | 0.0312   |
---------------------------------
---------------------------------
| avg reward         | -0.266   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.877    |
| fps                | 2107     |
| learning rate      | 0.001    |
| nupdates           | 47000    |
| policy_entropy     | 1.64     |
| policy_loss        | 0.0116   |
| total_timesteps    | 3760000  |
| value_loss         | 0.0148   |
---------------------------------
---------------------------------
| avg reward         | -0.411   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.654    |
| fps                | 2107     |
| learning rate      | 0.001    |
| nupdates           | 47100    |
| policy_entropy     | 1.55     |
| policy_loss        | -0.192   |
| total_timesteps    | 3768000  |
| value_loss         | 0.0448   |
---------------------------------
---------------------------------
| avg reward         | -0.372   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.878    |
| fps                | 2107     |
| learning rate      | 0.001    |
| nupdates           | 47200    |
| policy_entropy     | 1.62     |
| policy_loss        | -0.0513  |
| total_timesteps    | 3776000  |
| value_loss         | 0.00958  |
---------------------------------
---------------------------------
| avg reward         | -0.184   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.641    |
| fps                | 2107     |
| learning rate      | 0.001    |
| nupdates           | 47300    |
| policy_entropy     | 1.59     |
| policy_loss        | -0.0224  |
| total_timesteps    | 3784000  |
| value_loss         | 0.0183   |
---------------------------------
---------------------------------
| avg reward         | -0.328   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.774    |
| fps                | 2107     |
| learning rate      | 0.001    |
| nupdates           | 47400    |
| policy_entropy     | 1.61     |
| policy_loss        | 0.0162   |
| total_timesteps    | 3792000  |
| value_loss         | 0.0164   |
---------------------------------
---------------------------------
| avg reward         | -0.217   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.924    |
| fps                | 2107     |
| learning rate      | 0.001    |
| nupdates           | 47500    |
| policy_entropy     | 1.64     |
| policy_loss        | -0.0744  |
| total_timesteps    | 3800000  |
| value_loss         | 0.0116   |
---------------------------------
---------------------------------
| avg reward         | -0.1     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.88     |
| fps                | 2107     |
| learning rate      | 0.001    |
| nupdates           | 47600    |
| policy_entropy     | 1.58     |
| policy_loss        | 0.156    |
| total_timesteps    | 3808000  |
| value_loss         | 0.0236   |
---------------------------------
---------------------------------
| avg reward         | -0.255   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.846    |
| fps                | 2108     |
| learning rate      | 0.001    |
| nupdates           | 47700    |
| policy_entropy     | 1.44     |
| policy_loss        | 0.0845   |
| total_timesteps    | 3816000  |
| value_loss         | 0.0227   |
---------------------------------
---------------------------------
| avg reward         | -0.635   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.871    |
| fps                | 2108     |
| learning rate      | 0.001    |
| nupdates           | 47800    |
| policy_entropy     | 1.54     |
| policy_loss        | 0.0118   |
| total_timesteps    | 3824000  |
| value_loss         | 0.0112   |
---------------------------------
---------------------------------
| avg reward         | -0.533   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.904    |
| fps                | 2108     |
| learning rate      | 0.001    |
| nupdates           | 47900    |
| policy_entropy     | 1.67     |
| policy_loss        | 0.0879   |
| total_timesteps    | 3832000  |
| value_loss         | 0.0078   |
---------------------------------
---------------------------------
| avg reward         | -0.274   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.843    |
| fps                | 2108     |
| learning rate      | 0.001    |
| nupdates           | 48000    |
| policy_entropy     | 1.53     |
| policy_loss        | -0.0318  |
| total_timesteps    | 3840000  |
| value_loss         | 0.0213   |
---------------------------------
---------------------------------
| avg reward         | -0.231   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.893    |
| fps                | 2108     |
| learning rate      | 0.001    |
| nupdates           | 48100    |
| policy_entropy     | 1.67     |
| policy_loss        | -0.0549  |
| total_timesteps    | 3848000  |
| value_loss         | 0.00843  |
---------------------------------
---------------------------------
| avg reward         | -0.417   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.924    |
| fps                | 2108     |
| learning rate      | 0.001    |
| nupdates           | 48200    |
| policy_entropy     | 1.51     |
| policy_loss        | 0.0614   |
| total_timesteps    | 3856000  |
| value_loss         | 0.013    |
---------------------------------
---------------------------------
| avg reward         | 0.0275   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.901    |
| fps                | 2108     |
| learning rate      | 0.001    |
| nupdates           | 48300    |
| policy_entropy     | 1.43     |
| policy_loss        | 0.042    |
| total_timesteps    | 3864000  |
| value_loss         | 0.0191   |
---------------------------------
---------------------------------
| avg reward         | -0.29    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.73     |
| fps                | 2108     |
| learning rate      | 0.001    |
| nupdates           | 48400    |
| policy_entropy     | 1.61     |
| policy_loss        | 0.0717   |
| total_timesteps    | 3872000  |
| value_loss         | 0.0113   |
---------------------------------
---------------------------------
| avg reward         | 0.0753   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.724    |
| fps                | 2108     |
| learning rate      | 0.001    |
| nupdates           | 48500    |
| policy_entropy     | 1.62     |
| policy_loss        | 0.0366   |
| total_timesteps    | 3880000  |
| value_loss         | 0.0187   |
---------------------------------
---------------------------------
| avg reward         | -0.112   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.772    |
| fps                | 2108     |
| learning rate      | 0.001    |
| nupdates           | 48600    |
| policy_entropy     | 1.72     |
| policy_loss        | 0.0067   |
| total_timesteps    | 3888000  |
| value_loss         | 0.0153   |
---------------------------------
---------------------------------
| avg reward         | -0.296   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.606    |
| fps                | 2108     |
| learning rate      | 0.001    |
| nupdates           | 48700    |
| policy_entropy     | 1.73     |
| policy_loss        | 0.0337   |
| total_timesteps    | 3896000  |
| value_loss         | 0.0167   |
---------------------------------
---------------------------------
| avg reward         | -0.655   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.783    |
| fps                | 2108     |
| learning rate      | 0.001    |
| nupdates           | 48800    |
| policy_entropy     | 1.62     |
| policy_loss        | -0.042   |
| total_timesteps    | 3904000  |
| value_loss         | 0.00757  |
---------------------------------
---------------------------------
| avg reward         | -0.205   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.929    |
| fps                | 2108     |
| learning rate      | 0.001    |
| nupdates           | 48900    |
| policy_entropy     | 1.69     |
| policy_loss        | -0.0515  |
| total_timesteps    | 3912000  |
| value_loss         | 0.0054   |
---------------------------------
---------------------------------
| avg reward         | -0.293   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.74     |
| fps                | 2109     |
| learning rate      | 0.001    |
| nupdates           | 49000    |
| policy_entropy     | 1.54     |
| policy_loss        | 0.0896   |
| total_timesteps    | 3920000  |
| value_loss         | 0.0164   |
---------------------------------
---------------------------------
| avg reward         | -0.305   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.824    |
| fps                | 2109     |
| learning rate      | 0.001    |
| nupdates           | 49100    |
| policy_entropy     | 1.65     |
| policy_loss        | -0.0376  |
| total_timesteps    | 3928000  |
| value_loss         | 0.0174   |
---------------------------------
---------------------------------
| avg reward         | -0.0484  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.826    |
| fps                | 2109     |
| learning rate      | 0.001    |
| nupdates           | 49200    |
| policy_entropy     | 1.58     |
| policy_loss        | -0.0163  |
| total_timesteps    | 3936000  |
| value_loss         | 0.0202   |
---------------------------------
---------------------------------
| avg reward         | -0.264   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.872    |
| fps                | 2109     |
| learning rate      | 0.001    |
| nupdates           | 49300    |
| policy_entropy     | 1.56     |
| policy_loss        | -0.0558  |
| total_timesteps    | 3944000  |
| value_loss         | 0.0223   |
---------------------------------
---------------------------------
| avg reward         | -0.111   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.863    |
| fps                | 2109     |
| learning rate      | 0.001    |
| nupdates           | 49400    |
| policy_entropy     | 1.52     |
| policy_loss        | 0.0745   |
| total_timesteps    | 3952000  |
| value_loss         | 0.0147   |
---------------------------------
---------------------------------
| avg reward         | -0.185   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.646    |
| fps                | 2109     |
| learning rate      | 0.001    |
| nupdates           | 49500    |
| policy_entropy     | 1.61     |
| policy_loss        | 0.107    |
| total_timesteps    | 3960000  |
| value_loss         | 0.0241   |
---------------------------------
---------------------------------
| avg reward         | -0.157   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.804    |
| fps                | 2110     |
| learning rate      | 0.001    |
| nupdates           | 49600    |
| policy_entropy     | 1.7      |
| policy_loss        | -0.104   |
| total_timesteps    | 3968000  |
| value_loss         | 0.026    |
---------------------------------
---------------------------------
| avg reward         | -0.0974  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.905    |
| fps                | 2110     |
| learning rate      | 0.001    |
| nupdates           | 49700    |
| policy_entropy     | 1.57     |
| policy_loss        | 0.0125   |
| total_timesteps    | 3976000  |
| value_loss         | 0.0136   |
---------------------------------
---------------------------------
| avg reward         | -0.126   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.919    |
| fps                | 2110     |
| learning rate      | 0.001    |
| nupdates           | 49800    |
| policy_entropy     | 1.6      |
| policy_loss        | 0.0347   |
| total_timesteps    | 3984000  |
| value_loss         | 0.0107   |
---------------------------------
---------------------------------
| avg reward         | -0.0654  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.818    |
| fps                | 2110     |
| learning rate      | 0.001    |
| nupdates           | 49900    |
| policy_entropy     | 1.63     |
| policy_loss        | 0.066    |
| total_timesteps    | 3992000  |
| value_loss         | 0.0209   |
---------------------------------
---------------------------------
| avg reward         | -0.236   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.868    |
| fps                | 2110     |
| learning rate      | 0.001    |
| nupdates           | 50000    |
| policy_entropy     | 1.57     |
| policy_loss        | -0.0175  |
| total_timesteps    | 4000000  |
| value_loss         | 0.0122   |
---------------------------------
---------------------------------
| avg reward         | -0.367   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.541    |
| fps                | 2110     |
| learning rate      | 0.001    |
| nupdates           | 50100    |
| policy_entropy     | 1.71     |
| policy_loss        | -0.023   |
| total_timesteps    | 4008000  |
| value_loss         | 0.0184   |
---------------------------------
---------------------------------
| avg reward         | -0.0878  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.881    |
| fps                | 2110     |
| learning rate      | 0.001    |
| nupdates           | 50200    |
| policy_entropy     | 1.71     |
| policy_loss        | -0.0323  |
| total_timesteps    | 4016000  |
| value_loss         | 0.0148   |
---------------------------------
---------------------------------
| avg reward         | -0.324   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.878    |
| fps                | 2110     |
| learning rate      | 0.001    |
| nupdates           | 50300    |
| policy_entropy     | 1.6      |
| policy_loss        | 0.0855   |
| total_timesteps    | 4024000  |
| value_loss         | 0.0171   |
---------------------------------
---------------------------------
| avg reward         | -0.377   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.805    |
| fps                | 2111     |
| learning rate      | 0.001    |
| nupdates           | 50400    |
| policy_entropy     | 1.47     |
| policy_loss        | -0.068   |
| total_timesteps    | 4032000  |
| value_loss         | 0.0409   |
---------------------------------
---------------------------------
| avg reward         | -0.329   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.688    |
| fps                | 2111     |
| learning rate      | 0.001    |
| nupdates           | 50500    |
| policy_entropy     | 1.61     |
| policy_loss        | -0.00825 |
| total_timesteps    | 4040000  |
| value_loss         | 0.0188   |
---------------------------------
---------------------------------
| avg reward         | -0.139   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.802    |
| fps                | 2111     |
| learning rate      | 0.001    |
| nupdates           | 50600    |
| policy_entropy     | 1.65     |
| policy_loss        | -0.0132  |
| total_timesteps    | 4048000  |
| value_loss         | 0.0137   |
---------------------------------
---------------------------------
| avg reward         | -0.131   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.946    |
| fps                | 2111     |
| learning rate      | 0.001    |
| nupdates           | 50700    |
| policy_entropy     | 1.48     |
| policy_loss        | -0.0141  |
| total_timesteps    | 4056000  |
| value_loss         | 0.00635  |
---------------------------------
---------------------------------
| avg reward         | 0.089    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.828    |
| fps                | 2111     |
| learning rate      | 0.001    |
| nupdates           | 50800    |
| policy_entropy     | 1.47     |
| policy_loss        | 0.00907  |
| total_timesteps    | 4064000  |
| value_loss         | 0.00874  |
---------------------------------
---------------------------------
| avg reward         | -0.129   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.928    |
| fps                | 2111     |
| learning rate      | 0.001    |
| nupdates           | 50900    |
| policy_entropy     | 1.57     |
| policy_loss        | 0.022    |
| total_timesteps    | 4072000  |
| value_loss         | 0.00653  |
---------------------------------
---------------------------------
| avg reward         | -0.169   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.972    |
| fps                | 2112     |
| learning rate      | 0.001    |
| nupdates           | 51000    |
| policy_entropy     | 1.38     |
| policy_loss        | -0.0273  |
| total_timesteps    | 4080000  |
| value_loss         | 0.00453  |
---------------------------------
---------------------------------
| avg reward         | -0.2     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.859    |
| fps                | 2112     |
| learning rate      | 0.001    |
| nupdates           | 51100    |
| policy_entropy     | 1.61     |
| policy_loss        | 0.0882   |
| total_timesteps    | 4088000  |
| value_loss         | 0.0147   |
---------------------------------
---------------------------------
| avg reward         | -0.341   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.943    |
| fps                | 2112     |
| learning rate      | 0.001    |
| nupdates           | 51200    |
| policy_entropy     | 1.39     |
| policy_loss        | -0.0607  |
| total_timesteps    | 4096000  |
| value_loss         | 0.00872  |
---------------------------------
---------------------------------
| avg reward         | -0.0855  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.933    |
| fps                | 2112     |
| learning rate      | 0.001    |
| nupdates           | 51300    |
| policy_entropy     | 1.57     |
| policy_loss        | -0.0371  |
| total_timesteps    | 4104000  |
| value_loss         | 0.00487  |
---------------------------------
---------------------------------
| avg reward         | -0.289   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.756    |
| fps                | 2112     |
| learning rate      | 0.001    |
| nupdates           | 51400    |
| policy_entropy     | 1.55     |
| policy_loss        | -0.0262  |
| total_timesteps    | 4112000  |
| value_loss         | 0.0171   |
---------------------------------
---------------------------------
| avg reward         | -0.166   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.61     |
| fps                | 2112     |
| learning rate      | 0.001    |
| nupdates           | 51500    |
| policy_entropy     | 1.63     |
| policy_loss        | 0.0177   |
| total_timesteps    | 4120000  |
| value_loss         | 0.0515   |
---------------------------------
---------------------------------
| avg reward         | -0.217   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.869    |
| fps                | 2112     |
| learning rate      | 0.001    |
| nupdates           | 51600    |
| policy_entropy     | 1.57     |
| policy_loss        | -0.079   |
| total_timesteps    | 4128000  |
| value_loss         | 0.00914  |
---------------------------------
---------------------------------
| avg reward         | -0.228   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.84     |
| fps                | 2112     |
| learning rate      | 0.001    |
| nupdates           | 51700    |
| policy_entropy     | 1.45     |
| policy_loss        | 0.0315   |
| total_timesteps    | 4136000  |
| value_loss         | 0.0166   |
---------------------------------
---------------------------------
| avg reward         | -0.213   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.903    |
| fps                | 2112     |
| learning rate      | 0.001    |
| nupdates           | 51800    |
| policy_entropy     | 1.63     |
| policy_loss        | -0.0989  |
| total_timesteps    | 4144000  |
| value_loss         | 0.008    |
---------------------------------
---------------------------------
| avg reward         | -0.127   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.908    |
| fps                | 2112     |
| learning rate      | 0.001    |
| nupdates           | 51900    |
| policy_entropy     | 1.69     |
| policy_loss        | -0.0376  |
| total_timesteps    | 4152000  |
| value_loss         | 0.00406  |
---------------------------------
---------------------------------
| avg reward         | -0.216   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.819    |
| fps                | 2113     |
| learning rate      | 0.001    |
| nupdates           | 52000    |
| policy_entropy     | 1.65     |
| policy_loss        | -0.0364  |
| total_timesteps    | 4160000  |
| value_loss         | 0.0144   |
---------------------------------
---------------------------------
| avg reward         | -0.00329 |
| epsilonValue       | 0.000727 |
| explained_variance | 0.909    |
| fps                | 2113     |
| learning rate      | 0.001    |
| nupdates           | 52100    |
| policy_entropy     | 1.61     |
| policy_loss        | -0.0237  |
| total_timesteps    | 4168000  |
| value_loss         | 0.00848  |
---------------------------------
---------------------------------
| avg reward         | 0.0813   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.887    |
| fps                | 2113     |
| learning rate      | 0.001    |
| nupdates           | 52200    |
| policy_entropy     | 1.55     |
| policy_loss        | 0.0112   |
| total_timesteps    | 4176000  |
| value_loss         | 0.00929  |
---------------------------------
---------------------------------
| avg reward         | 0.036    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.947    |
| fps                | 2113     |
| learning rate      | 0.001    |
| nupdates           | 52300    |
| policy_entropy     | 1.57     |
| policy_loss        | -0.0172  |
| total_timesteps    | 4184000  |
| value_loss         | 0.00816  |
---------------------------------
---------------------------------
| avg reward         | -0.0985  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.806    |
| fps                | 2113     |
| learning rate      | 0.001    |
| nupdates           | 52400    |
| policy_entropy     | 1.45     |
| policy_loss        | -0.103   |
| total_timesteps    | 4192000  |
| value_loss         | 0.0403   |
---------------------------------
---------------------------------
| avg reward         | -0.292   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.741    |
| fps                | 2113     |
| learning rate      | 0.001    |
| nupdates           | 52500    |
| policy_entropy     | 1.52     |
| policy_loss        | 0.124    |
| total_timesteps    | 4200000  |
| value_loss         | 0.0217   |
---------------------------------
---------------------------------
| avg reward         | 0.169    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.892    |
| fps                | 2113     |
| learning rate      | 0.001    |
| nupdates           | 52600    |
| policy_entropy     | 1.55     |
| policy_loss        | -0.0727  |
| total_timesteps    | 4208000  |
| value_loss         | 0.0176   |
---------------------------------
---------------------------------
| avg reward         | -0.165   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.849    |
| fps                | 2114     |
| learning rate      | 0.001    |
| nupdates           | 52700    |
| policy_entropy     | 1.45     |
| policy_loss        | 0.0137   |
| total_timesteps    | 4216000  |
| value_loss         | 0.011    |
---------------------------------
---------------------------------
| avg reward         | 0.134    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.903    |
| fps                | 2114     |
| learning rate      | 0.001    |
| nupdates           | 52800    |
| policy_entropy     | 1.54     |
| policy_loss        | -0.0312  |
| total_timesteps    | 4224000  |
| value_loss         | 0.0123   |
---------------------------------
---------------------------------
| avg reward         | -0.0915  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.83     |
| fps                | 2114     |
| learning rate      | 0.001    |
| nupdates           | 52900    |
| policy_entropy     | 1.65     |
| policy_loss        | -0.031   |
| total_timesteps    | 4232000  |
| value_loss         | 0.0105   |
---------------------------------
---------------------------------
| avg reward         | -0.00579 |
| epsilonValue       | 0.000727 |
| explained_variance | 0.916    |
| fps                | 2114     |
| learning rate      | 0.001    |
| nupdates           | 53000    |
| policy_entropy     | 1.52     |
| policy_loss        | 0.112    |
| total_timesteps    | 4240000  |
| value_loss         | 0.0107   |
---------------------------------
---------------------------------
| avg reward         | -0.184   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.891    |
| fps                | 2114     |
| learning rate      | 0.001    |
| nupdates           | 53100    |
| policy_entropy     | 1.62     |
| policy_loss        | 0.0272   |
| total_timesteps    | 4248000  |
| value_loss         | 0.00397  |
---------------------------------
---------------------------------
| avg reward         | -0.261   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.838    |
| fps                | 2114     |
| learning rate      | 0.001    |
| nupdates           | 53200    |
| policy_entropy     | 1.62     |
| policy_loss        | -0.115   |
| total_timesteps    | 4256000  |
| value_loss         | 0.0193   |
---------------------------------
---------------------------------
| avg reward         | -0.051   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.804    |
| fps                | 2114     |
| learning rate      | 0.001    |
| nupdates           | 53300    |
| policy_entropy     | 1.64     |
| policy_loss        | 0.0514   |
| total_timesteps    | 4264000  |
| value_loss         | 0.0251   |
---------------------------------
---------------------------------
| avg reward         | -0.247   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.906    |
| fps                | 2115     |
| learning rate      | 0.001    |
| nupdates           | 53400    |
| policy_entropy     | 1.66     |
| policy_loss        | -0.0575  |
| total_timesteps    | 4272000  |
| value_loss         | 0.00991  |
---------------------------------
---------------------------------
| avg reward         | -0.442   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.946    |
| fps                | 2115     |
| learning rate      | 0.001    |
| nupdates           | 53500    |
| policy_entropy     | 1.53     |
| policy_loss        | 0.0305   |
| total_timesteps    | 4280000  |
| value_loss         | 0.00401  |
---------------------------------
---------------------------------
| avg reward         | 0.0735   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.864    |
| fps                | 2114     |
| learning rate      | 0.001    |
| nupdates           | 53600    |
| policy_entropy     | 1.43     |
| policy_loss        | -0.0756  |
| total_timesteps    | 4288000  |
| value_loss         | 0.0257   |
---------------------------------
---------------------------------
| avg reward         | -0.0843  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.915    |
| fps                | 2114     |
| learning rate      | 0.001    |
| nupdates           | 53700    |
| policy_entropy     | 1.64     |
| policy_loss        | -0.0447  |
| total_timesteps    | 4296000  |
| value_loss         | 0.0106   |
---------------------------------
---------------------------------
| avg reward         | -0.0822  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.728    |
| fps                | 2115     |
| learning rate      | 0.001    |
| nupdates           | 53800    |
| policy_entropy     | 1.57     |
| policy_loss        | 0.00979  |
| total_timesteps    | 4304000  |
| value_loss         | 0.00395  |
---------------------------------
---------------------------------
| avg reward         | -0.207   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.867    |
| fps                | 2115     |
| learning rate      | 0.001    |
| nupdates           | 53900    |
| policy_entropy     | 1.54     |
| policy_loss        | -0.106   |
| total_timesteps    | 4312000  |
| value_loss         | 0.01     |
---------------------------------
---------------------------------
| avg reward         | -0.312   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.815    |
| fps                | 2115     |
| learning rate      | 0.001    |
| nupdates           | 54000    |
| policy_entropy     | 1.53     |
| policy_loss        | 0.022    |
| total_timesteps    | 4320000  |
| value_loss         | 0.0222   |
---------------------------------
---------------------------------
| avg reward         | -0.0599  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.844    |
| fps                | 2115     |
| learning rate      | 0.001    |
| nupdates           | 54100    |
| policy_entropy     | 1.6      |
| policy_loss        | -0.0236  |
| total_timesteps    | 4328000  |
| value_loss         | 0.0115   |
---------------------------------
---------------------------------
| avg reward         | -0.0509  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.924    |
| fps                | 2115     |
| learning rate      | 0.001    |
| nupdates           | 54200    |
| policy_entropy     | 1.57     |
| policy_loss        | 0.028    |
| total_timesteps    | 4336000  |
| value_loss         | 0.00802  |
---------------------------------
---------------------------------
| avg reward         | -0.203   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.357    |
| fps                | 2115     |
| learning rate      | 0.001    |
| nupdates           | 54300    |
| policy_entropy     | 1.62     |
| policy_loss        | -0.0403  |
| total_timesteps    | 4344000  |
| value_loss         | 0.0191   |
---------------------------------
---------------------------------
| avg reward         | -0.212   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.736    |
| fps                | 2115     |
| learning rate      | 0.001    |
| nupdates           | 54400    |
| policy_entropy     | 1.62     |
| policy_loss        | 0.0459   |
| total_timesteps    | 4352000  |
| value_loss         | 0.0197   |
---------------------------------
---------------------------------
| avg reward         | -0.00258 |
| epsilonValue       | 0.000727 |
| explained_variance | 0.818    |
| fps                | 2115     |
| learning rate      | 0.001    |
| nupdates           | 54500    |
| policy_entropy     | 1.62     |
| policy_loss        | 0.0651   |
| total_timesteps    | 4360000  |
| value_loss         | 0.018    |
---------------------------------
---------------------------------
| avg reward         | -0.145   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.842    |
| fps                | 2116     |
| learning rate      | 0.001    |
| nupdates           | 54600    |
| policy_entropy     | 1.62     |
| policy_loss        | 0.00875  |
| total_timesteps    | 4368000  |
| value_loss         | 0.014    |
---------------------------------
---------------------------------
| avg reward         | -0.163   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.91     |
| fps                | 2116     |
| learning rate      | 0.001    |
| nupdates           | 54700    |
| policy_entropy     | 1.43     |
| policy_loss        | -0.0335  |
| total_timesteps    | 4376000  |
| value_loss         | 0.00914  |
---------------------------------
---------------------------------
| avg reward         | 0.0235   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.817    |
| fps                | 2116     |
| learning rate      | 0.001    |
| nupdates           | 54800    |
| policy_entropy     | 1.6      |
| policy_loss        | 0.0438   |
| total_timesteps    | 4384000  |
| value_loss         | 0.0167   |
---------------------------------
---------------------------------
| avg reward         | -0.187   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.642    |
| fps                | 2116     |
| learning rate      | 0.001    |
| nupdates           | 54900    |
| policy_entropy     | 1.51     |
| policy_loss        | -0.0236  |
| total_timesteps    | 4392000  |
| value_loss         | 0.0293   |
---------------------------------
---------------------------------
| avg reward         | -0.0213  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.837    |
| fps                | 2116     |
| learning rate      | 0.001    |
| nupdates           | 55000    |
| policy_entropy     | 1.65     |
| policy_loss        | 0.0174   |
| total_timesteps    | 4400000  |
| value_loss         | 0.0189   |
---------------------------------
---------------------------------
| avg reward         | 0.12     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.893    |
| fps                | 2116     |
| learning rate      | 0.001    |
| nupdates           | 55100    |
| policy_entropy     | 1.53     |
| policy_loss        | 0.0544   |
| total_timesteps    | 4408000  |
| value_loss         | 0.0132   |
---------------------------------
---------------------------------
| avg reward         | 0.0521   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.924    |
| fps                | 2116     |
| learning rate      | 0.001    |
| nupdates           | 55200    |
| policy_entropy     | 1.52     |
| policy_loss        | -0.0406  |
| total_timesteps    | 4416000  |
| value_loss         | 0.00811  |
---------------------------------
---------------------------------
| avg reward         | 0.094    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.808    |
| fps                | 2116     |
| learning rate      | 0.001    |
| nupdates           | 55300    |
| policy_entropy     | 1.48     |
| policy_loss        | 0.0101   |
| total_timesteps    | 4424000  |
| value_loss         | 0.0204   |
---------------------------------
---------------------------------
| avg reward         | -0.0369  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.887    |
| fps                | 2116     |
| learning rate      | 0.001    |
| nupdates           | 55400    |
| policy_entropy     | 1.53     |
| policy_loss        | -0.0193  |
| total_timesteps    | 4432000  |
| value_loss         | 0.0124   |
---------------------------------
---------------------------------
| avg reward         | -0.0657  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.869    |
| fps                | 2117     |
| learning rate      | 0.001    |
| nupdates           | 55500    |
| policy_entropy     | 1.49     |
| policy_loss        | -0.0932  |
| total_timesteps    | 4440000  |
| value_loss         | 0.0237   |
---------------------------------
---------------------------------
| avg reward         | -0.163   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.953    |
| fps                | 2117     |
| learning rate      | 0.001    |
| nupdates           | 55600    |
| policy_entropy     | 1.67     |
| policy_loss        | -0.0509  |
| total_timesteps    | 4448000  |
| value_loss         | 0.0068   |
---------------------------------
---------------------------------
| avg reward         | -0.0138  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.803    |
| fps                | 2117     |
| learning rate      | 0.001    |
| nupdates           | 55700    |
| policy_entropy     | 1.68     |
| policy_loss        | -0.0382  |
| total_timesteps    | 4456000  |
| value_loss         | 0.023    |
---------------------------------
---------------------------------
| avg reward         | -0.0933  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.839    |
| fps                | 2117     |
| learning rate      | 0.001    |
| nupdates           | 55800    |
| policy_entropy     | 1.63     |
| policy_loss        | -0.0459  |
| total_timesteps    | 4464000  |
| value_loss         | 0.0148   |
---------------------------------
---------------------------------
| avg reward         | -0.154   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.927    |
| fps                | 2117     |
| learning rate      | 0.001    |
| nupdates           | 55900    |
| policy_entropy     | 1.65     |
| policy_loss        | 0.0531   |
| total_timesteps    | 4472000  |
| value_loss         | 0.00744  |
---------------------------------
---------------------------------
| avg reward         | 0.0307   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.925    |
| fps                | 2117     |
| learning rate      | 0.001    |
| nupdates           | 56000    |
| policy_entropy     | 1.5      |
| policy_loss        | 0.119    |
| total_timesteps    | 4480000  |
| value_loss         | 0.0102   |
---------------------------------
---------------------------------
| avg reward         | 0.118    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.965    |
| fps                | 2117     |
| learning rate      | 0.001    |
| nupdates           | 56100    |
| policy_entropy     | 1.59     |
| policy_loss        | -0.0168  |
| total_timesteps    | 4488000  |
| value_loss         | 0.00527  |
---------------------------------
---------------------------------
| avg reward         | 0.0648   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.806    |
| fps                | 2117     |
| learning rate      | 0.001    |
| nupdates           | 56200    |
| policy_entropy     | 1.55     |
| policy_loss        | 0.0546   |
| total_timesteps    | 4496000  |
| value_loss         | 0.00904  |
---------------------------------
---------------------------------
| avg reward         | 0.0446   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.804    |
| fps                | 2117     |
| learning rate      | 0.001    |
| nupdates           | 56300    |
| policy_entropy     | 1.49     |
| policy_loss        | -0.0463  |
| total_timesteps    | 4504000  |
| value_loss         | 0.00865  |
---------------------------------
---------------------------------
| avg reward         | -0.00157 |
| epsilonValue       | 0.000727 |
| explained_variance | 0.61     |
| fps                | 2117     |
| learning rate      | 0.001    |
| nupdates           | 56400    |
| policy_entropy     | 1.48     |
| policy_loss        | -0.0446  |
| total_timesteps    | 4512000  |
| value_loss         | 0.0179   |
---------------------------------
---------------------------------
| avg reward         | 0.0456   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.794    |
| fps                | 2117     |
| learning rate      | 0.001    |
| nupdates           | 56500    |
| policy_entropy     | 1.59     |
| policy_loss        | -0.0339  |
| total_timesteps    | 4520000  |
| value_loss         | 0.00864  |
---------------------------------
---------------------------------
| avg reward         | 0.167    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.703    |
| fps                | 2117     |
| learning rate      | 0.001    |
| nupdates           | 56600    |
| policy_entropy     | 1.59     |
| policy_loss        | 0.0819   |
| total_timesteps    | 4528000  |
| value_loss         | 0.015    |
---------------------------------
---------------------------------
| avg reward         | -0.0937  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.782    |
| fps                | 2118     |
| learning rate      | 0.001    |
| nupdates           | 56700    |
| policy_entropy     | 1.67     |
| policy_loss        | -0.0386  |
| total_timesteps    | 4536000  |
| value_loss         | 0.00983  |
---------------------------------
---------------------------------
| avg reward         | 0.0663   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.893    |
| fps                | 2118     |
| learning rate      | 0.001    |
| nupdates           | 56800    |
| policy_entropy     | 1.54     |
| policy_loss        | 0.0596   |
| total_timesteps    | 4544000  |
| value_loss         | 0.0124   |
---------------------------------
---------------------------------
| avg reward         | 0.0931   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.918    |
| fps                | 2118     |
| learning rate      | 0.001    |
| nupdates           | 56900    |
| policy_entropy     | 1.56     |
| policy_loss        | 0.0358   |
| total_timesteps    | 4552000  |
| value_loss         | 0.00881  |
---------------------------------
---------------------------------
| avg reward         | 0.106    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.933    |
| fps                | 2118     |
| learning rate      | 0.001    |
| nupdates           | 57000    |
| policy_entropy     | 1.66     |
| policy_loss        | -0.0659  |
| total_timesteps    | 4560000  |
| value_loss         | 0.00711  |
---------------------------------
---------------------------------
| avg reward         | -0.137   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.797    |
| fps                | 2118     |
| learning rate      | 0.001    |
| nupdates           | 57100    |
| policy_entropy     | 1.59     |
| policy_loss        | -0.0853  |
| total_timesteps    | 4568000  |
| value_loss         | 0.0162   |
---------------------------------
---------------------------------
| avg reward         | -0.218   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.804    |
| fps                | 2118     |
| learning rate      | 0.001    |
| nupdates           | 57200    |
| policy_entropy     | 1.62     |
| policy_loss        | -0.011   |
| total_timesteps    | 4576000  |
| value_loss         | 0.0161   |
---------------------------------
---------------------------------
| avg reward         | -0.215   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.688    |
| fps                | 2118     |
| learning rate      | 0.001    |
| nupdates           | 57300    |
| policy_entropy     | 1.53     |
| policy_loss        | -0.0542  |
| total_timesteps    | 4584000  |
| value_loss         | 0.0225   |
---------------------------------
---------------------------------
| avg reward         | -0.157   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.8      |
| fps                | 2118     |
| learning rate      | 0.001    |
| nupdates           | 57400    |
| policy_entropy     | 1.58     |
| policy_loss        | -0.0227  |
| total_timesteps    | 4592000  |
| value_loss         | 0.0163   |
---------------------------------
---------------------------------
| avg reward         | 0.0881   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.907    |
| fps                | 2118     |
| learning rate      | 0.001    |
| nupdates           | 57500    |
| policy_entropy     | 1.65     |
| policy_loss        | 0.113    |
| total_timesteps    | 4600000  |
| value_loss         | 0.0147   |
---------------------------------
---------------------------------
| avg reward         | 0.0903   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.793    |
| fps                | 2119     |
| learning rate      | 0.001    |
| nupdates           | 57600    |
| policy_entropy     | 1.65     |
| policy_loss        | -0.0453  |
| total_timesteps    | 4608000  |
| value_loss         | 0.0185   |
---------------------------------
---------------------------------
| avg reward         | 0.179    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.813    |
| fps                | 2119     |
| learning rate      | 0.001    |
| nupdates           | 57700    |
| policy_entropy     | 1.65     |
| policy_loss        | 0.144    |
| total_timesteps    | 4616000  |
| value_loss         | 0.0218   |
---------------------------------
---------------------------------
| avg reward         | -0.0633  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.508    |
| fps                | 2119     |
| learning rate      | 0.001    |
| nupdates           | 57800    |
| policy_entropy     | 1.57     |
| policy_loss        | 0.039    |
| total_timesteps    | 4624000  |
| value_loss         | 0.0567   |
---------------------------------
---------------------------------
| avg reward         | -0.0401  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.726    |
| fps                | 2119     |
| learning rate      | 0.001    |
| nupdates           | 57900    |
| policy_entropy     | 1.67     |
| policy_loss        | 0.194    |
| total_timesteps    | 4632000  |
| value_loss         | 0.0425   |
---------------------------------
---------------------------------
| avg reward         | -0.156   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.817    |
| fps                | 2119     |
| learning rate      | 0.001    |
| nupdates           | 58000    |
| policy_entropy     | 1.66     |
| policy_loss        | -0.0478  |
| total_timesteps    | 4640000  |
| value_loss         | 0.00357  |
---------------------------------
---------------------------------
| avg reward         | -0.171   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.796    |
| fps                | 2119     |
| learning rate      | 0.001    |
| nupdates           | 58100    |
| policy_entropy     | 1.63     |
| policy_loss        | 0.0384   |
| total_timesteps    | 4648000  |
| value_loss         | 0.00728  |
---------------------------------
---------------------------------
| avg reward         | 0.0548   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.914    |
| fps                | 2119     |
| learning rate      | 0.001    |
| nupdates           | 58200    |
| policy_entropy     | 1.56     |
| policy_loss        | 0.0388   |
| total_timesteps    | 4656000  |
| value_loss         | 0.0124   |
---------------------------------
---------------------------------
| avg reward         | 0.136    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.785    |
| fps                | 2119     |
| learning rate      | 0.001    |
| nupdates           | 58300    |
| policy_entropy     | 1.6      |
| policy_loss        | 0.00436  |
| total_timesteps    | 4664000  |
| value_loss         | 0.0286   |
---------------------------------
---------------------------------
| avg reward         | -0.156   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.928    |
| fps                | 2119     |
| learning rate      | 0.001    |
| nupdates           | 58400    |
| policy_entropy     | 1.69     |
| policy_loss        | 0.01     |
| total_timesteps    | 4672000  |
| value_loss         | 0.00578  |
---------------------------------
---------------------------------
| avg reward         | -0.0351  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.852    |
| fps                | 2120     |
| learning rate      | 0.001    |
| nupdates           | 58500    |
| policy_entropy     | 1.6      |
| policy_loss        | 0.0451   |
| total_timesteps    | 4680000  |
| value_loss         | 0.013    |
---------------------------------
---------------------------------
| avg reward         | -0.0182  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.865    |
| fps                | 2120     |
| learning rate      | 0.001    |
| nupdates           | 58600    |
| policy_entropy     | 1.58     |
| policy_loss        | 0.0259   |
| total_timesteps    | 4688000  |
| value_loss         | 0.0139   |
---------------------------------
---------------------------------
| avg reward         | -0.135   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.898    |
| fps                | 2120     |
| learning rate      | 0.001    |
| nupdates           | 58700    |
| policy_entropy     | 1.66     |
| policy_loss        | 0.0251   |
| total_timesteps    | 4696000  |
| value_loss         | 0.00818  |
---------------------------------
---------------------------------
| avg reward         | 0.293    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.936    |
| fps                | 2120     |
| learning rate      | 0.001    |
| nupdates           | 58800    |
| policy_entropy     | 1.62     |
| policy_loss        | -0.0231  |
| total_timesteps    | 4704000  |
| value_loss         | 0.00952  |
---------------------------------
---------------------------------
| avg reward         | 0.198    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.746    |
| fps                | 2120     |
| learning rate      | 0.001    |
| nupdates           | 58900    |
| policy_entropy     | 1.68     |
| policy_loss        | -0.0173  |
| total_timesteps    | 4712000  |
| value_loss         | 0.0402   |
---------------------------------
---------------------------------
| avg reward         | -0.12    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.881    |
| fps                | 2120     |
| learning rate      | 0.001    |
| nupdates           | 59000    |
| policy_entropy     | 1.71     |
| policy_loss        | -0.045   |
| total_timesteps    | 4720000  |
| value_loss         | 0.0133   |
---------------------------------
---------------------------------
| avg reward         | 0.097    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.974    |
| fps                | 2121     |
| learning rate      | 0.001    |
| nupdates           | 59100    |
| policy_entropy     | 1.65     |
| policy_loss        | 0.0123   |
| total_timesteps    | 4728000  |
| value_loss         | 0.00213  |
---------------------------------
---------------------------------
| avg reward         | -0.0887  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.51     |
| fps                | 2121     |
| learning rate      | 0.001    |
| nupdates           | 59200    |
| policy_entropy     | 1.61     |
| policy_loss        | -0.0918  |
| total_timesteps    | 4736000  |
| value_loss         | 0.0241   |
---------------------------------
---------------------------------
| avg reward         | 0.109    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.949    |
| fps                | 2121     |
| learning rate      | 0.001    |
| nupdates           | 59300    |
| policy_entropy     | 1.6      |
| policy_loss        | 0.0871   |
| total_timesteps    | 4744000  |
| value_loss         | 0.00619  |
---------------------------------
---------------------------------
| avg reward         | 0.0266   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.917    |
| fps                | 2121     |
| learning rate      | 0.001    |
| nupdates           | 59400    |
| policy_entropy     | 1.57     |
| policy_loss        | -0.07    |
| total_timesteps    | 4752000  |
| value_loss         | 0.00589  |
---------------------------------
---------------------------------
| avg reward         | -0.398   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.719    |
| fps                | 2121     |
| learning rate      | 0.001    |
| nupdates           | 59500    |
| policy_entropy     | 1.62     |
| policy_loss        | -0.112   |
| total_timesteps    | 4760000  |
| value_loss         | 0.0173   |
---------------------------------
---------------------------------
| avg reward         | -0.00939 |
| epsilonValue       | 0.000727 |
| explained_variance | 0.877    |
| fps                | 2121     |
| learning rate      | 0.001    |
| nupdates           | 59600    |
| policy_entropy     | 1.6      |
| policy_loss        | 0.0121   |
| total_timesteps    | 4768000  |
| value_loss         | 0.0149   |
---------------------------------
---------------------------------
| avg reward         | 0.0629   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.844    |
| fps                | 2121     |
| learning rate      | 0.001    |
| nupdates           | 59700    |
| policy_entropy     | 1.54     |
| policy_loss        | 0.0094   |
| total_timesteps    | 4776000  |
| value_loss         | 0.0155   |
---------------------------------
---------------------------------
| avg reward         | 0.306    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.803    |
| fps                | 2121     |
| learning rate      | 0.001    |
| nupdates           | 59800    |
| policy_entropy     | 1.52     |
| policy_loss        | 0.125    |
| total_timesteps    | 4784000  |
| value_loss         | 0.0348   |
---------------------------------
---------------------------------
| avg reward         | 0.144    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.938    |
| fps                | 2121     |
| learning rate      | 0.001    |
| nupdates           | 59900    |
| policy_entropy     | 1.54     |
| policy_loss        | -0.0316  |
| total_timesteps    | 4792000  |
| value_loss         | 0.00796  |
---------------------------------
---------------------------------
| avg reward         | 0.0969   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.766    |
| fps                | 2121     |
| learning rate      | 0.001    |
| nupdates           | 60000    |
| policy_entropy     | 1.47     |
| policy_loss        | -0.182   |
| total_timesteps    | 4800000  |
| value_loss         | 0.0349   |
---------------------------------
---------------------------------
| avg reward         | -0.0982  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.899    |
| fps                | 2121     |
| learning rate      | 0.001    |
| nupdates           | 60100    |
| policy_entropy     | 1.47     |
| policy_loss        | -0.0308  |
| total_timesteps    | 4808000  |
| value_loss         | 0.0128   |
---------------------------------
---------------------------------
| avg reward         | -0.0435  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.852    |
| fps                | 2121     |
| learning rate      | 0.001    |
| nupdates           | 60200    |
| policy_entropy     | 1.59     |
| policy_loss        | 0.00665  |
| total_timesteps    | 4816000  |
| value_loss         | 0.0271   |
---------------------------------
---------------------------------
| avg reward         | 0.0883   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.902    |
| fps                | 2121     |
| learning rate      | 0.001    |
| nupdates           | 60300    |
| policy_entropy     | 1.62     |
| policy_loss        | -0.0289  |
| total_timesteps    | 4824000  |
| value_loss         | 0.0164   |
---------------------------------
---------------------------------
| avg reward         | -0.00972 |
| epsilonValue       | 0.000727 |
| explained_variance | 0.941    |
| fps                | 2122     |
| learning rate      | 0.001    |
| nupdates           | 60400    |
| policy_entropy     | 1.55     |
| policy_loss        | -0.0295  |
| total_timesteps    | 4832000  |
| value_loss         | 0.00806  |
---------------------------------
---------------------------------
| avg reward         | 0.0444   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.966    |
| fps                | 2122     |
| learning rate      | 0.001    |
| nupdates           | 60500    |
| policy_entropy     | 1.59     |
| policy_loss        | -0.0333  |
| total_timesteps    | 4840000  |
| value_loss         | 0.0032   |
---------------------------------
---------------------------------
| avg reward         | 0.168    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.934    |
| fps                | 2122     |
| learning rate      | 0.001    |
| nupdates           | 60600    |
| policy_entropy     | 1.67     |
| policy_loss        | -0.0153  |
| total_timesteps    | 4848000  |
| value_loss         | 0.00496  |
---------------------------------
---------------------------------
| avg reward         | -0.0238  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.914    |
| fps                | 2122     |
| learning rate      | 0.001    |
| nupdates           | 60700    |
| policy_entropy     | 1.61     |
| policy_loss        | -0.0515  |
| total_timesteps    | 4856000  |
| value_loss         | 0.00749  |
---------------------------------
---------------------------------
| avg reward         | 0.198    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.887    |
| fps                | 2122     |
| learning rate      | 0.001    |
| nupdates           | 60800    |
| policy_entropy     | 1.48     |
| policy_loss        | -0.0542  |
| total_timesteps    | 4864000  |
| value_loss         | 0.0113   |
---------------------------------
---------------------------------
| avg reward         | -0.0587  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.922    |
| fps                | 2122     |
| learning rate      | 0.001    |
| nupdates           | 60900    |
| policy_entropy     | 1.72     |
| policy_loss        | 0.00793  |
| total_timesteps    | 4872000  |
| value_loss         | 0.0116   |
---------------------------------
---------------------------------
| avg reward         | -0.154   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.789    |
| fps                | 2122     |
| learning rate      | 0.001    |
| nupdates           | 61000    |
| policy_entropy     | 1.5      |
| policy_loss        | -0.0407  |
| total_timesteps    | 4880000  |
| value_loss         | 0.00561  |
---------------------------------
---------------------------------
| avg reward         | -0.0755  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.796    |
| fps                | 2122     |
| learning rate      | 0.001    |
| nupdates           | 61100    |
| policy_entropy     | 1.65     |
| policy_loss        | 0.00185  |
| total_timesteps    | 4888000  |
| value_loss         | 0.00374  |
---------------------------------
---------------------------------
| avg reward         | -0.117   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.822    |
| fps                | 2122     |
| learning rate      | 0.001    |
| nupdates           | 61200    |
| policy_entropy     | 1.52     |
| policy_loss        | 0.139    |
| total_timesteps    | 4896000  |
| value_loss         | 0.0192   |
---------------------------------
---------------------------------
| avg reward         | -0.249   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.903    |
| fps                | 2123     |
| learning rate      | 0.001    |
| nupdates           | 61300    |
| policy_entropy     | 1.61     |
| policy_loss        | 0.0116   |
| total_timesteps    | 4904000  |
| value_loss         | 0.00501  |
---------------------------------
---------------------------------
| avg reward         | -0.0213  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.894    |
| fps                | 2123     |
| learning rate      | 0.001    |
| nupdates           | 61400    |
| policy_entropy     | 1.6      |
| policy_loss        | 0.0152   |
| total_timesteps    | 4912000  |
| value_loss         | 0.00932  |
---------------------------------
---------------------------------
| avg reward         | -0.282   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.476    |
| fps                | 2123     |
| learning rate      | 0.001    |
| nupdates           | 61500    |
| policy_entropy     | 1.63     |
| policy_loss        | -0.246   |
| total_timesteps    | 4920000  |
| value_loss         | 0.0582   |
---------------------------------
---------------------------------
| avg reward         | -0.0135  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.751    |
| fps                | 2123     |
| learning rate      | 0.001    |
| nupdates           | 61600    |
| policy_entropy     | 1.61     |
| policy_loss        | 0.0778   |
| total_timesteps    | 4928000  |
| value_loss         | 0.0163   |
---------------------------------
---------------------------------
| avg reward         | -0.119   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.843    |
| fps                | 2123     |
| learning rate      | 0.001    |
| nupdates           | 61700    |
| policy_entropy     | 1.7      |
| policy_loss        | -0.076   |
| total_timesteps    | 4936000  |
| value_loss         | 0.0117   |
---------------------------------
---------------------------------
| avg reward         | 0.232    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.947    |
| fps                | 2123     |
| learning rate      | 0.001    |
| nupdates           | 61800    |
| policy_entropy     | 1.45     |
| policy_loss        | 0.017    |
| total_timesteps    | 4944000  |
| value_loss         | 0.00587  |
---------------------------------
---------------------------------
| avg reward         | 0.127    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.776    |
| fps                | 2123     |
| learning rate      | 0.001    |
| nupdates           | 61900    |
| policy_entropy     | 1.57     |
| policy_loss        | -0.0623  |
| total_timesteps    | 4952000  |
| value_loss         | 0.0188   |
---------------------------------
---------------------------------
| avg reward         | 0.059    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.816    |
| fps                | 2123     |
| learning rate      | 0.001    |
| nupdates           | 62000    |
| policy_entropy     | 1.61     |
| policy_loss        | -0.0311  |
| total_timesteps    | 4960000  |
| value_loss         | 0.0133   |
---------------------------------
---------------------------------
| avg reward         | 0.319    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.919    |
| fps                | 2123     |
| learning rate      | 0.001    |
| nupdates           | 62100    |
| policy_entropy     | 1.62     |
| policy_loss        | 0.0726   |
| total_timesteps    | 4968000  |
| value_loss         | 0.0105   |
---------------------------------
---------------------------------
| avg reward         | -0.0771  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.837    |
| fps                | 2123     |
| learning rate      | 0.001    |
| nupdates           | 62200    |
| policy_entropy     | 1.5      |
| policy_loss        | -0.0624  |
| total_timesteps    | 4976000  |
| value_loss         | 0.0129   |
---------------------------------
---------------------------------
| avg reward         | 0.00371  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.934    |
| fps                | 2124     |
| learning rate      | 0.001    |
| nupdates           | 62300    |
| policy_entropy     | 1.53     |
| policy_loss        | -0.0117  |
| total_timesteps    | 4984000  |
| value_loss         | 0.00551  |
---------------------------------
---------------------------------
| avg reward         | -0.25    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.795    |
| fps                | 2124     |
| learning rate      | 0.001    |
| nupdates           | 62400    |
| policy_entropy     | 1.56     |
| policy_loss        | -0.0497  |
| total_timesteps    | 4992000  |
| value_loss         | 0.00751  |
---------------------------------
---------------------------------
| avg reward         | -0.0144  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.917    |
| fps                | 2124     |
| learning rate      | 0.001    |
| nupdates           | 62500    |
| policy_entropy     | 1.68     |
| policy_loss        | -0.0318  |
| total_timesteps    | 5000000  |
| value_loss         | 0.00542  |
---------------------------------
---------------------------------
| avg reward         | -0.051   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.864    |
| fps                | 2124     |
| learning rate      | 0.001    |
| nupdates           | 62600    |
| policy_entropy     | 1.5      |
| policy_loss        | -0.0143  |
| total_timesteps    | 5008000  |
| value_loss         | 0.00983  |
---------------------------------
---------------------------------
| avg reward         | 0.221    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.938    |
| fps                | 2124     |
| learning rate      | 0.001    |
| nupdates           | 62700    |
| policy_entropy     | 1.62     |
| policy_loss        | -0.0186  |
| total_timesteps    | 5016000  |
| value_loss         | 0.00833  |
---------------------------------
---------------------------------
| avg reward         | 0.258    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.87     |
| fps                | 2124     |
| learning rate      | 0.001    |
| nupdates           | 62800    |
| policy_entropy     | 1.62     |
| policy_loss        | -0.0117  |
| total_timesteps    | 5024000  |
| value_loss         | 0.0185   |
---------------------------------
---------------------------------
| avg reward         | -0.112   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.787    |
| fps                | 2124     |
| learning rate      | 0.001    |
| nupdates           | 62900    |
| policy_entropy     | 1.58     |
| policy_loss        | 0.0372   |
| total_timesteps    | 5032000  |
| value_loss         | 0.00461  |
---------------------------------
---------------------------------
| avg reward         | 0.0237   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.916    |
| fps                | 2124     |
| learning rate      | 0.001    |
| nupdates           | 63000    |
| policy_entropy     | 1.64     |
| policy_loss        | 0.0193   |
| total_timesteps    | 5040000  |
| value_loss         | 0.0078   |
---------------------------------
---------------------------------
| avg reward         | 0.195    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.935    |
| fps                | 2124     |
| learning rate      | 0.001    |
| nupdates           | 63100    |
| policy_entropy     | 1.47     |
| policy_loss        | -0.0412  |
| total_timesteps    | 5048000  |
| value_loss         | 0.00853  |
---------------------------------
---------------------------------
| avg reward         | 0.184    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.906    |
| fps                | 2125     |
| learning rate      | 0.001    |
| nupdates           | 63200    |
| policy_entropy     | 1.53     |
| policy_loss        | -0.0526  |
| total_timesteps    | 5056000  |
| value_loss         | 0.00696  |
---------------------------------
---------------------------------
| avg reward         | 0.141    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.886    |
| fps                | 2125     |
| learning rate      | 0.001    |
| nupdates           | 63300    |
| policy_entropy     | 1.54     |
| policy_loss        | 0.0411   |
| total_timesteps    | 5064000  |
| value_loss         | 0.0105   |
---------------------------------
---------------------------------
| avg reward         | -0.0415  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.841    |
| fps                | 2125     |
| learning rate      | 0.001    |
| nupdates           | 63400    |
| policy_entropy     | 1.43     |
| policy_loss        | 0.0168   |
| total_timesteps    | 5072000  |
| value_loss         | 0.0166   |
---------------------------------
----------------------------------
| avg reward         | 0.14      |
| epsilonValue       | 0.000727  |
| explained_variance | 0.924     |
| fps                | 2125      |
| learning rate      | 0.001     |
| nupdates           | 63500     |
| policy_entropy     | 1.64      |
| policy_loss        | -0.000729 |
| total_timesteps    | 5080000   |
| value_loss         | 0.0138    |
----------------------------------
---------------------------------
| avg reward         | 0.0745   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.888    |
| fps                | 2125     |
| learning rate      | 0.001    |
| nupdates           | 63600    |
| policy_entropy     | 1.5      |
| policy_loss        | -0.06    |
| total_timesteps    | 5088000  |
| value_loss         | 0.00868  |
---------------------------------
---------------------------------
| avg reward         | 0.022    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.867    |
| fps                | 2125     |
| learning rate      | 0.001    |
| nupdates           | 63700    |
| policy_entropy     | 1.53     |
| policy_loss        | -0.0257  |
| total_timesteps    | 5096000  |
| value_loss         | 0.0101   |
---------------------------------
---------------------------------
| avg reward         | -0.046   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.881    |
| fps                | 2125     |
| learning rate      | 0.001    |
| nupdates           | 63800    |
| policy_entropy     | 1.65     |
| policy_loss        | -0.0544  |
| total_timesteps    | 5104000  |
| value_loss         | 0.00346  |
---------------------------------
---------------------------------
| avg reward         | 0.379    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.923    |
| fps                | 2125     |
| learning rate      | 0.001    |
| nupdates           | 63900    |
| policy_entropy     | 1.62     |
| policy_loss        | -0.00109 |
| total_timesteps    | 5112000  |
| value_loss         | 0.00826  |
---------------------------------
---------------------------------
| avg reward         | 0.0768   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.958    |
| fps                | 2125     |
| learning rate      | 0.001    |
| nupdates           | 64000    |
| policy_entropy     | 1.58     |
| policy_loss        | 0.0499   |
| total_timesteps    | 5120000  |
| value_loss         | 0.00414  |
---------------------------------
---------------------------------
| avg reward         | 0.559    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.89     |
| fps                | 2125     |
| learning rate      | 0.001    |
| nupdates           | 64100    |
| policy_entropy     | 1.64     |
| policy_loss        | -0.146   |
| total_timesteps    | 5128000  |
| value_loss         | 0.0168   |
---------------------------------
---------------------------------
| avg reward         | 0.127    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.788    |
| fps                | 2126     |
| learning rate      | 0.001    |
| nupdates           | 64200    |
| policy_entropy     | 1.48     |
| policy_loss        | -0.0345  |
| total_timesteps    | 5136000  |
| value_loss         | 0.0279   |
---------------------------------
---------------------------------
| avg reward         | 0.0722   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.902    |
| fps                | 2126     |
| learning rate      | 0.001    |
| nupdates           | 64300    |
| policy_entropy     | 1.41     |
| policy_loss        | 0.0392   |
| total_timesteps    | 5144000  |
| value_loss         | 0.00662  |
---------------------------------
---------------------------------
| avg reward         | 0.0437   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.895    |
| fps                | 2126     |
| learning rate      | 0.001    |
| nupdates           | 64400    |
| policy_entropy     | 1.55     |
| policy_loss        | -0.0197  |
| total_timesteps    | 5152000  |
| value_loss         | 0.0109   |
---------------------------------
---------------------------------
| avg reward         | -0.00143 |
| epsilonValue       | 0.000727 |
| explained_variance | 0.809    |
| fps                | 2126     |
| learning rate      | 0.001    |
| nupdates           | 64500    |
| policy_entropy     | 1.48     |
| policy_loss        | 0.0643   |
| total_timesteps    | 5160000  |
| value_loss         | 0.0203   |
---------------------------------
---------------------------------
| avg reward         | -0.207   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.867    |
| fps                | 2126     |
| learning rate      | 0.001    |
| nupdates           | 64600    |
| policy_entropy     | 1.48     |
| policy_loss        | -0.0885  |
| total_timesteps    | 5168000  |
| value_loss         | 0.0193   |
---------------------------------
---------------------------------
| avg reward         | 0.195    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.799    |
| fps                | 2126     |
| learning rate      | 0.001    |
| nupdates           | 64700    |
| policy_entropy     | 1.61     |
| policy_loss        | -0.0237  |
| total_timesteps    | 5176000  |
| value_loss         | 0.0231   |
---------------------------------
---------------------------------
| avg reward         | 0.256    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.959    |
| fps                | 2126     |
| learning rate      | 0.001    |
| nupdates           | 64800    |
| policy_entropy     | 1.51     |
| policy_loss        | -0.0738  |
| total_timesteps    | 5184000  |
| value_loss         | 0.00671  |
---------------------------------
---------------------------------
| avg reward         | 0.117    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.898    |
| fps                | 2126     |
| learning rate      | 0.001    |
| nupdates           | 64900    |
| policy_entropy     | 1.47     |
| policy_loss        | -0.0346  |
| total_timesteps    | 5192000  |
| value_loss         | 0.00756  |
---------------------------------
---------------------------------
| avg reward         | 0.256    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.837    |
| fps                | 2126     |
| learning rate      | 0.001    |
| nupdates           | 65000    |
| policy_entropy     | 1.58     |
| policy_loss        | 0.0638   |
| total_timesteps    | 5200000  |
| value_loss         | 0.0206   |
---------------------------------
---------------------------------
| avg reward         | 0.294    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.915    |
| fps                | 2126     |
| learning rate      | 0.001    |
| nupdates           | 65100    |
| policy_entropy     | 1.59     |
| policy_loss        | -0.0433  |
| total_timesteps    | 5208000  |
| value_loss         | 0.0128   |
---------------------------------
---------------------------------
| avg reward         | -0.0669  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.905    |
| fps                | 2126     |
| learning rate      | 0.001    |
| nupdates           | 65200    |
| policy_entropy     | 1.68     |
| policy_loss        | -0.0101  |
| total_timesteps    | 5216000  |
| value_loss         | 0.00956  |
---------------------------------
---------------------------------
| avg reward         | -0.041   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.837    |
| fps                | 2126     |
| learning rate      | 0.001    |
| nupdates           | 65300    |
| policy_entropy     | 1.62     |
| policy_loss        | 0.0496   |
| total_timesteps    | 5224000  |
| value_loss         | 0.00622  |
---------------------------------
---------------------------------
| avg reward         | 0.0858   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.827    |
| fps                | 2126     |
| learning rate      | 0.001    |
| nupdates           | 65400    |
| policy_entropy     | 1.52     |
| policy_loss        | -0.0016  |
| total_timesteps    | 5232000  |
| value_loss         | 0.0104   |
---------------------------------
---------------------------------
| avg reward         | 0.272    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.877    |
| fps                | 2126     |
| learning rate      | 0.001    |
| nupdates           | 65500    |
| policy_entropy     | 1.61     |
| policy_loss        | 0.0899   |
| total_timesteps    | 5240000  |
| value_loss         | 0.0122   |
---------------------------------
---------------------------------
| avg reward         | 0.228    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.945    |
| fps                | 2126     |
| learning rate      | 0.001    |
| nupdates           | 65600    |
| policy_entropy     | 1.55     |
| policy_loss        | -0.113   |
| total_timesteps    | 5248000  |
| value_loss         | 0.0142   |
---------------------------------
---------------------------------
| avg reward         | 0.166    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.819    |
| fps                | 2126     |
| learning rate      | 0.001    |
| nupdates           | 65700    |
| policy_entropy     | 1.6      |
| policy_loss        | -0.00323 |
| total_timesteps    | 5256000  |
| value_loss         | 0.0154   |
---------------------------------
---------------------------------
| avg reward         | 0.166    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.96     |
| fps                | 2126     |
| learning rate      | 0.001    |
| nupdates           | 65800    |
| policy_entropy     | 1.55     |
| policy_loss        | -0.00297 |
| total_timesteps    | 5264000  |
| value_loss         | 0.00334  |
---------------------------------
---------------------------------
| avg reward         | 0.0459   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.924    |
| fps                | 2126     |
| learning rate      | 0.001    |
| nupdates           | 65900    |
| policy_entropy     | 1.56     |
| policy_loss        | -0.049   |
| total_timesteps    | 5272000  |
| value_loss         | 0.00674  |
---------------------------------
---------------------------------
| avg reward         | 0.0222   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.925    |
| fps                | 2126     |
| learning rate      | 0.001    |
| nupdates           | 66000    |
| policy_entropy     | 1.64     |
| policy_loss        | -0.0267  |
| total_timesteps    | 5280000  |
| value_loss         | 0.00463  |
---------------------------------
---------------------------------
| avg reward         | -0.208   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.772    |
| fps                | 2126     |
| learning rate      | 0.001    |
| nupdates           | 66100    |
| policy_entropy     | 1.61     |
| policy_loss        | -0.0512  |
| total_timesteps    | 5288000  |
| value_loss         | 0.0153   |
---------------------------------
---------------------------------
| avg reward         | -0.0498  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.799    |
| fps                | 2126     |
| learning rate      | 0.001    |
| nupdates           | 66200    |
| policy_entropy     | 1.52     |
| policy_loss        | -0.0102  |
| total_timesteps    | 5296000  |
| value_loss         | 0.012    |
---------------------------------
---------------------------------
| avg reward         | -0.0493  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.819    |
| fps                | 2126     |
| learning rate      | 0.001    |
| nupdates           | 66300    |
| policy_entropy     | 1.52     |
| policy_loss        | -0.0192  |
| total_timesteps    | 5304000  |
| value_loss         | 0.015    |
---------------------------------
---------------------------------
| avg reward         | 0.0623   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.966    |
| fps                | 2126     |
| learning rate      | 0.001    |
| nupdates           | 66400    |
| policy_entropy     | 1.61     |
| policy_loss        | 0.0215   |
| total_timesteps    | 5312000  |
| value_loss         | 0.00432  |
---------------------------------
---------------------------------
| avg reward         | -0.0276  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.93     |
| fps                | 2126     |
| learning rate      | 0.001    |
| nupdates           | 66500    |
| policy_entropy     | 1.64     |
| policy_loss        | 0.046    |
| total_timesteps    | 5320000  |
| value_loss         | 0.00618  |
---------------------------------
---------------------------------
| avg reward         | 0.0995   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.865    |
| fps                | 2127     |
| learning rate      | 0.001    |
| nupdates           | 66600    |
| policy_entropy     | 1.57     |
| policy_loss        | -0.0778  |
| total_timesteps    | 5328000  |
| value_loss         | 0.0215   |
---------------------------------
---------------------------------
| avg reward         | 0.289    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.928    |
| fps                | 2127     |
| learning rate      | 0.001    |
| nupdates           | 66700    |
| policy_entropy     | 1.49     |
| policy_loss        | -0.00801 |
| total_timesteps    | 5336000  |
| value_loss         | 0.0117   |
---------------------------------
---------------------------------
| avg reward         | 0.0373   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.955    |
| fps                | 2127     |
| learning rate      | 0.001    |
| nupdates           | 66800    |
| policy_entropy     | 1.59     |
| policy_loss        | 0.0416   |
| total_timesteps    | 5344000  |
| value_loss         | 0.00424  |
---------------------------------
---------------------------------
| avg reward         | 0.213    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.861    |
| fps                | 2127     |
| learning rate      | 0.001    |
| nupdates           | 66900    |
| policy_entropy     | 1.56     |
| policy_loss        | 0.0333   |
| total_timesteps    | 5352000  |
| value_loss         | 0.0125   |
---------------------------------
---------------------------------
| avg reward         | -0.0435  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.884    |
| fps                | 2127     |
| learning rate      | 0.001    |
| nupdates           | 67000    |
| policy_entropy     | 1.51     |
| policy_loss        | 0.0448   |
| total_timesteps    | 5360000  |
| value_loss         | 0.0116   |
---------------------------------
---------------------------------
| avg reward         | 0.00999  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.548    |
| fps                | 2127     |
| learning rate      | 0.001    |
| nupdates           | 67100    |
| policy_entropy     | 1.64     |
| policy_loss        | 0.0208   |
| total_timesteps    | 5368000  |
| value_loss         | 0.0114   |
---------------------------------
---------------------------------
| avg reward         | 0.187    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.833    |
| fps                | 2127     |
| learning rate      | 0.001    |
| nupdates           | 67200    |
| policy_entropy     | 1.38     |
| policy_loss        | 0.0384   |
| total_timesteps    | 5376000  |
| value_loss         | 0.0251   |
---------------------------------
---------------------------------
| avg reward         | 0.23     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.89     |
| fps                | 2127     |
| learning rate      | 0.001    |
| nupdates           | 67300    |
| policy_entropy     | 1.36     |
| policy_loss        | -0.0623  |
| total_timesteps    | 5384000  |
| value_loss         | 0.02     |
---------------------------------
---------------------------------
| avg reward         | 0.0885   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.893    |
| fps                | 2127     |
| learning rate      | 0.001    |
| nupdates           | 67400    |
| policy_entropy     | 1.58     |
| policy_loss        | -0.0409  |
| total_timesteps    | 5392000  |
| value_loss         | 0.0121   |
---------------------------------
---------------------------------
| avg reward         | 0.175    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.946    |
| fps                | 2127     |
| learning rate      | 0.001    |
| nupdates           | 67500    |
| policy_entropy     | 1.57     |
| policy_loss        | -0.0154  |
| total_timesteps    | 5400000  |
| value_loss         | 0.00573  |
---------------------------------
---------------------------------
| avg reward         | -0.00115 |
| epsilonValue       | 0.000727 |
| explained_variance | 0.926    |
| fps                | 2127     |
| learning rate      | 0.001    |
| nupdates           | 67600    |
| policy_entropy     | 1.57     |
| policy_loss        | -0.0289  |
| total_timesteps    | 5408000  |
| value_loss         | 0.00966  |
---------------------------------
---------------------------------
| avg reward         | 0.273    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.885    |
| fps                | 2127     |
| learning rate      | 0.001    |
| nupdates           | 67700    |
| policy_entropy     | 1.57     |
| policy_loss        | 0.0295   |
| total_timesteps    | 5416000  |
| value_loss         | 0.0117   |
---------------------------------
---------------------------------
| avg reward         | 0.0502   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.686    |
| fps                | 2127     |
| learning rate      | 0.001    |
| nupdates           | 67800    |
| policy_entropy     | 1.57     |
| policy_loss        | 0.0698   |
| total_timesteps    | 5424000  |
| value_loss         | 0.0374   |
---------------------------------
---------------------------------
| avg reward         | 0.348    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.887    |
| fps                | 2127     |
| learning rate      | 0.001    |
| nupdates           | 67900    |
| policy_entropy     | 1.66     |
| policy_loss        | 0.0158   |
| total_timesteps    | 5432000  |
| value_loss         | 0.0137   |
---------------------------------
---------------------------------
| avg reward         | 0.00609  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.876    |
| fps                | 2127     |
| learning rate      | 0.001    |
| nupdates           | 68000    |
| policy_entropy     | 1.59     |
| policy_loss        | 0.00581  |
| total_timesteps    | 5440000  |
| value_loss         | 0.0113   |
---------------------------------
---------------------------------
| avg reward         | 0.175    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.813    |
| fps                | 2127     |
| learning rate      | 0.001    |
| nupdates           | 68100    |
| policy_entropy     | 1.59     |
| policy_loss        | 0.0579   |
| total_timesteps    | 5448000  |
| value_loss         | 0.0155   |
---------------------------------
---------------------------------
| avg reward         | 0.456    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.804    |
| fps                | 2127     |
| learning rate      | 0.001    |
| nupdates           | 68200    |
| policy_entropy     | 1.56     |
| policy_loss        | 0.116    |
| total_timesteps    | 5456000  |
| value_loss         | 0.0346   |
---------------------------------
---------------------------------
| avg reward         | -0.00987 |
| epsilonValue       | 0.000727 |
| explained_variance | 0.827    |
| fps                | 2128     |
| learning rate      | 0.001    |
| nupdates           | 68300    |
| policy_entropy     | 1.57     |
| policy_loss        | -0.0627  |
| total_timesteps    | 5464000  |
| value_loss         | 0.00932  |
---------------------------------
---------------------------------
| avg reward         | 0.0625   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.77     |
| fps                | 2128     |
| learning rate      | 0.001    |
| nupdates           | 68400    |
| policy_entropy     | 1.56     |
| policy_loss        | 0.0419   |
| total_timesteps    | 5472000  |
| value_loss         | 0.0116   |
---------------------------------
---------------------------------
| avg reward         | 0.0979   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.92     |
| fps                | 2128     |
| learning rate      | 0.001    |
| nupdates           | 68500    |
| policy_entropy     | 1.67     |
| policy_loss        | 0.0156   |
| total_timesteps    | 5480000  |
| value_loss         | 0.00401  |
---------------------------------
---------------------------------
| avg reward         | 0.0357   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.819    |
| fps                | 2128     |
| learning rate      | 0.001    |
| nupdates           | 68600    |
| policy_entropy     | 1.57     |
| policy_loss        | -0.0171  |
| total_timesteps    | 5488000  |
| value_loss         | 0.00538  |
---------------------------------
---------------------------------
| avg reward         | -0.0528  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.887    |
| fps                | 2128     |
| learning rate      | 0.001    |
| nupdates           | 68700    |
| policy_entropy     | 1.61     |
| policy_loss        | -0.094   |
| total_timesteps    | 5496000  |
| value_loss         | 0.0148   |
---------------------------------
---------------------------------
| avg reward         | 0.21     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.886    |
| fps                | 2128     |
| learning rate      | 0.001    |
| nupdates           | 68800    |
| policy_entropy     | 1.47     |
| policy_loss        | 0.0551   |
| total_timesteps    | 5504000  |
| value_loss         | 0.0123   |
---------------------------------
---------------------------------
| avg reward         | 0.248    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.888    |
| fps                | 2128     |
| learning rate      | 0.001    |
| nupdates           | 68900    |
| policy_entropy     | 1.45     |
| policy_loss        | -0.0719  |
| total_timesteps    | 5512000  |
| value_loss         | 0.0158   |
---------------------------------
---------------------------------
| avg reward         | 0.044    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.695    |
| fps                | 2128     |
| learning rate      | 0.001    |
| nupdates           | 69000    |
| policy_entropy     | 1.45     |
| policy_loss        | -0.0338  |
| total_timesteps    | 5520000  |
| value_loss         | 0.0114   |
---------------------------------
---------------------------------
| avg reward         | 0.0807   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.917    |
| fps                | 2128     |
| learning rate      | 0.001    |
| nupdates           | 69100    |
| policy_entropy     | 1.57     |
| policy_loss        | -0.0444  |
| total_timesteps    | 5528000  |
| value_loss         | 0.00612  |
---------------------------------
---------------------------------
| avg reward         | 0.0303   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.925    |
| fps                | 2128     |
| learning rate      | 0.001    |
| nupdates           | 69200    |
| policy_entropy     | 1.69     |
| policy_loss        | 0.01     |
| total_timesteps    | 5536000  |
| value_loss         | 0.00671  |
---------------------------------
---------------------------------
| avg reward         | -0.16    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.446    |
| fps                | 2128     |
| learning rate      | 0.001    |
| nupdates           | 69300    |
| policy_entropy     | 1.44     |
| policy_loss        | 0.00964  |
| total_timesteps    | 5544000  |
| value_loss         | 0.013    |
---------------------------------
---------------------------------
| avg reward         | 0.15     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.786    |
| fps                | 2128     |
| learning rate      | 0.001    |
| nupdates           | 69400    |
| policy_entropy     | 1.47     |
| policy_loss        | 0.0199   |
| total_timesteps    | 5552000  |
| value_loss         | 0.0192   |
---------------------------------
---------------------------------
| avg reward         | -0.0407  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.869    |
| fps                | 2128     |
| learning rate      | 0.001    |
| nupdates           | 69500    |
| policy_entropy     | 1.59     |
| policy_loss        | 0.0792   |
| total_timesteps    | 5560000  |
| value_loss         | 0.00659  |
---------------------------------
---------------------------------
| avg reward         | -0.0656  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.544    |
| fps                | 2128     |
| learning rate      | 0.001    |
| nupdates           | 69600    |
| policy_entropy     | 1.59     |
| policy_loss        | 0.00957  |
| total_timesteps    | 5568000  |
| value_loss         | 0.0122   |
---------------------------------
---------------------------------
| avg reward         | -0.0607  |
| epsilonValue       | 0.000727 |
| explained_variance | 0.723    |
| fps                | 2128     |
| learning rate      | 0.001    |
| nupdates           | 69700    |
| policy_entropy     | 1.65     |
| policy_loss        | -0.0377  |
| total_timesteps    | 5576000  |
| value_loss         | 0.00678  |
---------------------------------
---------------------------------
| avg reward         | 0.349    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.926    |
| fps                | 2128     |
| learning rate      | 0.001    |
| nupdates           | 69800    |
| policy_entropy     | 1.54     |
| policy_loss        | -0.0447  |
| total_timesteps    | 5584000  |
| value_loss         | 0.00933  |
---------------------------------
---------------------------------
| avg reward         | 0.0907   |
| epsilonValue       | 0.000727 |
| explained_variance | 0.692    |
| fps                | 2128     |
| learning rate      | 0.001    |
| nupdates           | 69900    |
| policy_entropy     | 1.4      |
| policy_loss        | 0.00194  |
| total_timesteps    | 5592000  |
| value_loss         | 0.0147   |
---------------------------------
---------------------------------
| avg reward         | 0.44     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.895    |
| fps                | 2128     |
| learning rate      | 0.001    |
| nupdates           | 70000    |
| policy_entropy     | 1.68     |
| policy_loss        | 0.0314   |
| total_timesteps    | 5600000  |
| value_loss         | 0.0133   |
---------------------------------
---------------------------------
| avg reward         | 0.295    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.828    |
| fps                | 2128     |
| learning rate      | 0.001    |
| nupdates           | 70100    |
| policy_entropy     | 1.46     |
| policy_loss        | -0.0167  |
| total_timesteps    | 5608000  |
| value_loss         | 0.0199   |
---------------------------------
---------------------------------
| avg reward         | 0.574    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.936    |
| fps                | 2129     |
| learning rate      | 0.001    |
| nupdates           | 70200    |
| policy_entropy     | 1.71     |
| policy_loss        | 0.0455   |
| total_timesteps    | 5616000  |
| value_loss         | 0.0106   |
---------------------------------
----------------------------------
| avg reward         | 0.321     |
| epsilonValue       | 0.000727  |
| explained_variance | 0.868     |
| fps                | 2129      |
| learning rate      | 0.001     |
| nupdates           | 70300     |
| policy_entropy     | 1.57      |
| policy_loss        | -0.000595 |
| total_timesteps    | 5624000   |
| value_loss         | 0.0143    |
----------------------------------
---------------------------------
| avg reward         | 0.127    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.966    |
| fps                | 2129     |
| learning rate      | 0.001    |
| nupdates           | 70400    |
| policy_entropy     | 1.61     |
| policy_loss        | 0.0394   |
| total_timesteps    | 5632000  |
| value_loss         | 0.00281  |
---------------------------------
---------------------------------
| avg reward         | 0.55     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.683    |
| fps                | 2129     |
| learning rate      | 0.001    |
| nupdates           | 70500    |
| policy_entropy     | 1.51     |
| policy_loss        | 0.0144   |
| total_timesteps    | 5640000  |
| value_loss         | 0.014    |
---------------------------------
---------------------------------
| avg reward         | 0.3      |
| epsilonValue       | 0.000727 |
| explained_variance | 0.937    |
| fps                | 2129     |
| learning rate      | 0.001    |
| nupdates           | 70600    |
| policy_entropy     | 1.67     |
| policy_loss        | -0.00418 |
| total_timesteps    | 5648000  |
| value_loss         | 0.00832  |
---------------------------------
---------------------------------
| avg reward         | 0.199    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.842    |
| fps                | 2129     |
| learning rate      | 0.001    |
| nupdates           | 70700    |
| policy_entropy     | 1.6      |
| policy_loss        | -0.00665 |
| total_timesteps    | 5656000  |
| value_loss         | 0.0142   |
---------------------------------
---------------------------------
| avg reward         | -0.00528 |
| epsilonValue       | 0.000727 |
| explained_variance | 0.874    |
| fps                | 2129     |
| learning rate      | 0.001    |
| nupdates           | 70800    |
| policy_entropy     | 1.68     |
| policy_loss        | 0.0148   |
| total_timesteps    | 5664000  |
| value_loss         | 0.00836  |
---------------------------------
---------------------------------
| avg reward         | 0.455    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.617    |
| fps                | 2129     |
| learning rate      | 0.001    |
| nupdates           | 70900    |
| policy_entropy     | 1.54     |
| policy_loss        | 0.0896   |
| total_timesteps    | 5672000  |
| value_loss         | 0.0185   |
---------------------------------
---------------------------------
| avg reward         | 0.241    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.877    |
| fps                | 2129     |
| learning rate      | 0.001    |
| nupdates           | 71000    |
| policy_entropy     | 1.53     |
| policy_loss        | -0.0182  |
| total_timesteps    | 5680000  |
| value_loss         | 0.0125   |
---------------------------------
---------------------------------
| avg reward         | 0.548    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.902    |
| fps                | 2129     |
| learning rate      | 0.001    |
| nupdates           | 71100    |
| policy_entropy     | 1.58     |
| policy_loss        | -0.00985 |
| total_timesteps    | 5688000  |
| value_loss         | 0.00604  |
---------------------------------
---------------------------------
| avg reward         | 0.317    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.912    |
| fps                | 2129     |
| learning rate      | 0.001    |
| nupdates           | 71200    |
| policy_entropy     | 1.43     |
| policy_loss        | -0.0448  |
| total_timesteps    | 5696000  |
| value_loss         | 0.00878  |
---------------------------------
---------------------------------
| avg reward         | 0.443    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.904    |
| fps                | 2129     |
| learning rate      | 0.001    |
| nupdates           | 71300    |
| policy_entropy     | 1.6      |
| policy_loss        | -0.0936  |
| total_timesteps    | 5704000  |
| value_loss         | 0.0195   |
---------------------------------
---------------------------------
| avg reward         | 0.339    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.924    |
| fps                | 2129     |
| learning rate      | 0.001    |
| nupdates           | 71400    |
| policy_entropy     | 1.51     |
| policy_loss        | 0.0238   |
| total_timesteps    | 5712000  |
| value_loss         | 0.00887  |
---------------------------------
---------------------------------
| avg reward         | 0.151    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.859    |
| fps                | 2129     |
| learning rate      | 0.001    |
| nupdates           | 71500    |
| policy_entropy     | 1.59     |
| policy_loss        | -0.0775  |
| total_timesteps    | 5720000  |
| value_loss         | 0.0166   |
---------------------------------
---------------------------------
| avg reward         | 0.524    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.907    |
| fps                | 2129     |
| learning rate      | 0.001    |
| nupdates           | 71600    |
| policy_entropy     | 1.48     |
| policy_loss        | 0.0384   |
| total_timesteps    | 5728000  |
| value_loss         | 0.0103   |
---------------------------------
---------------------------------
| avg reward         | 0.356    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.879    |
| fps                | 2129     |
| learning rate      | 0.001    |
| nupdates           | 71700    |
| policy_entropy     | 1.53     |
| policy_loss        | 0.00467  |
| total_timesteps    | 5736000  |
| value_loss         | 0.0144   |
---------------------------------
---------------------------------
| avg reward         | 0.377    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.941    |
| fps                | 2129     |
| learning rate      | 0.001    |
| nupdates           | 71800    |
| policy_entropy     | 1.56     |
| policy_loss        | -0.00402 |
| total_timesteps    | 5744000  |
| value_loss         | 0.00724  |
---------------------------------
---------------------------------
| avg reward         | 0.408    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.857    |
| fps                | 2130     |
| learning rate      | 0.001    |
| nupdates           | 71900    |
| policy_entropy     | 1.45     |
| policy_loss        | -0.0509  |
| total_timesteps    | 5752000  |
| value_loss         | 0.0117   |
---------------------------------
---------------------------------
| avg reward         | 0.262    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.873    |
| fps                | 2130     |
| learning rate      | 0.001    |
| nupdates           | 72000    |
| policy_entropy     | 1.44     |
| policy_loss        | 0.0204   |
| total_timesteps    | 5760000  |
| value_loss         | 0.0137   |
---------------------------------
---------------------------------
| avg reward         | 0.301    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.928    |
| fps                | 2130     |
| learning rate      | 0.001    |
| nupdates           | 72100    |
| policy_entropy     | 1.57     |
| policy_loss        | -0.0358  |
| total_timesteps    | 5768000  |
| value_loss         | 0.00631  |
---------------------------------
---------------------------------
| avg reward         | 0.515    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.934    |
| fps                | 2130     |
| learning rate      | 0.001    |
| nupdates           | 72200    |
| policy_entropy     | 1.58     |
| policy_loss        | -0.038   |
| total_timesteps    | 5776000  |
| value_loss         | 0.00868  |
---------------------------------
---------------------------------
| avg reward         | 0.32     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.832    |
| fps                | 2130     |
| learning rate      | 0.001    |
| nupdates           | 72300    |
| policy_entropy     | 1.59     |
| policy_loss        | -0.0105  |
| total_timesteps    | 5784000  |
| value_loss         | 0.017    |
---------------------------------
---------------------------------
| avg reward         | 0.364    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.919    |
| fps                | 2130     |
| learning rate      | 0.001    |
| nupdates           | 72400    |
| policy_entropy     | 1.56     |
| policy_loss        | -0.0219  |
| total_timesteps    | 5792000  |
| value_loss         | 0.00894  |
---------------------------------
---------------------------------
| avg reward         | 0.685    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.853    |
| fps                | 2130     |
| learning rate      | 0.001    |
| nupdates           | 72500    |
| policy_entropy     | 1.42     |
| policy_loss        | -0.0494  |
| total_timesteps    | 5800000  |
| value_loss         | 0.0161   |
---------------------------------
---------------------------------
| avg reward         | 0.27     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.758    |
| fps                | 2130     |
| learning rate      | 0.001    |
| nupdates           | 72600    |
| policy_entropy     | 1.47     |
| policy_loss        | -0.0263  |
| total_timesteps    | 5808000  |
| value_loss         | 0.0129   |
---------------------------------
---------------------------------
| avg reward         | 0.527    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.955    |
| fps                | 2130     |
| learning rate      | 0.001    |
| nupdates           | 72700    |
| policy_entropy     | 1.56     |
| policy_loss        | 0.0294   |
| total_timesteps    | 5816000  |
| value_loss         | 0.00528  |
---------------------------------
---------------------------------
| avg reward         | 0.44     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.941    |
| fps                | 2130     |
| learning rate      | 0.001    |
| nupdates           | 72800    |
| policy_entropy     | 1.57     |
| policy_loss        | -0.0545  |
| total_timesteps    | 5824000  |
| value_loss         | 0.00921  |
---------------------------------
---------------------------------
| avg reward         | 0.377    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.926    |
| fps                | 2130     |
| learning rate      | 0.001    |
| nupdates           | 72900    |
| policy_entropy     | 1.49     |
| policy_loss        | 0.0243   |
| total_timesteps    | 5832000  |
| value_loss         | 0.0088   |
---------------------------------
---------------------------------
| avg reward         | 0.395    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.644    |
| fps                | 2130     |
| learning rate      | 0.001    |
| nupdates           | 73000    |
| policy_entropy     | 1.6      |
| policy_loss        | 0.069    |
| total_timesteps    | 5840000  |
| value_loss         | 0.0161   |
---------------------------------
---------------------------------
| avg reward         | 0.316    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.929    |
| fps                | 2130     |
| learning rate      | 0.001    |
| nupdates           | 73100    |
| policy_entropy     | 1.59     |
| policy_loss        | -0.0108  |
| total_timesteps    | 5848000  |
| value_loss         | 0.00647  |
---------------------------------
---------------------------------
| avg reward         | 0.556    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.966    |
| fps                | 2130     |
| learning rate      | 0.001    |
| nupdates           | 73200    |
| policy_entropy     | 1.69     |
| policy_loss        | 0.0516   |
| total_timesteps    | 5856000  |
| value_loss         | 0.0045   |
---------------------------------
---------------------------------
| avg reward         | 0.36     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.92     |
| fps                | 2130     |
| learning rate      | 0.001    |
| nupdates           | 73300    |
| policy_entropy     | 1.55     |
| policy_loss        | -0.00791 |
| total_timesteps    | 5864000  |
| value_loss         | 0.0072   |
---------------------------------
---------------------------------
| avg reward         | 0.267    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.954    |
| fps                | 2130     |
| learning rate      | 0.001    |
| nupdates           | 73400    |
| policy_entropy     | 1.5      |
| policy_loss        | -0.00246 |
| total_timesteps    | 5872000  |
| value_loss         | 0.00187  |
---------------------------------
---------------------------------
| avg reward         | 0.487    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.782    |
| fps                | 2130     |
| learning rate      | 0.001    |
| nupdates           | 73500    |
| policy_entropy     | 1.37     |
| policy_loss        | -0.0102  |
| total_timesteps    | 5880000  |
| value_loss         | 0.0224   |
---------------------------------
---------------------------------
| avg reward         | 0.656    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.941    |
| fps                | 2130     |
| learning rate      | 0.001    |
| nupdates           | 73600    |
| policy_entropy     | 1.63     |
| policy_loss        | -0.0517  |
| total_timesteps    | 5888000  |
| value_loss         | 0.00856  |
---------------------------------
---------------------------------
| avg reward         | 0.604    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.936    |
| fps                | 2130     |
| learning rate      | 0.001    |
| nupdates           | 73700    |
| policy_entropy     | 1.47     |
| policy_loss        | 0.0157   |
| total_timesteps    | 5896000  |
| value_loss         | 0.00655  |
---------------------------------
---------------------------------
| avg reward         | 0.709    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.879    |
| fps                | 2130     |
| learning rate      | 0.001    |
| nupdates           | 73800    |
| policy_entropy     | 1.58     |
| policy_loss        | 0.0526   |
| total_timesteps    | 5904000  |
| value_loss         | 0.00956  |
---------------------------------
---------------------------------
| avg reward         | 0.756    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.928    |
| fps                | 2130     |
| learning rate      | 0.001    |
| nupdates           | 73900    |
| policy_entropy     | 1.46     |
| policy_loss        | 0.00496  |
| total_timesteps    | 5912000  |
| value_loss         | 0.00766  |
---------------------------------
---------------------------------
| avg reward         | 0.42     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.876    |
| fps                | 2130     |
| learning rate      | 0.001    |
| nupdates           | 74000    |
| policy_entropy     | 1.67     |
| policy_loss        | -0.0485  |
| total_timesteps    | 5920000  |
| value_loss         | 0.00997  |
---------------------------------
---------------------------------
| avg reward         | 0.439    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.809    |
| fps                | 2130     |
| learning rate      | 0.001    |
| nupdates           | 74100    |
| policy_entropy     | 1.45     |
| policy_loss        | -0.0546  |
| total_timesteps    | 5928000  |
| value_loss         | 0.0116   |
---------------------------------
---------------------------------
| avg reward         | 0.625    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.891    |
| fps                | 2130     |
| learning rate      | 0.001    |
| nupdates           | 74200    |
| policy_entropy     | 1.36     |
| policy_loss        | 0.0335   |
| total_timesteps    | 5936000  |
| value_loss         | 0.0115   |
---------------------------------
---------------------------------
| avg reward         | 0.78     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.932    |
| fps                | 2130     |
| learning rate      | 0.001    |
| nupdates           | 74300    |
| policy_entropy     | 1.49     |
| policy_loss        | 0.0321   |
| total_timesteps    | 5944000  |
| value_loss         | 0.00562  |
---------------------------------
---------------------------------
| avg reward         | 1.02     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.922    |
| fps                | 2130     |
| learning rate      | 0.001    |
| nupdates           | 74400    |
| policy_entropy     | 1.35     |
| policy_loss        | -0.0372  |
| total_timesteps    | 5952000  |
| value_loss         | 0.00487  |
---------------------------------
---------------------------------
| avg reward         | 0.514    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.91     |
| fps                | 2131     |
| learning rate      | 0.001    |
| nupdates           | 74500    |
| policy_entropy     | 1.61     |
| policy_loss        | -0.0736  |
| total_timesteps    | 5960000  |
| value_loss         | 0.00603  |
---------------------------------
---------------------------------
| avg reward         | 0.424    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.915    |
| fps                | 2131     |
| learning rate      | 0.001    |
| nupdates           | 74600    |
| policy_entropy     | 1.38     |
| policy_loss        | -0.0222  |
| total_timesteps    | 5968000  |
| value_loss         | 0.00771  |
---------------------------------
---------------------------------
| avg reward         | 0.313    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.926    |
| fps                | 2131     |
| learning rate      | 0.001    |
| nupdates           | 74700    |
| policy_entropy     | 1.42     |
| policy_loss        | -0.0639  |
| total_timesteps    | 5976000  |
| value_loss         | 0.00855  |
---------------------------------
---------------------------------
| avg reward         | 0.443    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.765    |
| fps                | 2131     |
| learning rate      | 0.001    |
| nupdates           | 74800    |
| policy_entropy     | 1.52     |
| policy_loss        | -0.0612  |
| total_timesteps    | 5984000  |
| value_loss         | 0.0151   |
---------------------------------
---------------------------------
| avg reward         | 0.941    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.891    |
| fps                | 2131     |
| learning rate      | 0.001    |
| nupdates           | 74900    |
| policy_entropy     | 1.49     |
| policy_loss        | -0.0398  |
| total_timesteps    | 5992000  |
| value_loss         | 0.00914  |
---------------------------------
---------------------------------
| avg reward         | 0.907    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.959    |
| fps                | 2131     |
| learning rate      | 0.001    |
| nupdates           | 75000    |
| policy_entropy     | 1.56     |
| policy_loss        | -0.0301  |
| total_timesteps    | 6000000  |
| value_loss         | 0.0063   |
---------------------------------
---------------------------------
| avg reward         | 0.961    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.868    |
| fps                | 2131     |
| learning rate      | 0.001    |
| nupdates           | 75100    |
| policy_entropy     | 1.49     |
| policy_loss        | 0.0359   |
| total_timesteps    | 6008000  |
| value_loss         | 0.00756  |
---------------------------------
---------------------------------
| avg reward         | 1.15     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.835    |
| fps                | 2131     |
| learning rate      | 0.001    |
| nupdates           | 75200    |
| policy_entropy     | 1.57     |
| policy_loss        | -0.00634 |
| total_timesteps    | 6016000  |
| value_loss         | 0.00684  |
---------------------------------
---------------------------------
| avg reward         | 0.86     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.95     |
| fps                | 2131     |
| learning rate      | 0.001    |
| nupdates           | 75300    |
| policy_entropy     | 1.73     |
| policy_loss        | 0.0701   |
| total_timesteps    | 6024000  |
| value_loss         | 0.00453  |
---------------------------------
---------------------------------
| avg reward         | 1.1      |
| epsilonValue       | 0.000727 |
| explained_variance | 0.9      |
| fps                | 2131     |
| learning rate      | 0.001    |
| nupdates           | 75400    |
| policy_entropy     | 1.46     |
| policy_loss        | -0.0398  |
| total_timesteps    | 6032000  |
| value_loss         | 0.00566  |
---------------------------------
---------------------------------
| avg reward         | 0.89     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.868    |
| fps                | 2131     |
| learning rate      | 0.001    |
| nupdates           | 75500    |
| policy_entropy     | 1.66     |
| policy_loss        | 0.0767   |
| total_timesteps    | 6040000  |
| value_loss         | 0.00958  |
---------------------------------
---------------------------------
| avg reward         | 0.858    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.917    |
| fps                | 2131     |
| learning rate      | 0.001    |
| nupdates           | 75600    |
| policy_entropy     | 1.47     |
| policy_loss        | 0.00512  |
| total_timesteps    | 6048000  |
| value_loss         | 0.00732  |
---------------------------------
---------------------------------
| avg reward         | 0.813    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.935    |
| fps                | 2131     |
| learning rate      | 0.001    |
| nupdates           | 75700    |
| policy_entropy     | 1.62     |
| policy_loss        | 0.0133   |
| total_timesteps    | 6056000  |
| value_loss         | 0.0077   |
---------------------------------
---------------------------------
| avg reward         | 0.916    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.892    |
| fps                | 2131     |
| learning rate      | 0.001    |
| nupdates           | 75800    |
| policy_entropy     | 1.59     |
| policy_loss        | 0.00675  |
| total_timesteps    | 6064000  |
| value_loss         | 0.00279  |
---------------------------------
---------------------------------
| avg reward         | 0.818    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.91     |
| fps                | 2131     |
| learning rate      | 0.001    |
| nupdates           | 75900    |
| policy_entropy     | 1.48     |
| policy_loss        | -0.0797  |
| total_timesteps    | 6072000  |
| value_loss         | 0.0141   |
---------------------------------
---------------------------------
| avg reward         | 0.504    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.972    |
| fps                | 2131     |
| learning rate      | 0.001    |
| nupdates           | 76000    |
| policy_entropy     | 1.65     |
| policy_loss        | -0.0375  |
| total_timesteps    | 6080000  |
| value_loss         | 0.00379  |
---------------------------------
---------------------------------
| avg reward         | 0.309    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.922    |
| fps                | 2131     |
| learning rate      | 0.001    |
| nupdates           | 76100    |
| policy_entropy     | 1.57     |
| policy_loss        | -0.00461 |
| total_timesteps    | 6088000  |
| value_loss         | 0.011    |
---------------------------------
---------------------------------
| avg reward         | 0.347    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.916    |
| fps                | 2131     |
| learning rate      | 0.001    |
| nupdates           | 76200    |
| policy_entropy     | 1.67     |
| policy_loss        | 0.0183   |
| total_timesteps    | 6096000  |
| value_loss         | 0.00916  |
---------------------------------
---------------------------------
| avg reward         | 0.886    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.887    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 76300    |
| policy_entropy     | 1.59     |
| policy_loss        | -0.111   |
| total_timesteps    | 6104000  |
| value_loss         | 0.0108   |
---------------------------------
---------------------------------
| avg reward         | 0.713    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.89     |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 76400    |
| policy_entropy     | 1.66     |
| policy_loss        | -0.0749  |
| total_timesteps    | 6112000  |
| value_loss         | 0.0141   |
---------------------------------
---------------------------------
| avg reward         | 0.894    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.834    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 76500    |
| policy_entropy     | 1.67     |
| policy_loss        | 0.0364   |
| total_timesteps    | 6120000  |
| value_loss         | 0.0141   |
---------------------------------
---------------------------------
| avg reward         | 0.961    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.945    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 76600    |
| policy_entropy     | 1.48     |
| policy_loss        | 0.0169   |
| total_timesteps    | 6128000  |
| value_loss         | 0.00339  |
---------------------------------
---------------------------------
| avg reward         | 0.547    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.932    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 76700    |
| policy_entropy     | 1.6      |
| policy_loss        | 0.0364   |
| total_timesteps    | 6136000  |
| value_loss         | 0.0103   |
---------------------------------
---------------------------------
| avg reward         | 0.595    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.948    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 76800    |
| policy_entropy     | 1.62     |
| policy_loss        | -0.0458  |
| total_timesteps    | 6144000  |
| value_loss         | 0.00343  |
---------------------------------
---------------------------------
| avg reward         | 0.776    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.954    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 76900    |
| policy_entropy     | 1.56     |
| policy_loss        | -0.0161  |
| total_timesteps    | 6152000  |
| value_loss         | 0.00393  |
---------------------------------
---------------------------------
| avg reward         | 0.527    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.921    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 77000    |
| policy_entropy     | 1.58     |
| policy_loss        | 0.0179   |
| total_timesteps    | 6160000  |
| value_loss         | 0.00749  |
---------------------------------
---------------------------------
| avg reward         | 0.913    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.949    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 77100    |
| policy_entropy     | 1.5      |
| policy_loss        | 0.0199   |
| total_timesteps    | 6168000  |
| value_loss         | 0.00434  |
---------------------------------
---------------------------------
| avg reward         | 0.822    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.971    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 77200    |
| policy_entropy     | 1.59     |
| policy_loss        | 0.00879  |
| total_timesteps    | 6176000  |
| value_loss         | 0.00365  |
---------------------------------
---------------------------------
| avg reward         | 0.728    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.963    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 77300    |
| policy_entropy     | 1.61     |
| policy_loss        | -0.0148  |
| total_timesteps    | 6184000  |
| value_loss         | 0.00249  |
---------------------------------
---------------------------------
| avg reward         | 0.65     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.942    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 77400    |
| policy_entropy     | 1.47     |
| policy_loss        | -0.0516  |
| total_timesteps    | 6192000  |
| value_loss         | 0.00705  |
---------------------------------
---------------------------------
| avg reward         | 0.772    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.953    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 77500    |
| policy_entropy     | 1.54     |
| policy_loss        | 0.00211  |
| total_timesteps    | 6200000  |
| value_loss         | 0.00319  |
---------------------------------
---------------------------------
| avg reward         | 0.7      |
| epsilonValue       | 0.000727 |
| explained_variance | 0.983    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 77600    |
| policy_entropy     | 1.68     |
| policy_loss        | -0.00101 |
| total_timesteps    | 6208000  |
| value_loss         | 0.00201  |
---------------------------------
---------------------------------
| avg reward         | 1.03     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.959    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 77700    |
| policy_entropy     | 1.69     |
| policy_loss        | -0.00834 |
| total_timesteps    | 6216000  |
| value_loss         | 0.00344  |
---------------------------------
---------------------------------
| avg reward         | 0.928    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.862    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 77800    |
| policy_entropy     | 1.28     |
| policy_loss        | 0.0702   |
| total_timesteps    | 6224000  |
| value_loss         | 0.014    |
---------------------------------
---------------------------------
| avg reward         | 0.696    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.939    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 77900    |
| policy_entropy     | 1.52     |
| policy_loss        | 0.0286   |
| total_timesteps    | 6232000  |
| value_loss         | 0.00727  |
---------------------------------
---------------------------------
| avg reward         | 0.787    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.961    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 78000    |
| policy_entropy     | 1.49     |
| policy_loss        | -0.00241 |
| total_timesteps    | 6240000  |
| value_loss         | 0.00517  |
---------------------------------
---------------------------------
| avg reward         | 1        |
| epsilonValue       | 0.000727 |
| explained_variance | 0.782    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 78100    |
| policy_entropy     | 1.56     |
| policy_loss        | 0.0489   |
| total_timesteps    | 6248000  |
| value_loss         | 0.0195   |
---------------------------------
---------------------------------
| avg reward         | 0.893    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.839    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 78200    |
| policy_entropy     | 1.55     |
| policy_loss        | 0.05     |
| total_timesteps    | 6256000  |
| value_loss         | 0.0124   |
---------------------------------
---------------------------------
| avg reward         | 0.778    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.981    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 78300    |
| policy_entropy     | 1.72     |
| policy_loss        | -0.0347  |
| total_timesteps    | 6264000  |
| value_loss         | 0.00215  |
---------------------------------
---------------------------------
| avg reward         | 0.739    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.858    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 78400    |
| policy_entropy     | 1.65     |
| policy_loss        | -0.0435  |
| total_timesteps    | 6272000  |
| value_loss         | 0.00774  |
---------------------------------
---------------------------------
| avg reward         | 1.11     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.966    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 78500    |
| policy_entropy     | 1.58     |
| policy_loss        | 0.0094   |
| total_timesteps    | 6280000  |
| value_loss         | 0.00235  |
---------------------------------
---------------------------------
| avg reward         | 0.659    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.961    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 78600    |
| policy_entropy     | 1.66     |
| policy_loss        | -0.0518  |
| total_timesteps    | 6288000  |
| value_loss         | 0.00437  |
---------------------------------
---------------------------------
| avg reward         | 1.06     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.915    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 78700    |
| policy_entropy     | 1.56     |
| policy_loss        | 0.0337   |
| total_timesteps    | 6296000  |
| value_loss         | 0.00961  |
---------------------------------
---------------------------------
| avg reward         | 1.04     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.964    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 78800    |
| policy_entropy     | 1.65     |
| policy_loss        | -0.0223  |
| total_timesteps    | 6304000  |
| value_loss         | 0.00258  |
---------------------------------
---------------------------------
| avg reward         | 0.615    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.972    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 78900    |
| policy_entropy     | 1.71     |
| policy_loss        | 0.018    |
| total_timesteps    | 6312000  |
| value_loss         | 0.00266  |
---------------------------------
---------------------------------
| avg reward         | 0.745    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.952    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 79000    |
| policy_entropy     | 1.65     |
| policy_loss        | -0.00847 |
| total_timesteps    | 6320000  |
| value_loss         | 0.00335  |
---------------------------------
---------------------------------
| avg reward         | 0.917    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.959    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 79100    |
| policy_entropy     | 1.56     |
| policy_loss        | 0.011    |
| total_timesteps    | 6328000  |
| value_loss         | 0.00366  |
---------------------------------
---------------------------------
| avg reward         | 0.88     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.975    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 79200    |
| policy_entropy     | 1.61     |
| policy_loss        | -0.0292  |
| total_timesteps    | 6336000  |
| value_loss         | 0.00279  |
---------------------------------
---------------------------------
| avg reward         | 1.16     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.968    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 79300    |
| policy_entropy     | 1.58     |
| policy_loss        | 0.0226   |
| total_timesteps    | 6344000  |
| value_loss         | 0.00174  |
---------------------------------
---------------------------------
| avg reward         | 0.906    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.934    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 79400    |
| policy_entropy     | 1.67     |
| policy_loss        | -0.0203  |
| total_timesteps    | 6352000  |
| value_loss         | 0.0076   |
---------------------------------
---------------------------------
| avg reward         | 0.874    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.964    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 79500    |
| policy_entropy     | 1.67     |
| policy_loss        | -0.0214  |
| total_timesteps    | 6360000  |
| value_loss         | 0.00534  |
---------------------------------
---------------------------------
| avg reward         | 0.99     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.863    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 79600    |
| policy_entropy     | 1.63     |
| policy_loss        | 0.0231   |
| total_timesteps    | 6368000  |
| value_loss         | 0.00593  |
---------------------------------
---------------------------------
| avg reward         | 0.87     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.974    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 79700    |
| policy_entropy     | 1.56     |
| policy_loss        | 0.0548   |
| total_timesteps    | 6376000  |
| value_loss         | 0.00256  |
---------------------------------
---------------------------------
| avg reward         | 0.999    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.91     |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 79800    |
| policy_entropy     | 1.71     |
| policy_loss        | 0.0498   |
| total_timesteps    | 6384000  |
| value_loss         | 0.00747  |
---------------------------------
---------------------------------
| avg reward         | 0.901    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.907    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 79900    |
| policy_entropy     | 1.61     |
| policy_loss        | 0.0388   |
| total_timesteps    | 6392000  |
| value_loss         | 0.00702  |
---------------------------------
---------------------------------
| avg reward         | 0.842    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.979    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 80000    |
| policy_entropy     | 1.48     |
| policy_loss        | -0.0165  |
| total_timesteps    | 6400000  |
| value_loss         | 0.00384  |
---------------------------------
---------------------------------
| avg reward         | 0.929    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.962    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 80100    |
| policy_entropy     | 1.55     |
| policy_loss        | 0.000884 |
| total_timesteps    | 6408000  |
| value_loss         | 0.00217  |
---------------------------------
---------------------------------
| avg reward         | 0.997    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.942    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 80200    |
| policy_entropy     | 1.65     |
| policy_loss        | 0.021    |
| total_timesteps    | 6416000  |
| value_loss         | 0.00595  |
---------------------------------
---------------------------------
| avg reward         | 0.783    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.973    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 80300    |
| policy_entropy     | 1.52     |
| policy_loss        | 0.0148   |
| total_timesteps    | 6424000  |
| value_loss         | 0.00404  |
---------------------------------
---------------------------------
| avg reward         | 0.679    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.885    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 80400    |
| policy_entropy     | 1.34     |
| policy_loss        | -0.0295  |
| total_timesteps    | 6432000  |
| value_loss         | 0.019    |
---------------------------------
---------------------------------
| avg reward         | 0.714    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.952    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 80500    |
| policy_entropy     | 1.46     |
| policy_loss        | -0.00566 |
| total_timesteps    | 6440000  |
| value_loss         | 0.00652  |
---------------------------------
---------------------------------
| avg reward         | 0.721    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.886    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 80600    |
| policy_entropy     | 1.32     |
| policy_loss        | -0.0507  |
| total_timesteps    | 6448000  |
| value_loss         | 0.0149   |
---------------------------------
---------------------------------
| avg reward         | 0.364    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.96     |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 80700    |
| policy_entropy     | 1.71     |
| policy_loss        | -0.00385 |
| total_timesteps    | 6456000  |
| value_loss         | 0.00295  |
---------------------------------
---------------------------------
| avg reward         | 0.858    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.922    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 80800    |
| policy_entropy     | 1.24     |
| policy_loss        | -0.0238  |
| total_timesteps    | 6464000  |
| value_loss         | 0.00974  |
---------------------------------
---------------------------------
| avg reward         | 0.938    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.962    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 80900    |
| policy_entropy     | 1.65     |
| policy_loss        | -0.00912 |
| total_timesteps    | 6472000  |
| value_loss         | 0.00178  |
---------------------------------
---------------------------------
| avg reward         | 0.965    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.958    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 81000    |
| policy_entropy     | 1.57     |
| policy_loss        | -0.0297  |
| total_timesteps    | 6480000  |
| value_loss         | 0.00607  |
---------------------------------
---------------------------------
| avg reward         | 1.08     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.952    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 81100    |
| policy_entropy     | 1.6      |
| policy_loss        | -0.00702 |
| total_timesteps    | 6488000  |
| value_loss         | 0.00497  |
---------------------------------
---------------------------------
| avg reward         | 0.799    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.968    |
| fps                | 2132     |
| learning rate      | 0.001    |
| nupdates           | 81200    |
| policy_entropy     | 1.56     |
| policy_loss        | 0.0283   |
| total_timesteps    | 6496000  |
| value_loss         | 0.00358  |
---------------------------------
---------------------------------
| avg reward         | 1.25     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.963    |
| fps                | 2133     |
| learning rate      | 0.001    |
| nupdates           | 81300    |
| policy_entropy     | 1.61     |
| policy_loss        | 0.00454  |
| total_timesteps    | 6504000  |
| value_loss         | 0.00231  |
---------------------------------
---------------------------------
| avg reward         | 1.02     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.947    |
| fps                | 2133     |
| learning rate      | 0.001    |
| nupdates           | 81400    |
| policy_entropy     | 1.66     |
| policy_loss        | -0.0131  |
| total_timesteps    | 6512000  |
| value_loss         | 0.00407  |
---------------------------------
---------------------------------
| avg reward         | 0.854    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.946    |
| fps                | 2133     |
| learning rate      | 0.001    |
| nupdates           | 81500    |
| policy_entropy     | 1.55     |
| policy_loss        | 0.0379   |
| total_timesteps    | 6520000  |
| value_loss         | 0.00475  |
---------------------------------
---------------------------------
| avg reward         | 0.694    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.961    |
| fps                | 2133     |
| learning rate      | 0.001    |
| nupdates           | 81600    |
| policy_entropy     | 1.67     |
| policy_loss        | 0.0219   |
| total_timesteps    | 6528000  |
| value_loss         | 0.0063   |
---------------------------------
----------------------------------
| avg reward         | 0.511     |
| epsilonValue       | 0.000727  |
| explained_variance | 0.966     |
| fps                | 2133      |
| learning rate      | 0.001     |
| nupdates           | 81700     |
| policy_entropy     | 1.7       |
| policy_loss        | -0.000417 |
| total_timesteps    | 6536000   |
| value_loss         | 0.00316   |
----------------------------------
---------------------------------
| avg reward         | 0.975    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.966    |
| fps                | 2133     |
| learning rate      | 0.001    |
| nupdates           | 81800    |
| policy_entropy     | 1.68     |
| policy_loss        | -0.0713  |
| total_timesteps    | 6544000  |
| value_loss         | 0.00424  |
---------------------------------
---------------------------------
| avg reward         | 1.06     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.972    |
| fps                | 2133     |
| learning rate      | 0.001    |
| nupdates           | 81900    |
| policy_entropy     | 1.67     |
| policy_loss        | -0.0181  |
| total_timesteps    | 6552000  |
| value_loss         | 0.00244  |
---------------------------------
---------------------------------
| avg reward         | 0.783    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.876    |
| fps                | 2133     |
| learning rate      | 0.001    |
| nupdates           | 82000    |
| policy_entropy     | 1.63     |
| policy_loss        | 0.0771   |
| total_timesteps    | 6560000  |
| value_loss         | 0.0117   |
---------------------------------
---------------------------------
| avg reward         | 0.901    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.977    |
| fps                | 2133     |
| learning rate      | 0.001    |
| nupdates           | 82100    |
| policy_entropy     | 1.67     |
| policy_loss        | -0.00863 |
| total_timesteps    | 6568000  |
| value_loss         | 0.00324  |
---------------------------------
---------------------------------
| avg reward         | 0.926    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.923    |
| fps                | 2133     |
| learning rate      | 0.001    |
| nupdates           | 82200    |
| policy_entropy     | 1.59     |
| policy_loss        | 0.00501  |
| total_timesteps    | 6576000  |
| value_loss         | 0.00431  |
---------------------------------
---------------------------------
| avg reward         | 1.17     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.961    |
| fps                | 2133     |
| learning rate      | 0.001    |
| nupdates           | 82300    |
| policy_entropy     | 1.61     |
| policy_loss        | 0.0263   |
| total_timesteps    | 6584000  |
| value_loss         | 0.00144  |
---------------------------------
---------------------------------
| avg reward         | 0.719    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.966    |
| fps                | 2133     |
| learning rate      | 0.001    |
| nupdates           | 82400    |
| policy_entropy     | 1.6      |
| policy_loss        | -0.00683 |
| total_timesteps    | 6592000  |
| value_loss         | 0.00494  |
---------------------------------
---------------------------------
| avg reward         | 0.621    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.95     |
| fps                | 2133     |
| learning rate      | 0.001    |
| nupdates           | 82500    |
| policy_entropy     | 1.45     |
| policy_loss        | 0.00812  |
| total_timesteps    | 6600000  |
| value_loss         | 0.00522  |
---------------------------------
---------------------------------
| avg reward         | 0.739    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.979    |
| fps                | 2133     |
| learning rate      | 0.001    |
| nupdates           | 82600    |
| policy_entropy     | 1.64     |
| policy_loss        | -0.00786 |
| total_timesteps    | 6608000  |
| value_loss         | 0.00266  |
---------------------------------
---------------------------------
| avg reward         | 0.651    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.973    |
| fps                | 2133     |
| learning rate      | 0.001    |
| nupdates           | 82700    |
| policy_entropy     | 1.59     |
| policy_loss        | -0.0196  |
| total_timesteps    | 6616000  |
| value_loss         | 0.00296  |
---------------------------------
---------------------------------
| avg reward         | 0.676    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.967    |
| fps                | 2133     |
| learning rate      | 0.001    |
| nupdates           | 82800    |
| policy_entropy     | 1.63     |
| policy_loss        | -0.00289 |
| total_timesteps    | 6624000  |
| value_loss         | 0.00296  |
---------------------------------
---------------------------------
| avg reward         | 0.884    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.913    |
| fps                | 2133     |
| learning rate      | 0.001    |
| nupdates           | 82900    |
| policy_entropy     | 1.47     |
| policy_loss        | 0.0238   |
| total_timesteps    | 6632000  |
| value_loss         | 0.0101   |
---------------------------------
---------------------------------
| avg reward         | 0.824    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.94     |
| fps                | 2133     |
| learning rate      | 0.001    |
| nupdates           | 83000    |
| policy_entropy     | 1.58     |
| policy_loss        | 0.0513   |
| total_timesteps    | 6640000  |
| value_loss         | 0.0055   |
---------------------------------
---------------------------------
| avg reward         | 0.934    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.986    |
| fps                | 2133     |
| learning rate      | 0.001    |
| nupdates           | 83100    |
| policy_entropy     | 1.69     |
| policy_loss        | 0.0196   |
| total_timesteps    | 6648000  |
| value_loss         | 0.00112  |
---------------------------------
---------------------------------
| avg reward         | 0.978    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.965    |
| fps                | 2133     |
| learning rate      | 0.001    |
| nupdates           | 83200    |
| policy_entropy     | 1.69     |
| policy_loss        | 0.054    |
| total_timesteps    | 6656000  |
| value_loss         | 0.00337  |
---------------------------------
---------------------------------
| avg reward         | 1.05     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.982    |
| fps                | 2133     |
| learning rate      | 0.001    |
| nupdates           | 83300    |
| policy_entropy     | 1.54     |
| policy_loss        | -0.00973 |
| total_timesteps    | 6664000  |
| value_loss         | 0.00144  |
---------------------------------
---------------------------------
| avg reward         | 0.734    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.965    |
| fps                | 2133     |
| learning rate      | 0.001    |
| nupdates           | 83400    |
| policy_entropy     | 1.48     |
| policy_loss        | 0.02     |
| total_timesteps    | 6672000  |
| value_loss         | 0.008    |
---------------------------------
---------------------------------
| avg reward         | 0.775    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.968    |
| fps                | 2133     |
| learning rate      | 0.001    |
| nupdates           | 83500    |
| policy_entropy     | 1.43     |
| policy_loss        | 0.000225 |
| total_timesteps    | 6680000  |
| value_loss         | 0.00294  |
---------------------------------
---------------------------------
| avg reward         | 0.575    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.963    |
| fps                | 2133     |
| learning rate      | 0.001    |
| nupdates           | 83600    |
| policy_entropy     | 1.59     |
| policy_loss        | 0.000715 |
| total_timesteps    | 6688000  |
| value_loss         | 0.00452  |
---------------------------------
---------------------------------
| avg reward         | 0.933    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.779    |
| fps                | 2133     |
| learning rate      | 0.001    |
| nupdates           | 83700    |
| policy_entropy     | 1.49     |
| policy_loss        | -0.0744  |
| total_timesteps    | 6696000  |
| value_loss         | 0.023    |
---------------------------------
---------------------------------
| avg reward         | 0.839    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.983    |
| fps                | 2133     |
| learning rate      | 0.001    |
| nupdates           | 83800    |
| policy_entropy     | 1.43     |
| policy_loss        | 0.0108   |
| total_timesteps    | 6704000  |
| value_loss         | 0.00216  |
---------------------------------
---------------------------------
| avg reward         | 0.892    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.955    |
| fps                | 2133     |
| learning rate      | 0.001    |
| nupdates           | 83900    |
| policy_entropy     | 1.69     |
| policy_loss        | -0.0104  |
| total_timesteps    | 6712000  |
| value_loss         | 0.00488  |
---------------------------------
---------------------------------
| avg reward         | 0.905    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.939    |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 84000    |
| policy_entropy     | 1.53     |
| policy_loss        | -0.0276  |
| total_timesteps    | 6720000  |
| value_loss         | 0.0054   |
---------------------------------
---------------------------------
| avg reward         | 0.909    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.954    |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 84100    |
| policy_entropy     | 1.61     |
| policy_loss        | -0.0183  |
| total_timesteps    | 6728000  |
| value_loss         | 0.00477  |
---------------------------------
---------------------------------
| avg reward         | 1.08     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.826    |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 84200    |
| policy_entropy     | 1.38     |
| policy_loss        | 0.0411   |
| total_timesteps    | 6736000  |
| value_loss         | 0.02     |
---------------------------------
---------------------------------
| avg reward         | 0.898    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.971    |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 84300    |
| policy_entropy     | 1.58     |
| policy_loss        | -0.0352  |
| total_timesteps    | 6744000  |
| value_loss         | 0.0035   |
---------------------------------
---------------------------------
| avg reward         | 1.04     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.883    |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 84400    |
| policy_entropy     | 1.45     |
| policy_loss        | -0.00458 |
| total_timesteps    | 6752000  |
| value_loss         | 0.0107   |
---------------------------------
---------------------------------
| avg reward         | 0.93     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.979    |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 84500    |
| policy_entropy     | 1.61     |
| policy_loss        | -0.00351 |
| total_timesteps    | 6760000  |
| value_loss         | 0.00238  |
---------------------------------
---------------------------------
| avg reward         | 0.842    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.976    |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 84600    |
| policy_entropy     | 1.58     |
| policy_loss        | 0.00275  |
| total_timesteps    | 6768000  |
| value_loss         | 0.00366  |
---------------------------------
---------------------------------
| avg reward         | 1.18     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.826    |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 84700    |
| policy_entropy     | 1.57     |
| policy_loss        | 0.0193   |
| total_timesteps    | 6776000  |
| value_loss         | 0.00762  |
---------------------------------
---------------------------------
| avg reward         | 1.03     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.975    |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 84800    |
| policy_entropy     | 1.69     |
| policy_loss        | -0.00568 |
| total_timesteps    | 6784000  |
| value_loss         | 0.00161  |
---------------------------------
---------------------------------
| avg reward         | 0.931    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.976    |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 84900    |
| policy_entropy     | 1.52     |
| policy_loss        | -0.0197  |
| total_timesteps    | 6792000  |
| value_loss         | 0.00268  |
---------------------------------
---------------------------------
| avg reward         | 0.738    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.768    |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 85000    |
| policy_entropy     | 1.6      |
| policy_loss        | -0.126   |
| total_timesteps    | 6800000  |
| value_loss         | 0.0196   |
---------------------------------
---------------------------------
| avg reward         | 0.953    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.981    |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 85100    |
| policy_entropy     | 1.44     |
| policy_loss        | -0.0154  |
| total_timesteps    | 6808000  |
| value_loss         | 0.00371  |
---------------------------------
---------------------------------
| avg reward         | 0.484    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.926    |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 85200    |
| policy_entropy     | 1.42     |
| policy_loss        | -0.0792  |
| total_timesteps    | 6816000  |
| value_loss         | 0.00692  |
---------------------------------
---------------------------------
| avg reward         | 0.76     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.97     |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 85300    |
| policy_entropy     | 1.61     |
| policy_loss        | 0.0375   |
| total_timesteps    | 6824000  |
| value_loss         | 0.00354  |
---------------------------------
---------------------------------
| avg reward         | 0.882    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.906    |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 85400    |
| policy_entropy     | 1.44     |
| policy_loss        | 0.000108 |
| total_timesteps    | 6832000  |
| value_loss         | 0.00975  |
---------------------------------
---------------------------------
| avg reward         | 1.03     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.864    |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 85500    |
| policy_entropy     | 1.62     |
| policy_loss        | -0.0204  |
| total_timesteps    | 6840000  |
| value_loss         | 0.011    |
---------------------------------
----------------------------------
| avg reward         | 0.868     |
| epsilonValue       | 0.000727  |
| explained_variance | 0.976     |
| fps                | 2134      |
| learning rate      | 0.001     |
| nupdates           | 85600     |
| policy_entropy     | 1.57      |
| policy_loss        | -0.000352 |
| total_timesteps    | 6848000   |
| value_loss         | 0.00265   |
----------------------------------
---------------------------------
| avg reward         | 0.88     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.947    |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 85700    |
| policy_entropy     | 1.23     |
| policy_loss        | -0.0352  |
| total_timesteps    | 6856000  |
| value_loss         | 0.00858  |
---------------------------------
---------------------------------
| avg reward         | 1.05     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.978    |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 85800    |
| policy_entropy     | 1.65     |
| policy_loss        | 0.0269   |
| total_timesteps    | 6864000  |
| value_loss         | 0.00237  |
---------------------------------
----------------------------------
| avg reward         | 0.898     |
| epsilonValue       | 0.000727  |
| explained_variance | 0.962     |
| fps                | 2134      |
| learning rate      | 0.001     |
| nupdates           | 85900     |
| policy_entropy     | 1.6       |
| policy_loss        | -0.000175 |
| total_timesteps    | 6872000   |
| value_loss         | 0.00298   |
----------------------------------
---------------------------------
| avg reward         | 1.03     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.963    |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 86000    |
| policy_entropy     | 1.52     |
| policy_loss        | -0.00755 |
| total_timesteps    | 6880000  |
| value_loss         | 0.00218  |
---------------------------------
---------------------------------
| avg reward         | 1.07     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.981    |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 86100    |
| policy_entropy     | 1.64     |
| policy_loss        | -0.00686 |
| total_timesteps    | 6888000  |
| value_loss         | 0.00207  |
---------------------------------
---------------------------------
| avg reward         | 1.04     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.99     |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 86200    |
| policy_entropy     | 1.61     |
| policy_loss        | -0.0257  |
| total_timesteps    | 6896000  |
| value_loss         | 0.000922 |
---------------------------------
---------------------------------
| avg reward         | 1.12     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.949    |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 86300    |
| policy_entropy     | 1.65     |
| policy_loss        | 0.0146   |
| total_timesteps    | 6904000  |
| value_loss         | 0.00353  |
---------------------------------
---------------------------------
| avg reward         | 0.96     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.968    |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 86400    |
| policy_entropy     | 1.48     |
| policy_loss        | 0.000532 |
| total_timesteps    | 6912000  |
| value_loss         | 0.00166  |
---------------------------------
---------------------------------
| avg reward         | 0.863    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.985    |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 86500    |
| policy_entropy     | 1.5      |
| policy_loss        | 0.0111   |
| total_timesteps    | 6920000  |
| value_loss         | 0.00181  |
---------------------------------
---------------------------------
| avg reward         | 1.02     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.9      |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 86600    |
| policy_entropy     | 1.53     |
| policy_loss        | 0.0173   |
| total_timesteps    | 6928000  |
| value_loss         | 0.00703  |
---------------------------------
---------------------------------
| avg reward         | 0.959    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.943    |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 86700    |
| policy_entropy     | 1.56     |
| policy_loss        | 0.0268   |
| total_timesteps    | 6936000  |
| value_loss         | 0.00435  |
---------------------------------
---------------------------------
| avg reward         | 0.766    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.93     |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 86800    |
| policy_entropy     | 1.6      |
| policy_loss        | -0.00619 |
| total_timesteps    | 6944000  |
| value_loss         | 0.0112   |
---------------------------------
----------------------------------
| avg reward         | 0.967     |
| epsilonValue       | 0.000727  |
| explained_variance | 0.973     |
| fps                | 2134      |
| learning rate      | 0.001     |
| nupdates           | 86900     |
| policy_entropy     | 1.5       |
| policy_loss        | -0.000921 |
| total_timesteps    | 6952000   |
| value_loss         | 0.0021    |
----------------------------------
---------------------------------
| avg reward         | 0.959    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.965    |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 87000    |
| policy_entropy     | 1.51     |
| policy_loss        | 0.0317   |
| total_timesteps    | 6960000  |
| value_loss         | 0.00546  |
---------------------------------
---------------------------------
| avg reward         | 0.859    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.969    |
| fps                | 2134     |
| learning rate      | 0.001    |
| nupdates           | 87100    |
| policy_entropy     | 1.73     |
| policy_loss        | -0.0218  |
| total_timesteps    | 6968000  |
| value_loss         | 0.00333  |
---------------------------------
---------------------------------
| avg reward         | 0.628    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.866    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 87200    |
| policy_entropy     | 1.46     |
| policy_loss        | 0.000424 |
| total_timesteps    | 6976000  |
| value_loss         | 0.0105   |
---------------------------------
---------------------------------
| avg reward         | 0.997    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.971    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 87300    |
| policy_entropy     | 1.67     |
| policy_loss        | 0.0513   |
| total_timesteps    | 6984000  |
| value_loss         | 0.00295  |
---------------------------------
---------------------------------
| avg reward         | 0.755    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.946    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 87400    |
| policy_entropy     | 1.73     |
| policy_loss        | 0.0675   |
| total_timesteps    | 6992000  |
| value_loss         | 0.00622  |
---------------------------------
---------------------------------
| avg reward         | 1.06     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.972    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 87500    |
| policy_entropy     | 1.49     |
| policy_loss        | 0.0133   |
| total_timesteps    | 7000000  |
| value_loss         | 0.00256  |
---------------------------------
---------------------------------
| avg reward         | 0.701    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.968    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 87600    |
| policy_entropy     | 1.56     |
| policy_loss        | -0.0277  |
| total_timesteps    | 7008000  |
| value_loss         | 0.00356  |
---------------------------------
---------------------------------
| avg reward         | 1.01     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.959    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 87700    |
| policy_entropy     | 1.69     |
| policy_loss        | 0.0553   |
| total_timesteps    | 7016000  |
| value_loss         | 0.00335  |
---------------------------------
---------------------------------
| avg reward         | 0.916    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.967    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 87800    |
| policy_entropy     | 1.54     |
| policy_loss        | 0.0459   |
| total_timesteps    | 7024000  |
| value_loss         | 0.0042   |
---------------------------------
---------------------------------
| avg reward         | 0.923    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.957    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 87900    |
| policy_entropy     | 1.65     |
| policy_loss        | -0.00164 |
| total_timesteps    | 7032000  |
| value_loss         | 0.00275  |
---------------------------------
---------------------------------
| avg reward         | 1.08     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.94     |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 88000    |
| policy_entropy     | 1.58     |
| policy_loss        | 0.00901  |
| total_timesteps    | 7040000  |
| value_loss         | 0.00658  |
---------------------------------
---------------------------------
| avg reward         | 1.01     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.943    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 88100    |
| policy_entropy     | 1.61     |
| policy_loss        | 0.034    |
| total_timesteps    | 7048000  |
| value_loss         | 0.00421  |
---------------------------------
---------------------------------
| avg reward         | 0.975    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.943    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 88200    |
| policy_entropy     | 1.51     |
| policy_loss        | 0.00857  |
| total_timesteps    | 7056000  |
| value_loss         | 0.00624  |
---------------------------------
---------------------------------
| avg reward         | 0.794    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.855    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 88300    |
| policy_entropy     | 1.58     |
| policy_loss        | -0.0709  |
| total_timesteps    | 7064000  |
| value_loss         | 0.0178   |
---------------------------------
---------------------------------
| avg reward         | 1.1      |
| epsilonValue       | 0.000727 |
| explained_variance | 0.962    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 88400    |
| policy_entropy     | 1.49     |
| policy_loss        | 0.0624   |
| total_timesteps    | 7072000  |
| value_loss         | 0.0046   |
---------------------------------
---------------------------------
| avg reward         | 0.873    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.991    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 88500    |
| policy_entropy     | 1.65     |
| policy_loss        | 0.00776  |
| total_timesteps    | 7080000  |
| value_loss         | 0.00123  |
---------------------------------
---------------------------------
| avg reward         | 1.04     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.985    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 88600    |
| policy_entropy     | 1.67     |
| policy_loss        | -0.00426 |
| total_timesteps    | 7088000  |
| value_loss         | 0.00102  |
---------------------------------
---------------------------------
| avg reward         | 1        |
| epsilonValue       | 0.000727 |
| explained_variance | 0.976    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 88700    |
| policy_entropy     | 1.45     |
| policy_loss        | 0.0403   |
| total_timesteps    | 7096000  |
| value_loss         | 0.0038   |
---------------------------------
---------------------------------
| avg reward         | 0.859    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.978    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 88800    |
| policy_entropy     | 1.62     |
| policy_loss        | -0.0542  |
| total_timesteps    | 7104000  |
| value_loss         | 0.00337  |
---------------------------------
---------------------------------
| avg reward         | 0.836    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.977    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 88900    |
| policy_entropy     | 1.6      |
| policy_loss        | -0.0253  |
| total_timesteps    | 7112000  |
| value_loss         | 0.00365  |
---------------------------------
---------------------------------
| avg reward         | 0.992    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.939    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 89000    |
| policy_entropy     | 1.72     |
| policy_loss        | -0.0455  |
| total_timesteps    | 7120000  |
| value_loss         | 0.005    |
---------------------------------
---------------------------------
| avg reward         | 0.613    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.983    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 89100    |
| policy_entropy     | 1.66     |
| policy_loss        | -0.0187  |
| total_timesteps    | 7128000  |
| value_loss         | 0.00286  |
---------------------------------
---------------------------------
| avg reward         | 1.07     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.937    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 89200    |
| policy_entropy     | 1.65     |
| policy_loss        | -0.0316  |
| total_timesteps    | 7136000  |
| value_loss         | 0.00494  |
---------------------------------
---------------------------------
| avg reward         | 0.689    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.948    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 89300    |
| policy_entropy     | 1.49     |
| policy_loss        | -0.104   |
| total_timesteps    | 7144000  |
| value_loss         | 0.00887  |
---------------------------------
---------------------------------
| avg reward         | 0.908    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.946    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 89400    |
| policy_entropy     | 1.6      |
| policy_loss        | 0.0342   |
| total_timesteps    | 7152000  |
| value_loss         | 0.00715  |
---------------------------------
---------------------------------
| avg reward         | 0.518    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.849    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 89500    |
| policy_entropy     | 1.52     |
| policy_loss        | -0.135   |
| total_timesteps    | 7160000  |
| value_loss         | 0.0176   |
---------------------------------
---------------------------------
| avg reward         | 1.28     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.976    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 89600    |
| policy_entropy     | 1.71     |
| policy_loss        | -0.0381  |
| total_timesteps    | 7168000  |
| value_loss         | 0.00314  |
---------------------------------
---------------------------------
| avg reward         | 0.699    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.922    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 89700    |
| policy_entropy     | 1.67     |
| policy_loss        | 0.0442   |
| total_timesteps    | 7176000  |
| value_loss         | 0.00836  |
---------------------------------
---------------------------------
| avg reward         | 0.873    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.964    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 89800    |
| policy_entropy     | 1.47     |
| policy_loss        | 0.0196   |
| total_timesteps    | 7184000  |
| value_loss         | 0.00728  |
---------------------------------
---------------------------------
| avg reward         | 0.854    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.961    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 89900    |
| policy_entropy     | 1.58     |
| policy_loss        | 0.0143   |
| total_timesteps    | 7192000  |
| value_loss         | 0.00341  |
---------------------------------
----------------------------------
| avg reward         | 0.941     |
| epsilonValue       | 0.000727  |
| explained_variance | 0.978     |
| fps                | 2135      |
| learning rate      | 0.001     |
| nupdates           | 90000     |
| policy_entropy     | 1.57      |
| policy_loss        | -0.000116 |
| total_timesteps    | 7200000   |
| value_loss         | 0.00338   |
----------------------------------
---------------------------------
| avg reward         | 0.61     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.933    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 90100    |
| policy_entropy     | 1.61     |
| policy_loss        | -0.0198  |
| total_timesteps    | 7208000  |
| value_loss         | 0.00746  |
---------------------------------
---------------------------------
| avg reward         | 0.71     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.912    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 90200    |
| policy_entropy     | 1.58     |
| policy_loss        | 0.044    |
| total_timesteps    | 7216000  |
| value_loss         | 0.00831  |
---------------------------------
---------------------------------
| avg reward         | 1.19     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.862    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 90300    |
| policy_entropy     | 1.64     |
| policy_loss        | 0.0287   |
| total_timesteps    | 7224000  |
| value_loss         | 0.00742  |
---------------------------------
---------------------------------
| avg reward         | 0.777    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.983    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 90400    |
| policy_entropy     | 1.49     |
| policy_loss        | 0.0239   |
| total_timesteps    | 7232000  |
| value_loss         | 0.00257  |
---------------------------------
---------------------------------
| avg reward         | 0.94     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.982    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 90500    |
| policy_entropy     | 1.69     |
| policy_loss        | 0.00797  |
| total_timesteps    | 7240000  |
| value_loss         | 0.00116  |
---------------------------------
---------------------------------
| avg reward         | 1.03     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.969    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 90600    |
| policy_entropy     | 1.65     |
| policy_loss        | 0.0351   |
| total_timesteps    | 7248000  |
| value_loss         | 0.0022   |
---------------------------------
---------------------------------
| avg reward         | 0.935    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.972    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 90700    |
| policy_entropy     | 1.69     |
| policy_loss        | -0.0128  |
| total_timesteps    | 7256000  |
| value_loss         | 0.00259  |
---------------------------------
---------------------------------
| avg reward         | 1.03     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.976    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 90800    |
| policy_entropy     | 1.6      |
| policy_loss        | 0.0347   |
| total_timesteps    | 7264000  |
| value_loss         | 0.00235  |
---------------------------------
---------------------------------
| avg reward         | 0.732    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.938    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 90900    |
| policy_entropy     | 1.51     |
| policy_loss        | -0.0346  |
| total_timesteps    | 7272000  |
| value_loss         | 0.00904  |
---------------------------------
---------------------------------
| avg reward         | 1.15     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.957    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 91000    |
| policy_entropy     | 1.59     |
| policy_loss        | 0.0215   |
| total_timesteps    | 7280000  |
| value_loss         | 0.00157  |
---------------------------------
---------------------------------
| avg reward         | 1.22     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.974    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 91100    |
| policy_entropy     | 1.62     |
| policy_loss        | -0.0375  |
| total_timesteps    | 7288000  |
| value_loss         | 0.00166  |
---------------------------------
---------------------------------
| avg reward         | 0.966    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.945    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 91200    |
| policy_entropy     | 1.55     |
| policy_loss        | 0.0375   |
| total_timesteps    | 7296000  |
| value_loss         | 0.00444  |
---------------------------------
---------------------------------
| avg reward         | 1.14     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.948    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 91300    |
| policy_entropy     | 1.65     |
| policy_loss        | 0.0102   |
| total_timesteps    | 7304000  |
| value_loss         | 0.0028   |
---------------------------------
---------------------------------
| avg reward         | 0.864    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.885    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 91400    |
| policy_entropy     | 1.67     |
| policy_loss        | -0.0546  |
| total_timesteps    | 7312000  |
| value_loss         | 0.0187   |
---------------------------------
---------------------------------
| avg reward         | 0.793    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.967    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 91500    |
| policy_entropy     | 1.58     |
| policy_loss        | -0.039   |
| total_timesteps    | 7320000  |
| value_loss         | 0.00279  |
---------------------------------
---------------------------------
| avg reward         | 0.697    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.956    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 91600    |
| policy_entropy     | 1.64     |
| policy_loss        | 0.00343  |
| total_timesteps    | 7328000  |
| value_loss         | 0.00443  |
---------------------------------
---------------------------------
| avg reward         | 0.876    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.959    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 91700    |
| policy_entropy     | 1.69     |
| policy_loss        | 0.00189  |
| total_timesteps    | 7336000  |
| value_loss         | 0.00254  |
---------------------------------
---------------------------------
| avg reward         | 1.04     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.981    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 91800    |
| policy_entropy     | 1.58     |
| policy_loss        | -0.0396  |
| total_timesteps    | 7344000  |
| value_loss         | 0.00191  |
---------------------------------
---------------------------------
| avg reward         | 0.909    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.988    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 91900    |
| policy_entropy     | 1.7      |
| policy_loss        | 0.0128   |
| total_timesteps    | 7352000  |
| value_loss         | 0.00129  |
---------------------------------
---------------------------------
| avg reward         | 0.874    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.98     |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 92000    |
| policy_entropy     | 1.69     |
| policy_loss        | -0.0193  |
| total_timesteps    | 7360000  |
| value_loss         | 0.00365  |
---------------------------------
---------------------------------
| avg reward         | 0.972    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.957    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 92100    |
| policy_entropy     | 1.71     |
| policy_loss        | 0.00245  |
| total_timesteps    | 7368000  |
| value_loss         | 0.00337  |
---------------------------------
---------------------------------
| avg reward         | 0.896    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.941    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 92200    |
| policy_entropy     | 1.42     |
| policy_loss        | -0.0128  |
| total_timesteps    | 7376000  |
| value_loss         | 0.00451  |
---------------------------------
---------------------------------
| avg reward         | 0.937    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.989    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 92300    |
| policy_entropy     | 1.62     |
| policy_loss        | -0.018   |
| total_timesteps    | 7384000  |
| value_loss         | 0.00185  |
---------------------------------
---------------------------------
| avg reward         | 0.949    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.98     |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 92400    |
| policy_entropy     | 1.77     |
| policy_loss        | -0.00135 |
| total_timesteps    | 7392000  |
| value_loss         | 0.000897 |
---------------------------------
---------------------------------
| avg reward         | 0.723    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.964    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 92500    |
| policy_entropy     | 1.53     |
| policy_loss        | 0.00862  |
| total_timesteps    | 7400000  |
| value_loss         | 0.00365  |
---------------------------------
---------------------------------
| avg reward         | 0.833    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.972    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 92600    |
| policy_entropy     | 1.66     |
| policy_loss        | 0.0233   |
| total_timesteps    | 7408000  |
| value_loss         | 0.00595  |
---------------------------------
---------------------------------
| avg reward         | 0.914    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.976    |
| fps                | 2135     |
| learning rate      | 0.001    |
| nupdates           | 92700    |
| policy_entropy     | 1.58     |
| policy_loss        | -0.031   |
| total_timesteps    | 7416000  |
| value_loss         | 0.00326  |
---------------------------------
---------------------------------
| avg reward         | 0.852    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.929    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 92800    |
| policy_entropy     | 1.66     |
| policy_loss        | -0.0157  |
| total_timesteps    | 7424000  |
| value_loss         | 0.00589  |
---------------------------------
---------------------------------
| avg reward         | 1.11     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.951    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 92900    |
| policy_entropy     | 1.58     |
| policy_loss        | -0.0402  |
| total_timesteps    | 7432000  |
| value_loss         | 0.00668  |
---------------------------------
---------------------------------
| avg reward         | 0.611    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.974    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 93000    |
| policy_entropy     | 1.65     |
| policy_loss        | -0.0101  |
| total_timesteps    | 7440000  |
| value_loss         | 0.00278  |
---------------------------------
---------------------------------
| avg reward         | 0.829    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.931    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 93100    |
| policy_entropy     | 1.51     |
| policy_loss        | -0.00891 |
| total_timesteps    | 7448000  |
| value_loss         | 0.0105   |
---------------------------------
---------------------------------
| avg reward         | 0.671    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.964    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 93200    |
| policy_entropy     | 1.71     |
| policy_loss        | -0.0271  |
| total_timesteps    | 7456000  |
| value_loss         | 0.00429  |
---------------------------------
---------------------------------
| avg reward         | 0.872    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.937    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 93300    |
| policy_entropy     | 1.61     |
| policy_loss        | -0.0385  |
| total_timesteps    | 7464000  |
| value_loss         | 0.00811  |
---------------------------------
---------------------------------
| avg reward         | 0.725    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.98     |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 93400    |
| policy_entropy     | 1.7      |
| policy_loss        | 0.0546   |
| total_timesteps    | 7472000  |
| value_loss         | 0.00455  |
---------------------------------
---------------------------------
| avg reward         | 1.17     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.976    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 93500    |
| policy_entropy     | 1.58     |
| policy_loss        | -0.00622 |
| total_timesteps    | 7480000  |
| value_loss         | 0.00202  |
---------------------------------
---------------------------------
| avg reward         | 1.11     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.992    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 93600    |
| policy_entropy     | 1.61     |
| policy_loss        | -0.026   |
| total_timesteps    | 7488000  |
| value_loss         | 0.000938 |
---------------------------------
---------------------------------
| avg reward         | 0.884    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.957    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 93700    |
| policy_entropy     | 1.52     |
| policy_loss        | 0.0088   |
| total_timesteps    | 7496000  |
| value_loss         | 0.00498  |
---------------------------------
---------------------------------
| avg reward         | 0.757    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.978    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 93800    |
| policy_entropy     | 1.51     |
| policy_loss        | 0.0197   |
| total_timesteps    | 7504000  |
| value_loss         | 0.00208  |
---------------------------------
---------------------------------
| avg reward         | 1.08     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.982    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 93900    |
| policy_entropy     | 1.56     |
| policy_loss        | -0.0076  |
| total_timesteps    | 7512000  |
| value_loss         | 0.00149  |
---------------------------------
---------------------------------
| avg reward         | 0.97     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.978    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 94000    |
| policy_entropy     | 1.66     |
| policy_loss        | -0.0449  |
| total_timesteps    | 7520000  |
| value_loss         | 0.00441  |
---------------------------------
---------------------------------
| avg reward         | 0.78     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.984    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 94100    |
| policy_entropy     | 1.47     |
| policy_loss        | 0.00433  |
| total_timesteps    | 7528000  |
| value_loss         | 0.00229  |
---------------------------------
---------------------------------
| avg reward         | 0.909    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.978    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 94200    |
| policy_entropy     | 1.69     |
| policy_loss        | -0.024   |
| total_timesteps    | 7536000  |
| value_loss         | 0.00302  |
---------------------------------
---------------------------------
| avg reward         | 0.723    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.97     |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 94300    |
| policy_entropy     | 1.71     |
| policy_loss        | -0.0102  |
| total_timesteps    | 7544000  |
| value_loss         | 0.00284  |
---------------------------------
---------------------------------
| avg reward         | 0.662    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.943    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 94400    |
| policy_entropy     | 1.55     |
| policy_loss        | -0.0059  |
| total_timesteps    | 7552000  |
| value_loss         | 0.0083   |
---------------------------------
---------------------------------
| avg reward         | 1.05     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.877    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 94500    |
| policy_entropy     | 1.66     |
| policy_loss        | -0.0715  |
| total_timesteps    | 7560000  |
| value_loss         | 0.0138   |
---------------------------------
---------------------------------
| avg reward         | 0.874    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.973    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 94600    |
| policy_entropy     | 1.52     |
| policy_loss        | -0.0235  |
| total_timesteps    | 7568000  |
| value_loss         | 0.00286  |
---------------------------------
---------------------------------
| avg reward         | 0.931    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.96     |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 94700    |
| policy_entropy     | 1.74     |
| policy_loss        | 0.0519   |
| total_timesteps    | 7576000  |
| value_loss         | 0.0036   |
---------------------------------
---------------------------------
| avg reward         | 0.815    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.984    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 94800    |
| policy_entropy     | 1.52     |
| policy_loss        | 0.0069   |
| total_timesteps    | 7584000  |
| value_loss         | 0.00254  |
---------------------------------
---------------------------------
| avg reward         | 0.864    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.948    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 94900    |
| policy_entropy     | 1.53     |
| policy_loss        | -0.0327  |
| total_timesteps    | 7592000  |
| value_loss         | 0.00716  |
---------------------------------
---------------------------------
| avg reward         | 1.06     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.964    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 95000    |
| policy_entropy     | 1.47     |
| policy_loss        | -0.0109  |
| total_timesteps    | 7600000  |
| value_loss         | 0.00132  |
---------------------------------
---------------------------------
| avg reward         | 0.909    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.985    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 95100    |
| policy_entropy     | 1.52     |
| policy_loss        | -0.0153  |
| total_timesteps    | 7608000  |
| value_loss         | 0.00206  |
---------------------------------
---------------------------------
| avg reward         | 0.836    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.913    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 95200    |
| policy_entropy     | 1.53     |
| policy_loss        | -0.0367  |
| total_timesteps    | 7616000  |
| value_loss         | 0.00825  |
---------------------------------
---------------------------------
| avg reward         | 0.877    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.985    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 95300    |
| policy_entropy     | 1.63     |
| policy_loss        | 0.0262   |
| total_timesteps    | 7624000  |
| value_loss         | 0.00128  |
---------------------------------
---------------------------------
| avg reward         | 1.16     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.986    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 95400    |
| policy_entropy     | 1.51     |
| policy_loss        | -0.00928 |
| total_timesteps    | 7632000  |
| value_loss         | 0.00232  |
---------------------------------
---------------------------------
| avg reward         | 0.961    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.912    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 95500    |
| policy_entropy     | 1.58     |
| policy_loss        | 0.00953  |
| total_timesteps    | 7640000  |
| value_loss         | 0.00431  |
---------------------------------
---------------------------------
| avg reward         | 0.729    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.9      |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 95600    |
| policy_entropy     | 1.7      |
| policy_loss        | -0.0273  |
| total_timesteps    | 7648000  |
| value_loss         | 0.00688  |
---------------------------------
---------------------------------
| avg reward         | 0.839    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.967    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 95700    |
| policy_entropy     | 1.58     |
| policy_loss        | -0.00274 |
| total_timesteps    | 7656000  |
| value_loss         | 0.00147  |
---------------------------------
---------------------------------
| avg reward         | 1.12     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.982    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 95800    |
| policy_entropy     | 1.55     |
| policy_loss        | -0.0389  |
| total_timesteps    | 7664000  |
| value_loss         | 0.00273  |
---------------------------------
---------------------------------
| avg reward         | 0.918    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.985    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 95900    |
| policy_entropy     | 1.54     |
| policy_loss        | 0.000505 |
| total_timesteps    | 7672000  |
| value_loss         | 0.00204  |
---------------------------------
---------------------------------
| avg reward         | 0.534    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.85     |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 96000    |
| policy_entropy     | 1.52     |
| policy_loss        | -0.0575  |
| total_timesteps    | 7680000  |
| value_loss         | 0.0278   |
---------------------------------
---------------------------------
| avg reward         | 0.925    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.976    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 96100    |
| policy_entropy     | 1.73     |
| policy_loss        | -0.0209  |
| total_timesteps    | 7688000  |
| value_loss         | 0.00167  |
---------------------------------
---------------------------------
| avg reward         | 0.475    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.951    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 96200    |
| policy_entropy     | 1.58     |
| policy_loss        | -0.00901 |
| total_timesteps    | 7696000  |
| value_loss         | 0.00539  |
---------------------------------
---------------------------------
| avg reward         | 0.486    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.8      |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 96300    |
| policy_entropy     | 1.36     |
| policy_loss        | -0.00385 |
| total_timesteps    | 7704000  |
| value_loss         | 0.0314   |
---------------------------------
---------------------------------
| avg reward         | 0.833    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.965    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 96400    |
| policy_entropy     | 1.65     |
| policy_loss        | 0.0654   |
| total_timesteps    | 7712000  |
| value_loss         | 0.00521  |
---------------------------------
---------------------------------
| avg reward         | 0.894    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.992    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 96500    |
| policy_entropy     | 1.56     |
| policy_loss        | 0.015    |
| total_timesteps    | 7720000  |
| value_loss         | 0.00117  |
---------------------------------
---------------------------------
| avg reward         | 0.844    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.975    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 96600    |
| policy_entropy     | 1.62     |
| policy_loss        | -0.0252  |
| total_timesteps    | 7728000  |
| value_loss         | 0.00438  |
---------------------------------
---------------------------------
| avg reward         | 0.758    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.975    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 96700    |
| policy_entropy     | 1.64     |
| policy_loss        | 0.0264   |
| total_timesteps    | 7736000  |
| value_loss         | 0.00292  |
---------------------------------
---------------------------------
| avg reward         | 0.7      |
| epsilonValue       | 0.000727 |
| explained_variance | 0.986    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 96800    |
| policy_entropy     | 1.6      |
| policy_loss        | -0.0186  |
| total_timesteps    | 7744000  |
| value_loss         | 0.00173  |
---------------------------------
---------------------------------
| avg reward         | 0.894    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.969    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 96900    |
| policy_entropy     | 1.65     |
| policy_loss        | 0.00638  |
| total_timesteps    | 7752000  |
| value_loss         | 0.00373  |
---------------------------------
---------------------------------
| avg reward         | 0.551    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.976    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 97000    |
| policy_entropy     | 1.61     |
| policy_loss        | -0.0166  |
| total_timesteps    | 7760000  |
| value_loss         | 0.00266  |
---------------------------------
---------------------------------
| avg reward         | 0.512    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.82     |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 97100    |
| policy_entropy     | 1.57     |
| policy_loss        | -0.00432 |
| total_timesteps    | 7768000  |
| value_loss         | 0.0203   |
---------------------------------
---------------------------------
| avg reward         | 0.962    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.957    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 97200    |
| policy_entropy     | 1.55     |
| policy_loss        | 0.000602 |
| total_timesteps    | 7776000  |
| value_loss         | 0.00262  |
---------------------------------
---------------------------------
| avg reward         | 1.07     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.98     |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 97300    |
| policy_entropy     | 1.59     |
| policy_loss        | -0.0136  |
| total_timesteps    | 7784000  |
| value_loss         | 0.00289  |
---------------------------------
---------------------------------
| avg reward         | 0.754    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.957    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 97400    |
| policy_entropy     | 1.57     |
| policy_loss        | -0.0359  |
| total_timesteps    | 7792000  |
| value_loss         | 0.00394  |
---------------------------------
---------------------------------
| avg reward         | 1.1      |
| epsilonValue       | 0.000727 |
| explained_variance | 0.98     |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 97500    |
| policy_entropy     | 1.56     |
| policy_loss        | 0.0149   |
| total_timesteps    | 7800000  |
| value_loss         | 0.00147  |
---------------------------------
----------------------------------
| avg reward         | 1.17      |
| epsilonValue       | 0.000727  |
| explained_variance | 0.974     |
| fps                | 2136      |
| learning rate      | 0.001     |
| nupdates           | 97600     |
| policy_entropy     | 1.46      |
| policy_loss        | -0.000529 |
| total_timesteps    | 7808000   |
| value_loss         | 0.00116   |
----------------------------------
---------------------------------
| avg reward         | 0.943    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.841    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 97700    |
| policy_entropy     | 1.59     |
| policy_loss        | -0.0431  |
| total_timesteps    | 7816000  |
| value_loss         | 0.0219   |
---------------------------------
---------------------------------
| avg reward         | 0.657    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.977    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 97800    |
| policy_entropy     | 1.57     |
| policy_loss        | -0.0464  |
| total_timesteps    | 7824000  |
| value_loss         | 0.00359  |
---------------------------------
---------------------------------
| avg reward         | 0.685    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.955    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 97900    |
| policy_entropy     | 1.53     |
| policy_loss        | -0.0274  |
| total_timesteps    | 7832000  |
| value_loss         | 0.00726  |
---------------------------------
---------------------------------
| avg reward         | 1.02     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.917    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 98000    |
| policy_entropy     | 1.51     |
| policy_loss        | -0.0423  |
| total_timesteps    | 7840000  |
| value_loss         | 0.00705  |
---------------------------------
---------------------------------
| avg reward         | 0.828    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.918    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 98100    |
| policy_entropy     | 1.55     |
| policy_loss        | 0.0369   |
| total_timesteps    | 7848000  |
| value_loss         | 0.00799  |
---------------------------------
---------------------------------
| avg reward         | 0.8      |
| epsilonValue       | 0.000727 |
| explained_variance | 0.501    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 98200    |
| policy_entropy     | 1.58     |
| policy_loss        | -0.0675  |
| total_timesteps    | 7856000  |
| value_loss         | 0.0516   |
---------------------------------
---------------------------------
| avg reward         | 0.93     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.984    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 98300    |
| policy_entropy     | 1.58     |
| policy_loss        | -0.00368 |
| total_timesteps    | 7864000  |
| value_loss         | 0.00156  |
---------------------------------
---------------------------------
| avg reward         | 0.684    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.962    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 98400    |
| policy_entropy     | 1.44     |
| policy_loss        | 0.0265   |
| total_timesteps    | 7872000  |
| value_loss         | 0.00455  |
---------------------------------
---------------------------------
| avg reward         | 0.627    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.918    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 98500    |
| policy_entropy     | 1.68     |
| policy_loss        | 0.0959   |
| total_timesteps    | 7880000  |
| value_loss         | 0.0109   |
---------------------------------
---------------------------------
| avg reward         | 0.916    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.552    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 98600    |
| policy_entropy     | 1.52     |
| policy_loss        | -0.0428  |
| total_timesteps    | 7888000  |
| value_loss         | 0.0178   |
---------------------------------
---------------------------------
| avg reward         | 1.04     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.981    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 98700    |
| policy_entropy     | 1.57     |
| policy_loss        | -0.0125  |
| total_timesteps    | 7896000  |
| value_loss         | 0.00236  |
---------------------------------
---------------------------------
| avg reward         | 0.817    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.934    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 98800    |
| policy_entropy     | 1.66     |
| policy_loss        | 0.00852  |
| total_timesteps    | 7904000  |
| value_loss         | 0.00892  |
---------------------------------
---------------------------------
| avg reward         | 0.903    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.987    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 98900    |
| policy_entropy     | 1.54     |
| policy_loss        | 0.0134   |
| total_timesteps    | 7912000  |
| value_loss         | 0.00157  |
---------------------------------
----------------------------------
| avg reward         | 0.877     |
| epsilonValue       | 0.000727  |
| explained_variance | 0.982     |
| fps                | 2136      |
| learning rate      | 0.001     |
| nupdates           | 99000     |
| policy_entropy     | 1.62      |
| policy_loss        | -0.000586 |
| total_timesteps    | 7920000   |
| value_loss         | 0.00251   |
----------------------------------
---------------------------------
| avg reward         | 0.863    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.93     |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 99100    |
| policy_entropy     | 1.61     |
| policy_loss        | -0.0167  |
| total_timesteps    | 7928000  |
| value_loss         | 0.00611  |
---------------------------------
---------------------------------
| avg reward         | 0.965    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.988    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 99200    |
| policy_entropy     | 1.58     |
| policy_loss        | -0.0227  |
| total_timesteps    | 7936000  |
| value_loss         | 0.00221  |
---------------------------------
---------------------------------
| avg reward         | 0.798    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.97     |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 99300    |
| policy_entropy     | 1.75     |
| policy_loss        | -0.0131  |
| total_timesteps    | 7944000  |
| value_loss         | 0.0027   |
---------------------------------
---------------------------------
| avg reward         | 0.731    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.966    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 99400    |
| policy_entropy     | 1.48     |
| policy_loss        | 0.013    |
| total_timesteps    | 7952000  |
| value_loss         | 0.00364  |
---------------------------------
---------------------------------
| avg reward         | 1.13     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.98     |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 99500    |
| policy_entropy     | 1.54     |
| policy_loss        | 0.0107   |
| total_timesteps    | 7960000  |
| value_loss         | 0.00271  |
---------------------------------
---------------------------------
| avg reward         | 0.821    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.974    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 99600    |
| policy_entropy     | 1.73     |
| policy_loss        | -0.0195  |
| total_timesteps    | 7968000  |
| value_loss         | 0.00404  |
---------------------------------
---------------------------------
| avg reward         | 1.17     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.926    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 99700    |
| policy_entropy     | 1.54     |
| policy_loss        | -0.00277 |
| total_timesteps    | 7976000  |
| value_loss         | 0.00343  |
---------------------------------
---------------------------------
| avg reward         | 1.16     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.974    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 99800    |
| policy_entropy     | 1.53     |
| policy_loss        | -0.0205  |
| total_timesteps    | 7984000  |
| value_loss         | 0.00231  |
---------------------------------
---------------------------------
| avg reward         | 0.93     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.952    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 99900    |
| policy_entropy     | 1.65     |
| policy_loss        | -0.0157  |
| total_timesteps    | 7992000  |
| value_loss         | 0.00199  |
---------------------------------
---------------------------------
| avg reward         | 0.953    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.947    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 100000   |
| policy_entropy     | 1.44     |
| policy_loss        | -0.00255 |
| total_timesteps    | 8000000  |
| value_loss         | 0.00404  |
---------------------------------
---------------------------------
| avg reward         | 1        |
| epsilonValue       | 0.000727 |
| explained_variance | 0.984    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 100100   |
| policy_entropy     | 1.55     |
| policy_loss        | -0.0108  |
| total_timesteps    | 8008000  |
| value_loss         | 0.00177  |
---------------------------------
---------------------------------
| avg reward         | 1.08     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.953    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 100200   |
| policy_entropy     | 1.68     |
| policy_loss        | 0.0316   |
| total_timesteps    | 8016000  |
| value_loss         | 0.00226  |
---------------------------------
---------------------------------
| avg reward         | 0.814    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.944    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 100300   |
| policy_entropy     | 1.48     |
| policy_loss        | -0.0146  |
| total_timesteps    | 8024000  |
| value_loss         | 0.00719  |
---------------------------------
---------------------------------
| avg reward         | 0.703    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.98     |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 100400   |
| policy_entropy     | 1.63     |
| policy_loss        | 0.0224   |
| total_timesteps    | 8032000  |
| value_loss         | 0.00185  |
---------------------------------
---------------------------------
| avg reward         | 0.565    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.909    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 100500   |
| policy_entropy     | 1.64     |
| policy_loss        | 0.0179   |
| total_timesteps    | 8040000  |
| value_loss         | 0.0118   |
---------------------------------
---------------------------------
| avg reward         | 0.749    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.947    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 100600   |
| policy_entropy     | 1.55     |
| policy_loss        | 0.0594   |
| total_timesteps    | 8048000  |
| value_loss         | 0.00362  |
---------------------------------
---------------------------------
| avg reward         | 0.759    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.91     |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 100700   |
| policy_entropy     | 1.42     |
| policy_loss        | -0.00259 |
| total_timesteps    | 8056000  |
| value_loss         | 0.00601  |
---------------------------------
---------------------------------
| avg reward         | 0.76     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.978    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 100800   |
| policy_entropy     | 1.59     |
| policy_loss        | -0.00823 |
| total_timesteps    | 8064000  |
| value_loss         | 0.00277  |
---------------------------------
---------------------------------
| avg reward         | 1.17     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.966    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 100900   |
| policy_entropy     | 1.61     |
| policy_loss        | 0.0293   |
| total_timesteps    | 8072000  |
| value_loss         | 0.00173  |
---------------------------------
----------------------------------
| avg reward         | 1.02      |
| epsilonValue       | 0.000727  |
| explained_variance | 0.979     |
| fps                | 2136      |
| learning rate      | 0.001     |
| nupdates           | 101000    |
| policy_entropy     | 1.75      |
| policy_loss        | -0.000348 |
| total_timesteps    | 8080000   |
| value_loss         | 0.000943  |
----------------------------------
---------------------------------
| avg reward         | 0.833    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.988    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 101100   |
| policy_entropy     | 1.64     |
| policy_loss        | 0.015    |
| total_timesteps    | 8088000  |
| value_loss         | 0.00105  |
---------------------------------
---------------------------------
| avg reward         | 1.04     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.992    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 101200   |
| policy_entropy     | 1.6      |
| policy_loss        | -0.00129 |
| total_timesteps    | 8096000  |
| value_loss         | 0.000872 |
---------------------------------
---------------------------------
| avg reward         | 0.817    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.984    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 101300   |
| policy_entropy     | 1.66     |
| policy_loss        | -0.0352  |
| total_timesteps    | 8104000  |
| value_loss         | 0.00257  |
---------------------------------
---------------------------------
| avg reward         | 1.07     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.872    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 101400   |
| policy_entropy     | 1.64     |
| policy_loss        | 0.0409   |
| total_timesteps    | 8112000  |
| value_loss         | 0.00743  |
---------------------------------
---------------------------------
| avg reward         | 1.1      |
| epsilonValue       | 0.000727 |
| explained_variance | 0.953    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 101500   |
| policy_entropy     | 1.59     |
| policy_loss        | -0.0292  |
| total_timesteps    | 8120000  |
| value_loss         | 0.00485  |
---------------------------------
---------------------------------
| avg reward         | 1.11     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.97     |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 101600   |
| policy_entropy     | 1.74     |
| policy_loss        | 0.0166   |
| total_timesteps    | 8128000  |
| value_loss         | 0.00229  |
---------------------------------
---------------------------------
| avg reward         | 1.05     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.934    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 101700   |
| policy_entropy     | 1.53     |
| policy_loss        | -0.0209  |
| total_timesteps    | 8136000  |
| value_loss         | 0.00322  |
---------------------------------
---------------------------------
| avg reward         | 0.755    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.917    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 101800   |
| policy_entropy     | 1.59     |
| policy_loss        | -0.00783 |
| total_timesteps    | 8144000  |
| value_loss         | 0.00529  |
---------------------------------
---------------------------------
| avg reward         | 1.11     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.935    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 101900   |
| policy_entropy     | 1.67     |
| policy_loss        | 0.0118   |
| total_timesteps    | 8152000  |
| value_loss         | 0.00512  |
---------------------------------
---------------------------------
| avg reward         | 0.833    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.894    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 102000   |
| policy_entropy     | 1.57     |
| policy_loss        | -0.0145  |
| total_timesteps    | 8160000  |
| value_loss         | 0.0074   |
---------------------------------
---------------------------------
| avg reward         | 0.888    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.961    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 102100   |
| policy_entropy     | 1.51     |
| policy_loss        | -0.0529  |
| total_timesteps    | 8168000  |
| value_loss         | 0.00357  |
---------------------------------
---------------------------------
| avg reward         | 0.898    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.966    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 102200   |
| policy_entropy     | 1.56     |
| policy_loss        | 0.0246   |
| total_timesteps    | 8176000  |
| value_loss         | 0.00336  |
---------------------------------
---------------------------------
| avg reward         | 0.815    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.97     |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 102300   |
| policy_entropy     | 1.68     |
| policy_loss        | 0.0428   |
| total_timesteps    | 8184000  |
| value_loss         | 0.00271  |
---------------------------------
----------------------------------
| avg reward         | 1.14      |
| epsilonValue       | 0.000727  |
| explained_variance | 0.974     |
| fps                | 2136      |
| learning rate      | 0.001     |
| nupdates           | 102400    |
| policy_entropy     | 1.6       |
| policy_loss        | -0.000411 |
| total_timesteps    | 8192000   |
| value_loss         | 0.00244   |
----------------------------------
---------------------------------
| avg reward         | 0.934    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.932    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 102500   |
| policy_entropy     | 1.41     |
| policy_loss        | -0.0262  |
| total_timesteps    | 8200000  |
| value_loss         | 0.00942  |
---------------------------------
---------------------------------
| avg reward         | 0.972    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.953    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 102600   |
| policy_entropy     | 1.65     |
| policy_loss        | 0.041    |
| total_timesteps    | 8208000  |
| value_loss         | 0.00426  |
---------------------------------
---------------------------------
| avg reward         | 0.949    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.872    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 102700   |
| policy_entropy     | 1.47     |
| policy_loss        | 0.0125   |
| total_timesteps    | 8216000  |
| value_loss         | 0.0136   |
---------------------------------
---------------------------------
| avg reward         | 1.07     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.968    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 102800   |
| policy_entropy     | 1.55     |
| policy_loss        | -0.00852 |
| total_timesteps    | 8224000  |
| value_loss         | 0.0025   |
---------------------------------
---------------------------------
| avg reward         | 1.1      |
| epsilonValue       | 0.000727 |
| explained_variance | 0.972    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 102900   |
| policy_entropy     | 1.63     |
| policy_loss        | 0.00207  |
| total_timesteps    | 8232000  |
| value_loss         | 0.00165  |
---------------------------------
---------------------------------
| avg reward         | 1.08     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.968    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 103000   |
| policy_entropy     | 1.69     |
| policy_loss        | -0.014   |
| total_timesteps    | 8240000  |
| value_loss         | 0.00263  |
---------------------------------
---------------------------------
| avg reward         | 0.901    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.962    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 103100   |
| policy_entropy     | 1.75     |
| policy_loss        | -0.0178  |
| total_timesteps    | 8248000  |
| value_loss         | 0.00343  |
---------------------------------
---------------------------------
| avg reward         | 0.788    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.986    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 103200   |
| policy_entropy     | 1.57     |
| policy_loss        | 0.0143   |
| total_timesteps    | 8256000  |
| value_loss         | 0.00167  |
---------------------------------
----------------------------------
| avg reward         | 1.19      |
| epsilonValue       | 0.000727  |
| explained_variance | 0.847     |
| fps                | 2136      |
| learning rate      | 0.001     |
| nupdates           | 103300    |
| policy_entropy     | 1.51      |
| policy_loss        | -0.000378 |
| total_timesteps    | 8264000   |
| value_loss         | 0.00386   |
----------------------------------
---------------------------------
| avg reward         | 0.807    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.902    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 103400   |
| policy_entropy     | 1.63     |
| policy_loss        | -0.0264  |
| total_timesteps    | 8272000  |
| value_loss         | 0.0115   |
---------------------------------
---------------------------------
| avg reward         | 0.997    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.911    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 103500   |
| policy_entropy     | 1.65     |
| policy_loss        | -0.0348  |
| total_timesteps    | 8280000  |
| value_loss         | 0.00562  |
---------------------------------
---------------------------------
| avg reward         | 1.04     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.95     |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 103600   |
| policy_entropy     | 1.59     |
| policy_loss        | -0.0182  |
| total_timesteps    | 8288000  |
| value_loss         | 0.00238  |
---------------------------------
---------------------------------
| avg reward         | 0.997    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.952    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 103700   |
| policy_entropy     | 1.69     |
| policy_loss        | -0.00518 |
| total_timesteps    | 8296000  |
| value_loss         | 0.00499  |
---------------------------------
---------------------------------
| avg reward         | 1.2      |
| epsilonValue       | 0.000727 |
| explained_variance | 0.955    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 103800   |
| policy_entropy     | 1.74     |
| policy_loss        | -0.0127  |
| total_timesteps    | 8304000  |
| value_loss         | 0.00244  |
---------------------------------
---------------------------------
| avg reward         | 1.08     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.982    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 103900   |
| policy_entropy     | 1.75     |
| policy_loss        | 0.0201   |
| total_timesteps    | 8312000  |
| value_loss         | 0.00121  |
---------------------------------
---------------------------------
| avg reward         | 1.13     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.981    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 104000   |
| policy_entropy     | 1.55     |
| policy_loss        | 0.0363   |
| total_timesteps    | 8320000  |
| value_loss         | 0.00115  |
---------------------------------
---------------------------------
| avg reward         | 1.09     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.952    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 104100   |
| policy_entropy     | 1.43     |
| policy_loss        | 0.00145  |
| total_timesteps    | 8328000  |
| value_loss         | 0.00325  |
---------------------------------
---------------------------------
| avg reward         | 1.01     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.967    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 104200   |
| policy_entropy     | 1.72     |
| policy_loss        | -0.00937 |
| total_timesteps    | 8336000  |
| value_loss         | 0.00202  |
---------------------------------
---------------------------------
| avg reward         | 1.17     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.965    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 104300   |
| policy_entropy     | 1.54     |
| policy_loss        | -0.00349 |
| total_timesteps    | 8344000  |
| value_loss         | 0.00127  |
---------------------------------
---------------------------------
| avg reward         | 0.961    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.885    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 104400   |
| policy_entropy     | 1.62     |
| policy_loss        | 0.0125   |
| total_timesteps    | 8352000  |
| value_loss         | 0.0101   |
---------------------------------
---------------------------------
| avg reward         | 0.97     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.982    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 104500   |
| policy_entropy     | 1.61     |
| policy_loss        | -0.0138  |
| total_timesteps    | 8360000  |
| value_loss         | 0.00166  |
---------------------------------
---------------------------------
| avg reward         | 1        |
| epsilonValue       | 0.000727 |
| explained_variance | 0.729    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 104600   |
| policy_entropy     | 1.68     |
| policy_loss        | -0.0842  |
| total_timesteps    | 8368000  |
| value_loss         | 0.0191   |
---------------------------------
---------------------------------
| avg reward         | 1.18     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.945    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 104700   |
| policy_entropy     | 1.43     |
| policy_loss        | -0.00559 |
| total_timesteps    | 8376000  |
| value_loss         | 0.00221  |
---------------------------------
---------------------------------
| avg reward         | 0.783    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.964    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 104800   |
| policy_entropy     | 1.69     |
| policy_loss        | -0.00129 |
| total_timesteps    | 8384000  |
| value_loss         | 0.00393  |
---------------------------------
---------------------------------
| avg reward         | 1.02     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.958    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 104900   |
| policy_entropy     | 1.53     |
| policy_loss        | 0.0064   |
| total_timesteps    | 8392000  |
| value_loss         | 0.00353  |
---------------------------------
---------------------------------
| avg reward         | 1.05     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.948    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 105000   |
| policy_entropy     | 1.41     |
| policy_loss        | 0.00175  |
| total_timesteps    | 8400000  |
| value_loss         | 0.00281  |
---------------------------------
----------------------------------
| avg reward         | 1.11      |
| epsilonValue       | 0.000727  |
| explained_variance | 0.96      |
| fps                | 2136      |
| learning rate      | 0.001     |
| nupdates           | 105100    |
| policy_entropy     | 1.59      |
| policy_loss        | -0.000194 |
| total_timesteps    | 8408000   |
| value_loss         | 0.00137   |
----------------------------------
---------------------------------
| avg reward         | 1.18     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.967    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 105200   |
| policy_entropy     | 1.56     |
| policy_loss        | 0.0208   |
| total_timesteps    | 8416000  |
| value_loss         | 0.00199  |
---------------------------------
---------------------------------
| avg reward         | 1.08     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.955    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 105300   |
| policy_entropy     | 1.57     |
| policy_loss        | -0.00968 |
| total_timesteps    | 8424000  |
| value_loss         | 0.00414  |
---------------------------------
---------------------------------
| avg reward         | 0.908    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.931    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 105400   |
| policy_entropy     | 1.57     |
| policy_loss        | -0.0357  |
| total_timesteps    | 8432000  |
| value_loss         | 0.00533  |
---------------------------------
---------------------------------
| avg reward         | 0.933    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.977    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 105500   |
| policy_entropy     | 1.46     |
| policy_loss        | 0.0348   |
| total_timesteps    | 8440000  |
| value_loss         | 0.00245  |
---------------------------------
---------------------------------
| avg reward         | 0.942    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.964    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 105600   |
| policy_entropy     | 1.7      |
| policy_loss        | -0.0509  |
| total_timesteps    | 8448000  |
| value_loss         | 0.00278  |
---------------------------------
---------------------------------
| avg reward         | 0.983    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.973    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 105700   |
| policy_entropy     | 1.63     |
| policy_loss        | -0.01    |
| total_timesteps    | 8456000  |
| value_loss         | 0.00368  |
---------------------------------
---------------------------------
| avg reward         | 1.05     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.992    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 105800   |
| policy_entropy     | 1.77     |
| policy_loss        | -0.00118 |
| total_timesteps    | 8464000  |
| value_loss         | 0.00048  |
---------------------------------
---------------------------------
| avg reward         | 1.31     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.982    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 105900   |
| policy_entropy     | 1.63     |
| policy_loss        | 0.0176   |
| total_timesteps    | 8472000  |
| value_loss         | 0.0015   |
---------------------------------
---------------------------------
| avg reward         | 1.12     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.982    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 106000   |
| policy_entropy     | 1.63     |
| policy_loss        | 0.000141 |
| total_timesteps    | 8480000  |
| value_loss         | 0.00199  |
---------------------------------
---------------------------------
| avg reward         | 0.941    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.97     |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 106100   |
| policy_entropy     | 1.64     |
| policy_loss        | 0.0169   |
| total_timesteps    | 8488000  |
| value_loss         | 0.00114  |
---------------------------------
---------------------------------
| avg reward         | 1.07     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.981    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 106200   |
| policy_entropy     | 1.38     |
| policy_loss        | 0.00187  |
| total_timesteps    | 8496000  |
| value_loss         | 0.0014   |
---------------------------------
---------------------------------
| avg reward         | 1.19     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.985    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 106300   |
| policy_entropy     | 1.61     |
| policy_loss        | -0.029   |
| total_timesteps    | 8504000  |
| value_loss         | 0.00157  |
---------------------------------
---------------------------------
| avg reward         | 0.922    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.982    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 106400   |
| policy_entropy     | 1.65     |
| policy_loss        | 0.00688  |
| total_timesteps    | 8512000  |
| value_loss         | 0.00247  |
---------------------------------
---------------------------------
| avg reward         | 1.2      |
| epsilonValue       | 0.000727 |
| explained_variance | 0.969    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 106500   |
| policy_entropy     | 1.62     |
| policy_loss        | -0.00112 |
| total_timesteps    | 8520000  |
| value_loss         | 0.00101  |
---------------------------------
---------------------------------
| avg reward         | 0.93     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.965    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 106600   |
| policy_entropy     | 1.47     |
| policy_loss        | -0.0236  |
| total_timesteps    | 8528000  |
| value_loss         | 0.00304  |
---------------------------------
---------------------------------
| avg reward         | 1.03     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.988    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 106700   |
| policy_entropy     | 1.69     |
| policy_loss        | 0.000438 |
| total_timesteps    | 8536000  |
| value_loss         | 0.000794 |
---------------------------------
---------------------------------
| avg reward         | 0.99     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.972    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 106800   |
| policy_entropy     | 1.74     |
| policy_loss        | 0.00392  |
| total_timesteps    | 8544000  |
| value_loss         | 0.00214  |
---------------------------------
---------------------------------
| avg reward         | 1.29     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.978    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 106900   |
| policy_entropy     | 1.59     |
| policy_loss        | 0.0433   |
| total_timesteps    | 8552000  |
| value_loss         | 0.00207  |
---------------------------------
---------------------------------
| avg reward         | 1.02     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.622    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 107000   |
| policy_entropy     | 1.56     |
| policy_loss        | -0.0282  |
| total_timesteps    | 8560000  |
| value_loss         | 0.0213   |
---------------------------------
---------------------------------
| avg reward         | 1.21     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.952    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 107100   |
| policy_entropy     | 1.69     |
| policy_loss        | 0.0128   |
| total_timesteps    | 8568000  |
| value_loss         | 0.00385  |
---------------------------------
---------------------------------
| avg reward         | 1.14     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.978    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 107200   |
| policy_entropy     | 1.43     |
| policy_loss        | 0.00604  |
| total_timesteps    | 8576000  |
| value_loss         | 0.000967 |
---------------------------------
---------------------------------
| avg reward         | 0.924    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.984    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 107300   |
| policy_entropy     | 1.57     |
| policy_loss        | -0.041   |
| total_timesteps    | 8584000  |
| value_loss         | 0.00208  |
---------------------------------
---------------------------------
| avg reward         | 1        |
| epsilonValue       | 0.000727 |
| explained_variance | 0.981    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 107400   |
| policy_entropy     | 1.64     |
| policy_loss        | -0.00112 |
| total_timesteps    | 8592000  |
| value_loss         | 0.00228  |
---------------------------------
---------------------------------
| avg reward         | 1.15     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.965    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 107500   |
| policy_entropy     | 1.53     |
| policy_loss        | 0.0271   |
| total_timesteps    | 8600000  |
| value_loss         | 0.0035   |
---------------------------------
---------------------------------
| avg reward         | 1.3      |
| epsilonValue       | 0.000727 |
| explained_variance | 0.981    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 107600   |
| policy_entropy     | 1.6      |
| policy_loss        | -0.00151 |
| total_timesteps    | 8608000  |
| value_loss         | 0.000931 |
---------------------------------
---------------------------------
| avg reward         | 1.13     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.977    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 107700   |
| policy_entropy     | 1.53     |
| policy_loss        | -0.0117  |
| total_timesteps    | 8616000  |
| value_loss         | 0.00218  |
---------------------------------
---------------------------------
| avg reward         | 1.01     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.953    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 107800   |
| policy_entropy     | 1.68     |
| policy_loss        | 0.0165   |
| total_timesteps    | 8624000  |
| value_loss         | 0.00131  |
---------------------------------
---------------------------------
| avg reward         | 1.07     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.989    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 107900   |
| policy_entropy     | 1.53     |
| policy_loss        | 0.0152   |
| total_timesteps    | 8632000  |
| value_loss         | 0.000806 |
---------------------------------
---------------------------------
| avg reward         | 0.912    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.936    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 108000   |
| policy_entropy     | 1.68     |
| policy_loss        | 0.0212   |
| total_timesteps    | 8640000  |
| value_loss         | 0.00421  |
---------------------------------
---------------------------------
| avg reward         | 1.25     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.989    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 108100   |
| policy_entropy     | 1.58     |
| policy_loss        | 0.0239   |
| total_timesteps    | 8648000  |
| value_loss         | 0.00106  |
---------------------------------
---------------------------------
| avg reward         | 1.08     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.99     |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 108200   |
| policy_entropy     | 1.64     |
| policy_loss        | 0.00961  |
| total_timesteps    | 8656000  |
| value_loss         | 0.000882 |
---------------------------------
---------------------------------
| avg reward         | 1.13     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.983    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 108300   |
| policy_entropy     | 1.72     |
| policy_loss        | 0.00876  |
| total_timesteps    | 8664000  |
| value_loss         | 0.00109  |
---------------------------------
---------------------------------
| avg reward         | 1.09     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.993    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 108400   |
| policy_entropy     | 1.6      |
| policy_loss        | -0.00543 |
| total_timesteps    | 8672000  |
| value_loss         | 0.000599 |
---------------------------------
---------------------------------
| avg reward         | 1.25     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.985    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 108500   |
| policy_entropy     | 1.59     |
| policy_loss        | 0.00901  |
| total_timesteps    | 8680000  |
| value_loss         | 0.00064  |
---------------------------------
---------------------------------
| avg reward         | 1.05     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.978    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 108600   |
| policy_entropy     | 1.72     |
| policy_loss        | 0.000123 |
| total_timesteps    | 8688000  |
| value_loss         | 0.00236  |
---------------------------------
---------------------------------
| avg reward         | 1.11     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.99     |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 108700   |
| policy_entropy     | 1.55     |
| policy_loss        | -0.0139  |
| total_timesteps    | 8696000  |
| value_loss         | 0.000775 |
---------------------------------
---------------------------------
| avg reward         | 1.17     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.972    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 108800   |
| policy_entropy     | 1.6      |
| policy_loss        | -0.0215  |
| total_timesteps    | 8704000  |
| value_loss         | 0.00339  |
---------------------------------
---------------------------------
| avg reward         | 0.899    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.955    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 108900   |
| policy_entropy     | 1.43     |
| policy_loss        | 0.000219 |
| total_timesteps    | 8712000  |
| value_loss         | 0.0036   |
---------------------------------
---------------------------------
| avg reward         | 1.14     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.969    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 109000   |
| policy_entropy     | 1.54     |
| policy_loss        | -0.0149  |
| total_timesteps    | 8720000  |
| value_loss         | 0.00257  |
---------------------------------
---------------------------------
| avg reward         | 1.04     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.985    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 109100   |
| policy_entropy     | 1.62     |
| policy_loss        | -0.0133  |
| total_timesteps    | 8728000  |
| value_loss         | 0.00166  |
---------------------------------
---------------------------------
| avg reward         | 1.43     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.969    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 109200   |
| policy_entropy     | 1.5      |
| policy_loss        | 0.0164   |
| total_timesteps    | 8736000  |
| value_loss         | 0.00154  |
---------------------------------
---------------------------------
| avg reward         | 0.929    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.991    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 109300   |
| policy_entropy     | 1.76     |
| policy_loss        | 0.00395  |
| total_timesteps    | 8744000  |
| value_loss         | 0.000863 |
---------------------------------
---------------------------------
| avg reward         | 1.28     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.959    |
| fps                | 2136     |
| learning rate      | 0.001    |
| nupdates           | 109400   |
| policy_entropy     | 1.67     |
| policy_loss        | 0.01     |
| total_timesteps    | 8752000  |
| value_loss         | 0.00163  |
---------------------------------
---------------------------------
| avg reward         | 1.13     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.972    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 109500   |
| policy_entropy     | 1.53     |
| policy_loss        | 0.00503  |
| total_timesteps    | 8760000  |
| value_loss         | 0.00233  |
---------------------------------
---------------------------------
| avg reward         | 1.12     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.984    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 109600   |
| policy_entropy     | 1.54     |
| policy_loss        | 0.00967  |
| total_timesteps    | 8768000  |
| value_loss         | 0.000986 |
---------------------------------
---------------------------------
| avg reward         | 0.966    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.964    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 109700   |
| policy_entropy     | 1.61     |
| policy_loss        | 0.034    |
| total_timesteps    | 8776000  |
| value_loss         | 0.00207  |
---------------------------------
---------------------------------
| avg reward         | 1.05     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.981    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 109800   |
| policy_entropy     | 1.63     |
| policy_loss        | 0.0181   |
| total_timesteps    | 8784000  |
| value_loss         | 0.00141  |
---------------------------------
---------------------------------
| avg reward         | 1.08     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.964    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 109900   |
| policy_entropy     | 1.63     |
| policy_loss        | -0.0138  |
| total_timesteps    | 8792000  |
| value_loss         | 0.00187  |
---------------------------------
---------------------------------
| avg reward         | 1.09     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.983    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 110000   |
| policy_entropy     | 1.73     |
| policy_loss        | 0.0218   |
| total_timesteps    | 8800000  |
| value_loss         | 0.000794 |
---------------------------------
---------------------------------
| avg reward         | 0.904    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.868    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 110100   |
| policy_entropy     | 1.31     |
| policy_loss        | -0.0483  |
| total_timesteps    | 8808000  |
| value_loss         | 0.0141   |
---------------------------------
---------------------------------
| avg reward         | 0.972    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.904    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 110200   |
| policy_entropy     | 1.67     |
| policy_loss        | 0.0139   |
| total_timesteps    | 8816000  |
| value_loss         | 0.00202  |
---------------------------------
---------------------------------
| avg reward         | 1.12     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.987    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 110300   |
| policy_entropy     | 1.36     |
| policy_loss        | -0.0131  |
| total_timesteps    | 8824000  |
| value_loss         | 0.000935 |
---------------------------------
---------------------------------
| avg reward         | 1.15     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.977    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 110400   |
| policy_entropy     | 1.55     |
| policy_loss        | -0.0271  |
| total_timesteps    | 8832000  |
| value_loss         | 0.00287  |
---------------------------------
---------------------------------
| avg reward         | 1.17     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.964    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 110500   |
| policy_entropy     | 1.3      |
| policy_loss        | 0.00661  |
| total_timesteps    | 8840000  |
| value_loss         | 0.00125  |
---------------------------------
---------------------------------
| avg reward         | 1.06     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.991    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 110600   |
| policy_entropy     | 1.69     |
| policy_loss        | -0.013   |
| total_timesteps    | 8848000  |
| value_loss         | 0.000583 |
---------------------------------
---------------------------------
| avg reward         | 1.2      |
| epsilonValue       | 0.000727 |
| explained_variance | 0.972    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 110700   |
| policy_entropy     | 1.51     |
| policy_loss        | -0.00149 |
| total_timesteps    | 8856000  |
| value_loss         | 0.00128  |
---------------------------------
---------------------------------
| avg reward         | 1.09     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.983    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 110800   |
| policy_entropy     | 1.65     |
| policy_loss        | -0.0209  |
| total_timesteps    | 8864000  |
| value_loss         | 0.0019   |
---------------------------------
---------------------------------
| avg reward         | 0.936    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.949    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 110900   |
| policy_entropy     | 1.63     |
| policy_loss        | 0.0135   |
| total_timesteps    | 8872000  |
| value_loss         | 0.00419  |
---------------------------------
---------------------------------
| avg reward         | 1.11     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.931    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 111000   |
| policy_entropy     | 1.74     |
| policy_loss        | -0.0185  |
| total_timesteps    | 8880000  |
| value_loss         | 0.00304  |
---------------------------------
---------------------------------
| avg reward         | 1.24     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.987    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 111100   |
| policy_entropy     | 1.73     |
| policy_loss        | 0.0278   |
| total_timesteps    | 8888000  |
| value_loss         | 0.000878 |
---------------------------------
---------------------------------
| avg reward         | 1.1      |
| epsilonValue       | 0.000727 |
| explained_variance | 0.984    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 111200   |
| policy_entropy     | 1.58     |
| policy_loss        | -0.00903 |
| total_timesteps    | 8896000  |
| value_loss         | 0.00115  |
---------------------------------
---------------------------------
| avg reward         | 1.28     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.987    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 111300   |
| policy_entropy     | 1.53     |
| policy_loss        | 0.0085   |
| total_timesteps    | 8904000  |
| value_loss         | 0.000643 |
---------------------------------
---------------------------------
| avg reward         | 0.989    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.986    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 111400   |
| policy_entropy     | 1.63     |
| policy_loss        | 0.0063   |
| total_timesteps    | 8912000  |
| value_loss         | 0.00101  |
---------------------------------
---------------------------------
| avg reward         | 1.11     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.975    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 111500   |
| policy_entropy     | 1.72     |
| policy_loss        | -0.02    |
| total_timesteps    | 8920000  |
| value_loss         | 0.000965 |
---------------------------------
---------------------------------
| avg reward         | 1.13     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.991    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 111600   |
| policy_entropy     | 1.75     |
| policy_loss        | -0.00481 |
| total_timesteps    | 8928000  |
| value_loss         | 0.000936 |
---------------------------------
---------------------------------
| avg reward         | 0.908    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.98     |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 111700   |
| policy_entropy     | 1.6      |
| policy_loss        | -0.0125  |
| total_timesteps    | 8936000  |
| value_loss         | 0.00106  |
---------------------------------
---------------------------------
| avg reward         | 1.11     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.982    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 111800   |
| policy_entropy     | 1.67     |
| policy_loss        | 0.0229   |
| total_timesteps    | 8944000  |
| value_loss         | 0.00125  |
---------------------------------
---------------------------------
| avg reward         | 0.903    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.979    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 111900   |
| policy_entropy     | 1.58     |
| policy_loss        | -0.0423  |
| total_timesteps    | 8952000  |
| value_loss         | 0.00218  |
---------------------------------
---------------------------------
| avg reward         | 1.12     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.992    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 112000   |
| policy_entropy     | 1.7      |
| policy_loss        | 0.0081   |
| total_timesteps    | 8960000  |
| value_loss         | 0.00028  |
---------------------------------
---------------------------------
| avg reward         | 0.826    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.96     |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 112100   |
| policy_entropy     | 1.51     |
| policy_loss        | 0.0143   |
| total_timesteps    | 8968000  |
| value_loss         | 0.00484  |
---------------------------------
---------------------------------
| avg reward         | 1.09     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.994    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 112200   |
| policy_entropy     | 1.77     |
| policy_loss        | -0.0175  |
| total_timesteps    | 8976000  |
| value_loss         | 0.000388 |
---------------------------------
---------------------------------
| avg reward         | 1.18     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.87     |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 112300   |
| policy_entropy     | 1.53     |
| policy_loss        | -0.0748  |
| total_timesteps    | 8984000  |
| value_loss         | 0.00771  |
---------------------------------
---------------------------------
| avg reward         | 1.32     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.995    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 112400   |
| policy_entropy     | 1.76     |
| policy_loss        | 0.0084   |
| total_timesteps    | 8992000  |
| value_loss         | 0.000347 |
---------------------------------
---------------------------------
| avg reward         | 1.21     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.892    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 112500   |
| policy_entropy     | 1.76     |
| policy_loss        | 0.0718   |
| total_timesteps    | 9000000  |
| value_loss         | 0.0121   |
---------------------------------
---------------------------------
| avg reward         | 1.22     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.977    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 112600   |
| policy_entropy     | 1.68     |
| policy_loss        | -0.00576 |
| total_timesteps    | 9008000  |
| value_loss         | 0.000593 |
---------------------------------
---------------------------------
| avg reward         | 1.16     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.78     |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 112700   |
| policy_entropy     | 1.34     |
| policy_loss        | -0.00514 |
| total_timesteps    | 9016000  |
| value_loss         | 0.0184   |
---------------------------------
---------------------------------
| avg reward         | 0.944    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.989    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 112800   |
| policy_entropy     | 1.66     |
| policy_loss        | 0.00369  |
| total_timesteps    | 9024000  |
| value_loss         | 0.00132  |
---------------------------------
---------------------------------
| avg reward         | 0.999    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.983    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 112900   |
| policy_entropy     | 1.7      |
| policy_loss        | -0.00772 |
| total_timesteps    | 9032000  |
| value_loss         | 0.00111  |
---------------------------------
---------------------------------
| avg reward         | 1.35     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.985    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 113000   |
| policy_entropy     | 1.7      |
| policy_loss        | 0.0062   |
| total_timesteps    | 9040000  |
| value_loss         | 0.000853 |
---------------------------------
---------------------------------
| avg reward         | 1.22     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.975    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 113100   |
| policy_entropy     | 1.56     |
| policy_loss        | 0.00477  |
| total_timesteps    | 9048000  |
| value_loss         | 0.000815 |
---------------------------------
---------------------------------
| avg reward         | 1.12     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.952    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 113200   |
| policy_entropy     | 1.53     |
| policy_loss        | -0.0309  |
| total_timesteps    | 9056000  |
| value_loss         | 0.00494  |
---------------------------------
---------------------------------
| avg reward         | 1.09     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.986    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 113300   |
| policy_entropy     | 1.54     |
| policy_loss        | -0.00354 |
| total_timesteps    | 9064000  |
| value_loss         | 0.00122  |
---------------------------------
---------------------------------
| avg reward         | 1.29     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.969    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 113400   |
| policy_entropy     | 1.6      |
| policy_loss        | -0.0204  |
| total_timesteps    | 9072000  |
| value_loss         | 0.00161  |
---------------------------------
---------------------------------
| avg reward         | 0.953    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.977    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 113500   |
| policy_entropy     | 1.64     |
| policy_loss        | -0.0173  |
| total_timesteps    | 9080000  |
| value_loss         | 0.00232  |
---------------------------------
---------------------------------
| avg reward         | 1.06     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.973    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 113600   |
| policy_entropy     | 1.66     |
| policy_loss        | -0.0133  |
| total_timesteps    | 9088000  |
| value_loss         | 0.000764 |
---------------------------------
---------------------------------
| avg reward         | 1.28     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.972    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 113700   |
| policy_entropy     | 1.59     |
| policy_loss        | -0.00329 |
| total_timesteps    | 9096000  |
| value_loss         | 0.000917 |
---------------------------------
---------------------------------
| avg reward         | 1.33     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.952    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 113800   |
| policy_entropy     | 1.59     |
| policy_loss        | -0.0271  |
| total_timesteps    | 9104000  |
| value_loss         | 0.00243  |
---------------------------------
---------------------------------
| avg reward         | 0.929    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.942    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 113900   |
| policy_entropy     | 1.4      |
| policy_loss        | 0.0261   |
| total_timesteps    | 9112000  |
| value_loss         | 0.00683  |
---------------------------------
---------------------------------
| avg reward         | 1.07     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.956    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 114000   |
| policy_entropy     | 1.64     |
| policy_loss        | -0.0139  |
| total_timesteps    | 9120000  |
| value_loss         | 0.00557  |
---------------------------------
---------------------------------
| avg reward         | 1.12     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.973    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 114100   |
| policy_entropy     | 1.75     |
| policy_loss        | -0.0173  |
| total_timesteps    | 9128000  |
| value_loss         | 0.00222  |
---------------------------------
---------------------------------
| avg reward         | 1.1      |
| epsilonValue       | 0.000727 |
| explained_variance | 0.971    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 114200   |
| policy_entropy     | 1.65     |
| policy_loss        | -0.0226  |
| total_timesteps    | 9136000  |
| value_loss         | 0.00175  |
---------------------------------
---------------------------------
| avg reward         | 1.01     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.88     |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 114300   |
| policy_entropy     | 1.51     |
| policy_loss        | 0.0306   |
| total_timesteps    | 9144000  |
| value_loss         | 0.00889  |
---------------------------------
---------------------------------
| avg reward         | 1.04     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.975    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 114400   |
| policy_entropy     | 1.6      |
| policy_loss        | 0.00857  |
| total_timesteps    | 9152000  |
| value_loss         | 0.00178  |
---------------------------------
---------------------------------
| avg reward         | 1.24     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.983    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 114500   |
| policy_entropy     | 1.64     |
| policy_loss        | 0.00575  |
| total_timesteps    | 9160000  |
| value_loss         | 0.000583 |
---------------------------------
---------------------------------
| avg reward         | 1.34     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.977    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 114600   |
| policy_entropy     | 1.48     |
| policy_loss        | 0.00531  |
| total_timesteps    | 9168000  |
| value_loss         | 0.000722 |
---------------------------------
---------------------------------
| avg reward         | 0.872    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.987    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 114700   |
| policy_entropy     | 1.63     |
| policy_loss        | 0.0122   |
| total_timesteps    | 9176000  |
| value_loss         | 0.00166  |
---------------------------------
---------------------------------
| avg reward         | 1.32     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.964    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 114800   |
| policy_entropy     | 1.51     |
| policy_loss        | -0.0308  |
| total_timesteps    | 9184000  |
| value_loss         | 0.0016   |
---------------------------------
---------------------------------
| avg reward         | 0.813    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.979    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 114900   |
| policy_entropy     | 1.73     |
| policy_loss        | 0.0148   |
| total_timesteps    | 9192000  |
| value_loss         | 0.00293  |
---------------------------------
---------------------------------
| avg reward         | 1.09     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.884    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 115000   |
| policy_entropy     | 1.62     |
| policy_loss        | 0.0273   |
| total_timesteps    | 9200000  |
| value_loss         | 0.0122   |
---------------------------------
---------------------------------
| avg reward         | 1.02     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.975    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 115100   |
| policy_entropy     | 1.68     |
| policy_loss        | 0.00438  |
| total_timesteps    | 9208000  |
| value_loss         | 0.00117  |
---------------------------------
---------------------------------
| avg reward         | 1.27     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.988    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 115200   |
| policy_entropy     | 1.58     |
| policy_loss        | -0.0173  |
| total_timesteps    | 9216000  |
| value_loss         | 0.00056  |
---------------------------------
---------------------------------
| avg reward         | 1.01     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.979    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 115300   |
| policy_entropy     | 1.63     |
| policy_loss        | -0.0144  |
| total_timesteps    | 9224000  |
| value_loss         | 0.000965 |
---------------------------------
---------------------------------
| avg reward         | 1.04     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.944    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 115400   |
| policy_entropy     | 1.5      |
| policy_loss        | 0.00321  |
| total_timesteps    | 9232000  |
| value_loss         | 0.00413  |
---------------------------------
---------------------------------
| avg reward         | 0.873    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.976    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 115500   |
| policy_entropy     | 1.59     |
| policy_loss        | -0.0198  |
| total_timesteps    | 9240000  |
| value_loss         | 0.00226  |
---------------------------------
---------------------------------
| avg reward         | 0.839    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.93     |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 115600   |
| policy_entropy     | 1.71     |
| policy_loss        | -0.0236  |
| total_timesteps    | 9248000  |
| value_loss         | 0.0036   |
---------------------------------
---------------------------------
| avg reward         | 1.02     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.972    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 115700   |
| policy_entropy     | 1.45     |
| policy_loss        | -0.0506  |
| total_timesteps    | 9256000  |
| value_loss         | 0.00491  |
---------------------------------
---------------------------------
| avg reward         | 0.906    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.985    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 115800   |
| policy_entropy     | 1.71     |
| policy_loss        | 0.0176   |
| total_timesteps    | 9264000  |
| value_loss         | 0.00113  |
---------------------------------
---------------------------------
| avg reward         | 1.11     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.967    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 115900   |
| policy_entropy     | 1.62     |
| policy_loss        | -0.0126  |
| total_timesteps    | 9272000  |
| value_loss         | 0.00245  |
---------------------------------
---------------------------------
| avg reward         | 1.14     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.98     |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 116000   |
| policy_entropy     | 1.53     |
| policy_loss        | -0.00615 |
| total_timesteps    | 9280000  |
| value_loss         | 0.00168  |
---------------------------------
---------------------------------
| avg reward         | 0.965    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.988    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 116100   |
| policy_entropy     | 1.69     |
| policy_loss        | -0.0094  |
| total_timesteps    | 9288000  |
| value_loss         | 0.00125  |
---------------------------------
---------------------------------
| avg reward         | 1.08     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.976    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 116200   |
| policy_entropy     | 1.53     |
| policy_loss        | 0.0365   |
| total_timesteps    | 9296000  |
| value_loss         | 0.00128  |
---------------------------------
---------------------------------
| avg reward         | 1.11     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.975    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 116300   |
| policy_entropy     | 1.66     |
| policy_loss        | 0.00635  |
| total_timesteps    | 9304000  |
| value_loss         | 0.00171  |
---------------------------------
---------------------------------
| avg reward         | 1.19     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.985    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 116400   |
| policy_entropy     | 1.67     |
| policy_loss        | 0.00996  |
| total_timesteps    | 9312000  |
| value_loss         | 0.00107  |
---------------------------------
---------------------------------
| avg reward         | 1.28     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.939    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 116500   |
| policy_entropy     | 1.69     |
| policy_loss        | 0.0255   |
| total_timesteps    | 9320000  |
| value_loss         | 0.00359  |
---------------------------------
---------------------------------
| avg reward         | 1.26     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.982    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 116600   |
| policy_entropy     | 1.58     |
| policy_loss        | -0.00469 |
| total_timesteps    | 9328000  |
| value_loss         | 0.000391 |
---------------------------------
---------------------------------
| avg reward         | 1.07     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.974    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 116700   |
| policy_entropy     | 1.52     |
| policy_loss        | -0.0301  |
| total_timesteps    | 9336000  |
| value_loss         | 0.00321  |
---------------------------------
---------------------------------
| avg reward         | 1.02     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.995    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 116800   |
| policy_entropy     | 1.56     |
| policy_loss        | 0.0188   |
| total_timesteps    | 9344000  |
| value_loss         | 0.000668 |
---------------------------------
---------------------------------
| avg reward         | 1.07     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.984    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 116900   |
| policy_entropy     | 1.62     |
| policy_loss        | 0.015    |
| total_timesteps    | 9352000  |
| value_loss         | 0.00136  |
---------------------------------
---------------------------------
| avg reward         | 1.05     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.991    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 117000   |
| policy_entropy     | 1.55     |
| policy_loss        | -0.00936 |
| total_timesteps    | 9360000  |
| value_loss         | 0.000949 |
---------------------------------
---------------------------------
| avg reward         | 1        |
| epsilonValue       | 0.000727 |
| explained_variance | 0.979    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 117100   |
| policy_entropy     | 1.49     |
| policy_loss        | 0.00102  |
| total_timesteps    | 9368000  |
| value_loss         | 0.0027   |
---------------------------------
---------------------------------
| avg reward         | 1.04     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.989    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 117200   |
| policy_entropy     | 1.72     |
| policy_loss        | -0.0136  |
| total_timesteps    | 9376000  |
| value_loss         | 0.000787 |
---------------------------------
---------------------------------
| avg reward         | 1.3      |
| epsilonValue       | 0.000727 |
| explained_variance | 0.952    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 117300   |
| policy_entropy     | 1.36     |
| policy_loss        | 0.0127   |
| total_timesteps    | 9384000  |
| value_loss         | 0.00182  |
---------------------------------
---------------------------------
| avg reward         | 0.868    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.985    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 117400   |
| policy_entropy     | 1.55     |
| policy_loss        | 0.00699  |
| total_timesteps    | 9392000  |
| value_loss         | 0.00115  |
---------------------------------
---------------------------------
| avg reward         | 0.828    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.982    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 117500   |
| policy_entropy     | 1.58     |
| policy_loss        | -0.0102  |
| total_timesteps    | 9400000  |
| value_loss         | 0.00203  |
---------------------------------
---------------------------------
| avg reward         | 1.07     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.747    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 117600   |
| policy_entropy     | 1.66     |
| policy_loss        | -0.0634  |
| total_timesteps    | 9408000  |
| value_loss         | 0.0126   |
---------------------------------
---------------------------------
| avg reward         | 1.23     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.971    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 117700   |
| policy_entropy     | 1.56     |
| policy_loss        | -0.00893 |
| total_timesteps    | 9416000  |
| value_loss         | 0.00182  |
---------------------------------
---------------------------------
| avg reward         | 1.05     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.917    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 117800   |
| policy_entropy     | 1.52     |
| policy_loss        | 0.0288   |
| total_timesteps    | 9424000  |
| value_loss         | 0.0043   |
---------------------------------
---------------------------------
| avg reward         | 1.03     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.981    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 117900   |
| policy_entropy     | 1.57     |
| policy_loss        | -0.028   |
| total_timesteps    | 9432000  |
| value_loss         | 0.00202  |
---------------------------------
---------------------------------
| avg reward         | 1.09     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.989    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 118000   |
| policy_entropy     | 1.69     |
| policy_loss        | 0.014    |
| total_timesteps    | 9440000  |
| value_loss         | 0.000906 |
---------------------------------
---------------------------------
| avg reward         | 1.16     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.99     |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 118100   |
| policy_entropy     | 1.64     |
| policy_loss        | 0.00638  |
| total_timesteps    | 9448000  |
| value_loss         | 0.000409 |
---------------------------------
---------------------------------
| avg reward         | 1.04     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.968    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 118200   |
| policy_entropy     | 1.45     |
| policy_loss        | -0.0321  |
| total_timesteps    | 9456000  |
| value_loss         | 0.00428  |
---------------------------------
---------------------------------
| avg reward         | 1.02     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.985    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 118300   |
| policy_entropy     | 1.73     |
| policy_loss        | -0.00137 |
| total_timesteps    | 9464000  |
| value_loss         | 0.00128  |
---------------------------------
---------------------------------
| avg reward         | 1.05     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.992    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 118400   |
| policy_entropy     | 1.67     |
| policy_loss        | 0.00299  |
| total_timesteps    | 9472000  |
| value_loss         | 0.000663 |
---------------------------------
---------------------------------
| avg reward         | 1.3      |
| epsilonValue       | 0.000727 |
| explained_variance | 0.992    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 118500   |
| policy_entropy     | 1.62     |
| policy_loss        | 0.00247  |
| total_timesteps    | 9480000  |
| value_loss         | 0.000541 |
---------------------------------
---------------------------------
| avg reward         | 1.3      |
| epsilonValue       | 0.000727 |
| explained_variance | 0.916    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 118600   |
| policy_entropy     | 1.7      |
| policy_loss        | 0.00542  |
| total_timesteps    | 9488000  |
| value_loss         | 0.00375  |
---------------------------------
---------------------------------
| avg reward         | 1.18     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.985    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 118700   |
| policy_entropy     | 1.63     |
| policy_loss        | -0.0181  |
| total_timesteps    | 9496000  |
| value_loss         | 0.000709 |
---------------------------------
---------------------------------
| avg reward         | 1.1      |
| epsilonValue       | 0.000727 |
| explained_variance | 0.989    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 118800   |
| policy_entropy     | 1.52     |
| policy_loss        | 0.000483 |
| total_timesteps    | 9504000  |
| value_loss         | 0.000906 |
---------------------------------
---------------------------------
| avg reward         | 1.12     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.988    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 118900   |
| policy_entropy     | 1.47     |
| policy_loss        | -0.00515 |
| total_timesteps    | 9512000  |
| value_loss         | 0.00131  |
---------------------------------
---------------------------------
| avg reward         | 1.07     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.973    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 119000   |
| policy_entropy     | 1.75     |
| policy_loss        | 0.0186   |
| total_timesteps    | 9520000  |
| value_loss         | 0.00174  |
---------------------------------
---------------------------------
| avg reward         | 1        |
| epsilonValue       | 0.000727 |
| explained_variance | 0.983    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 119100   |
| policy_entropy     | 1.72     |
| policy_loss        | -0.00025 |
| total_timesteps    | 9528000  |
| value_loss         | 0.00133  |
---------------------------------
---------------------------------
| avg reward         | 1.18     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.986    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 119200   |
| policy_entropy     | 1.52     |
| policy_loss        | 0.0151   |
| total_timesteps    | 9536000  |
| value_loss         | 0.00109  |
---------------------------------
---------------------------------
| avg reward         | 1.1      |
| epsilonValue       | 0.000727 |
| explained_variance | 0.98     |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 119300   |
| policy_entropy     | 1.49     |
| policy_loss        | 0.0171   |
| total_timesteps    | 9544000  |
| value_loss         | 0.00117  |
---------------------------------
---------------------------------
| avg reward         | 1.15     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.979    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 119400   |
| policy_entropy     | 1.38     |
| policy_loss        | 0.0117   |
| total_timesteps    | 9552000  |
| value_loss         | 0.00132  |
---------------------------------
---------------------------------
| avg reward         | 1.18     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.993    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 119500   |
| policy_entropy     | 1.66     |
| policy_loss        | 0.00593  |
| total_timesteps    | 9560000  |
| value_loss         | 0.000472 |
---------------------------------
---------------------------------
| avg reward         | 1.1      |
| epsilonValue       | 0.000727 |
| explained_variance | 0.982    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 119600   |
| policy_entropy     | 1.66     |
| policy_loss        | 0.0375   |
| total_timesteps    | 9568000  |
| value_loss         | 0.00211  |
---------------------------------
---------------------------------
| avg reward         | 1.11     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.958    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 119700   |
| policy_entropy     | 1.62     |
| policy_loss        | 0.0109   |
| total_timesteps    | 9576000  |
| value_loss         | 0.0023   |
---------------------------------
---------------------------------
| avg reward         | 1.05     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.977    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 119800   |
| policy_entropy     | 1.62     |
| policy_loss        | -0.0124  |
| total_timesteps    | 9584000  |
| value_loss         | 0.00275  |
---------------------------------
---------------------------------
| avg reward         | 1.03     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.986    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 119900   |
| policy_entropy     | 1.5      |
| policy_loss        | -0.0186  |
| total_timesteps    | 9592000  |
| value_loss         | 0.00132  |
---------------------------------
---------------------------------
| avg reward         | 1.18     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.991    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 120000   |
| policy_entropy     | 1.52     |
| policy_loss        | 0.00544  |
| total_timesteps    | 9600000  |
| value_loss         | 0.000713 |
---------------------------------
---------------------------------
| avg reward         | 1.06     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.887    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 120100   |
| policy_entropy     | 1.67     |
| policy_loss        | -0.0256  |
| total_timesteps    | 9608000  |
| value_loss         | 0.0114   |
---------------------------------
---------------------------------
| avg reward         | 1.01     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.986    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 120200   |
| policy_entropy     | 1.71     |
| policy_loss        | -0.00958 |
| total_timesteps    | 9616000  |
| value_loss         | 0.000485 |
---------------------------------
---------------------------------
| avg reward         | 1.25     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.981    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 120300   |
| policy_entropy     | 1.51     |
| policy_loss        | 0.000454 |
| total_timesteps    | 9624000  |
| value_loss         | 0.00105  |
---------------------------------
---------------------------------
| avg reward         | 1.28     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.987    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 120400   |
| policy_entropy     | 1.61     |
| policy_loss        | 0.0121   |
| total_timesteps    | 9632000  |
| value_loss         | 0.00114  |
---------------------------------
---------------------------------
| avg reward         | 0.948    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.995    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 120500   |
| policy_entropy     | 1.74     |
| policy_loss        | 0.00288  |
| total_timesteps    | 9640000  |
| value_loss         | 0.000471 |
---------------------------------
---------------------------------
| avg reward         | 1.11     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.986    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 120600   |
| policy_entropy     | 1.67     |
| policy_loss        | 0.00125  |
| total_timesteps    | 9648000  |
| value_loss         | 0.00119  |
---------------------------------
---------------------------------
| avg reward         | 1.16     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.977    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 120700   |
| policy_entropy     | 1.54     |
| policy_loss        | 0.017    |
| total_timesteps    | 9656000  |
| value_loss         | 0.00164  |
---------------------------------
---------------------------------
| avg reward         | 1.16     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.878    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 120800   |
| policy_entropy     | 1.69     |
| policy_loss        | 0.0399   |
| total_timesteps    | 9664000  |
| value_loss         | 0.00429  |
---------------------------------
---------------------------------
| avg reward         | 1.13     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.993    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 120900   |
| policy_entropy     | 1.59     |
| policy_loss        | -0.00245 |
| total_timesteps    | 9672000  |
| value_loss         | 0.000559 |
---------------------------------
---------------------------------
| avg reward         | 1.02     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.992    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 121000   |
| policy_entropy     | 1.68     |
| policy_loss        | 0.00297  |
| total_timesteps    | 9680000  |
| value_loss         | 0.000723 |
---------------------------------
---------------------------------
| avg reward         | 1.09     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.991    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 121100   |
| policy_entropy     | 1.56     |
| policy_loss        | -0.0121  |
| total_timesteps    | 9688000  |
| value_loss         | 0.00064  |
---------------------------------
---------------------------------
| avg reward         | 1.03     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.975    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 121200   |
| policy_entropy     | 1.59     |
| policy_loss        | -0.019   |
| total_timesteps    | 9696000  |
| value_loss         | 0.0033   |
---------------------------------
---------------------------------
| avg reward         | 1.25     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.986    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 121300   |
| policy_entropy     | 1.6      |
| policy_loss        | -0.0168  |
| total_timesteps    | 9704000  |
| value_loss         | 0.000759 |
---------------------------------
---------------------------------
| avg reward         | 1.2      |
| epsilonValue       | 0.000727 |
| explained_variance | 0.94     |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 121400   |
| policy_entropy     | 1.62     |
| policy_loss        | -0.00622 |
| total_timesteps    | 9712000  |
| value_loss         | 0.00255  |
---------------------------------
---------------------------------
| avg reward         | 1.15     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.991    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 121500   |
| policy_entropy     | 1.62     |
| policy_loss        | -0.00645 |
| total_timesteps    | 9720000  |
| value_loss         | 0.000657 |
---------------------------------
---------------------------------
| avg reward         | 1.08     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.972    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 121600   |
| policy_entropy     | 1.65     |
| policy_loss        | 0.0365   |
| total_timesteps    | 9728000  |
| value_loss         | 0.00249  |
---------------------------------
---------------------------------
| avg reward         | 1.2      |
| epsilonValue       | 0.000727 |
| explained_variance | 0.965    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 121700   |
| policy_entropy     | 1.71     |
| policy_loss        | 0.0198   |
| total_timesteps    | 9736000  |
| value_loss         | 0.00191  |
---------------------------------
---------------------------------
| avg reward         | 1.31     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.984    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 121800   |
| policy_entropy     | 1.61     |
| policy_loss        | 0.00168  |
| total_timesteps    | 9744000  |
| value_loss         | 0.00119  |
---------------------------------
---------------------------------
| avg reward         | 1.04     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.943    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 121900   |
| policy_entropy     | 1.63     |
| policy_loss        | 0.0179   |
| total_timesteps    | 9752000  |
| value_loss         | 0.00313  |
---------------------------------
---------------------------------
| avg reward         | 1.07     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.995    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 122000   |
| policy_entropy     | 1.74     |
| policy_loss        | -0.0217  |
| total_timesteps    | 9760000  |
| value_loss         | 0.000456 |
---------------------------------
---------------------------------
| avg reward         | 1.03     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.869    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 122100   |
| policy_entropy     | 1.47     |
| policy_loss        | 0.00833  |
| total_timesteps    | 9768000  |
| value_loss         | 0.00445  |
---------------------------------
---------------------------------
| avg reward         | 1.17     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.963    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 122200   |
| policy_entropy     | 1.51     |
| policy_loss        | -0.00716 |
| total_timesteps    | 9776000  |
| value_loss         | 0.00231  |
---------------------------------
---------------------------------
| avg reward         | 1.11     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.991    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 122300   |
| policy_entropy     | 1.66     |
| policy_loss        | -0.00323 |
| total_timesteps    | 9784000  |
| value_loss         | 0.00105  |
---------------------------------
---------------------------------
| avg reward         | 1.06     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.974    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 122400   |
| policy_entropy     | 1.6      |
| policy_loss        | -0.0105  |
| total_timesteps    | 9792000  |
| value_loss         | 0.00152  |
---------------------------------
---------------------------------
| avg reward         | 1.06     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.993    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 122500   |
| policy_entropy     | 1.73     |
| policy_loss        | 0.00231  |
| total_timesteps    | 9800000  |
| value_loss         | 0.000542 |
---------------------------------
---------------------------------
| avg reward         | 1.07     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.973    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 122600   |
| policy_entropy     | 1.67     |
| policy_loss        | 0.00589  |
| total_timesteps    | 9808000  |
| value_loss         | 0.00176  |
---------------------------------
---------------------------------
| avg reward         | 1.15     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.993    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 122700   |
| policy_entropy     | 1.75     |
| policy_loss        | 0.0202   |
| total_timesteps    | 9816000  |
| value_loss         | 0.000517 |
---------------------------------
---------------------------------
| avg reward         | 1.12     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.976    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 122800   |
| policy_entropy     | 1.62     |
| policy_loss        | 0.0184   |
| total_timesteps    | 9824000  |
| value_loss         | 0.00134  |
---------------------------------
---------------------------------
| avg reward         | 1.22     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.979    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 122900   |
| policy_entropy     | 1.39     |
| policy_loss        | 0.00234  |
| total_timesteps    | 9832000  |
| value_loss         | 0.000857 |
---------------------------------
---------------------------------
| avg reward         | 0.882    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.969    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 123000   |
| policy_entropy     | 1.64     |
| policy_loss        | -0.0241  |
| total_timesteps    | 9840000  |
| value_loss         | 0.00295  |
---------------------------------
---------------------------------
| avg reward         | 1.02     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.987    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 123100   |
| policy_entropy     | 1.71     |
| policy_loss        | -0.0114  |
| total_timesteps    | 9848000  |
| value_loss         | 0.000742 |
---------------------------------
---------------------------------
| avg reward         | 1.15     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.944    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 123200   |
| policy_entropy     | 1.64     |
| policy_loss        | -0.0161  |
| total_timesteps    | 9856000  |
| value_loss         | 0.00225  |
---------------------------------
---------------------------------
| avg reward         | 1.16     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.983    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 123300   |
| policy_entropy     | 1.64     |
| policy_loss        | -0.0266  |
| total_timesteps    | 9864000  |
| value_loss         | 0.00178  |
---------------------------------
---------------------------------
| avg reward         | 1.19     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.987    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 123400   |
| policy_entropy     | 1.64     |
| policy_loss        | 0.0102   |
| total_timesteps    | 9872000  |
| value_loss         | 0.00103  |
---------------------------------
---------------------------------
| avg reward         | 1.19     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.988    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 123500   |
| policy_entropy     | 1.59     |
| policy_loss        | -0.00954 |
| total_timesteps    | 9880000  |
| value_loss         | 0.000827 |
---------------------------------
---------------------------------
| avg reward         | 1.01     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.984    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 123600   |
| policy_entropy     | 1.67     |
| policy_loss        | 0.00354  |
| total_timesteps    | 9888000  |
| value_loss         | 0.000763 |
---------------------------------
---------------------------------
| avg reward         | 0.922    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.895    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 123700   |
| policy_entropy     | 1.58     |
| policy_loss        | -0.0385  |
| total_timesteps    | 9896000  |
| value_loss         | 0.012    |
---------------------------------
---------------------------------
| avg reward         | 1.07     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.976    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 123800   |
| policy_entropy     | 1.68     |
| policy_loss        | 0.00748  |
| total_timesteps    | 9904000  |
| value_loss         | 0.000855 |
---------------------------------
---------------------------------
| avg reward         | 0.981    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.975    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 123900   |
| policy_entropy     | 1.47     |
| policy_loss        | -0.00913 |
| total_timesteps    | 9912000  |
| value_loss         | 0.00234  |
---------------------------------
---------------------------------
| avg reward         | 1.07     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.988    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 124000   |
| policy_entropy     | 1.63     |
| policy_loss        | -0.00034 |
| total_timesteps    | 9920000  |
| value_loss         | 0.000778 |
---------------------------------
---------------------------------
| avg reward         | 1.02     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.981    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 124100   |
| policy_entropy     | 1.75     |
| policy_loss        | -0.00882 |
| total_timesteps    | 9928000  |
| value_loss         | 0.00158  |
---------------------------------
----------------------------------
| avg reward         | 1.08      |
| epsilonValue       | 0.000727  |
| explained_variance | 0.99      |
| fps                | 2137      |
| learning rate      | 0.001     |
| nupdates           | 124200    |
| policy_entropy     | 1.67      |
| policy_loss        | -0.000487 |
| total_timesteps    | 9936000   |
| value_loss         | 0.000973  |
----------------------------------
---------------------------------
| avg reward         | 1.2      |
| epsilonValue       | 0.000727 |
| explained_variance | 0.973    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 124300   |
| policy_entropy     | 1.58     |
| policy_loss        | -0.0289  |
| total_timesteps    | 9944000  |
| value_loss         | 0.00264  |
---------------------------------
---------------------------------
| avg reward         | 0.961    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.945    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 124400   |
| policy_entropy     | 1.6      |
| policy_loss        | -0.0128  |
| total_timesteps    | 9952000  |
| value_loss         | 0.00668  |
---------------------------------
---------------------------------
| avg reward         | 1.11     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.853    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 124500   |
| policy_entropy     | 1.52     |
| policy_loss        | 0.0417   |
| total_timesteps    | 9960000  |
| value_loss         | 0.00507  |
---------------------------------
---------------------------------
| avg reward         | 1.37     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.983    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 124600   |
| policy_entropy     | 1.62     |
| policy_loss        | -0.00206 |
| total_timesteps    | 9968000  |
| value_loss         | 0.000733 |
---------------------------------
---------------------------------
| avg reward         | 1.11     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.985    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 124700   |
| policy_entropy     | 1.45     |
| policy_loss        | -0.00484 |
| total_timesteps    | 9976000  |
| value_loss         | 0.000797 |
---------------------------------
---------------------------------
| avg reward         | 1.06     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.988    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 124800   |
| policy_entropy     | 1.61     |
| policy_loss        | 0.00439  |
| total_timesteps    | 9984000  |
| value_loss         | 0.000726 |
---------------------------------
---------------------------------
| avg reward         | 1.13     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.995    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 124900   |
| policy_entropy     | 1.51     |
| policy_loss        | 0.00837  |
| total_timesteps    | 9992000  |
| value_loss         | 0.000432 |
---------------------------------
---------------------------------
| avg reward         | 1.16     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.753    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 125000   |
| policy_entropy     | 1.64     |
| policy_loss        | 0.0513   |
| total_timesteps    | 10000000 |
| value_loss         | 0.0147   |
---------------------------------
---------------------------------
| avg reward         | 1.08     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.989    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 125100   |
| policy_entropy     | 1.64     |
| policy_loss        | 0.00461  |
| total_timesteps    | 10008000 |
| value_loss         | 0.000603 |
---------------------------------
---------------------------------
| avg reward         | 0.967    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.963    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 125200   |
| policy_entropy     | 1.58     |
| policy_loss        | -0.0122  |
| total_timesteps    | 10016000 |
| value_loss         | 0.00295  |
---------------------------------
----------------------------------
| avg reward         | 1.24      |
| epsilonValue       | 0.000727  |
| explained_variance | 0.958     |
| fps                | 2137      |
| learning rate      | 0.001     |
| nupdates           | 125300    |
| policy_entropy     | 1.38      |
| policy_loss        | -0.000463 |
| total_timesteps    | 10024000  |
| value_loss         | 0.003     |
----------------------------------
---------------------------------
| avg reward         | 0.991    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.994    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 125400   |
| policy_entropy     | 1.77     |
| policy_loss        | 0.00683  |
| total_timesteps    | 10032000 |
| value_loss         | 0.000385 |
---------------------------------
---------------------------------
| avg reward         | 1.21     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.995    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 125500   |
| policy_entropy     | 1.64     |
| policy_loss        | -0.00753 |
| total_timesteps    | 10040000 |
| value_loss         | 0.00057  |
---------------------------------
---------------------------------
| avg reward         | 1.02     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.868    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 125600   |
| policy_entropy     | 1.59     |
| policy_loss        | -0.0148  |
| total_timesteps    | 10048000 |
| value_loss         | 0.00835  |
---------------------------------
---------------------------------
| avg reward         | 1.16     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.99     |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 125700   |
| policy_entropy     | 1.52     |
| policy_loss        | -0.00519 |
| total_timesteps    | 10056000 |
| value_loss         | 0.000679 |
---------------------------------
---------------------------------
| avg reward         | 1.14     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.979    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 125800   |
| policy_entropy     | 1.63     |
| policy_loss        | 0.0115   |
| total_timesteps    | 10064000 |
| value_loss         | 0.0012   |
---------------------------------
---------------------------------
| avg reward         | 1.15     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.988    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 125900   |
| policy_entropy     | 1.67     |
| policy_loss        | 0.0209   |
| total_timesteps    | 10072000 |
| value_loss         | 0.00101  |
---------------------------------
---------------------------------
| avg reward         | 1.23     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.987    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 126000   |
| policy_entropy     | 1.72     |
| policy_loss        | -0.00979 |
| total_timesteps    | 10080000 |
| value_loss         | 0.000461 |
---------------------------------
---------------------------------
| avg reward         | 1.28     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.985    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 126100   |
| policy_entropy     | 1.62     |
| policy_loss        | -0.00305 |
| total_timesteps    | 10088000 |
| value_loss         | 0.00116  |
---------------------------------
---------------------------------
| avg reward         | 1.12     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.975    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 126200   |
| policy_entropy     | 1.76     |
| policy_loss        | -0.00269 |
| total_timesteps    | 10096000 |
| value_loss         | 0.0022   |
---------------------------------
---------------------------------
| avg reward         | 1.09     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.993    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 126300   |
| policy_entropy     | 1.63     |
| policy_loss        | -0.0135  |
| total_timesteps    | 10104000 |
| value_loss         | 0.000588 |
---------------------------------
---------------------------------
| avg reward         | 1.16     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.975    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 126400   |
| policy_entropy     | 1.58     |
| policy_loss        | -0.0268  |
| total_timesteps    | 10112000 |
| value_loss         | 0.00165  |
---------------------------------
---------------------------------
| avg reward         | 1.14     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.989    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 126500   |
| policy_entropy     | 1.54     |
| policy_loss        | -0.00312 |
| total_timesteps    | 10120000 |
| value_loss         | 0.000701 |
---------------------------------
---------------------------------
| avg reward         | 1.08     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.972    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 126600   |
| policy_entropy     | 1.52     |
| policy_loss        | -0.0347  |
| total_timesteps    | 10128000 |
| value_loss         | 0.0026   |
---------------------------------
---------------------------------
| avg reward         | 1.04     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.972    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 126700   |
| policy_entropy     | 1.78     |
| policy_loss        | 0.0023   |
| total_timesteps    | 10136000 |
| value_loss         | 0.00173  |
---------------------------------
---------------------------------
| avg reward         | 1.16     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.989    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 126800   |
| policy_entropy     | 1.52     |
| policy_loss        | 0.00829  |
| total_timesteps    | 10144000 |
| value_loss         | 0.000659 |
---------------------------------
---------------------------------
| avg reward         | 1.03     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.99     |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 126900   |
| policy_entropy     | 1.62     |
| policy_loss        | -0.00659 |
| total_timesteps    | 10152000 |
| value_loss         | 0.000684 |
---------------------------------
---------------------------------
| avg reward         | 1.14     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.993    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 127000   |
| policy_entropy     | 1.4      |
| policy_loss        | 0.00921  |
| total_timesteps    | 10160000 |
| value_loss         | 0.00123  |
---------------------------------
----------------------------------
| avg reward         | 1.11      |
| epsilonValue       | 0.000727  |
| explained_variance | 0.978     |
| fps                | 2137      |
| learning rate      | 0.001     |
| nupdates           | 127100    |
| policy_entropy     | 1.68      |
| policy_loss        | -0.000676 |
| total_timesteps    | 10168000  |
| value_loss         | 0.000817  |
----------------------------------
---------------------------------
| avg reward         | 1.28     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.985    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 127200   |
| policy_entropy     | 1.59     |
| policy_loss        | 1.56e-05 |
| total_timesteps    | 10176000 |
| value_loss         | 0.000605 |
---------------------------------
---------------------------------
| avg reward         | 1.02     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.994    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 127300   |
| policy_entropy     | 1.77     |
| policy_loss        | -0.0124  |
| total_timesteps    | 10184000 |
| value_loss         | 0.000566 |
---------------------------------
---------------------------------
| avg reward         | 1.19     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.991    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 127400   |
| policy_entropy     | 1.55     |
| policy_loss        | -0.00325 |
| total_timesteps    | 10192000 |
| value_loss         | 0.000445 |
---------------------------------
---------------------------------
| avg reward         | 1.23     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.992    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 127500   |
| policy_entropy     | 1.54     |
| policy_loss        | -0.00587 |
| total_timesteps    | 10200000 |
| value_loss         | 0.000562 |
---------------------------------
---------------------------------
| avg reward         | 1.03     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.963    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 127600   |
| policy_entropy     | 1.61     |
| policy_loss        | 0.033    |
| total_timesteps    | 10208000 |
| value_loss         | 0.00361  |
---------------------------------
---------------------------------
| avg reward         | 1.12     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.99     |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 127700   |
| policy_entropy     | 1.68     |
| policy_loss        | -0.00325 |
| total_timesteps    | 10216000 |
| value_loss         | 0.000456 |
---------------------------------
---------------------------------
| avg reward         | 1.09     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.99     |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 127800   |
| policy_entropy     | 1.66     |
| policy_loss        | -0.0052  |
| total_timesteps    | 10224000 |
| value_loss         | 0.000533 |
---------------------------------
---------------------------------
| avg reward         | 1.2      |
| epsilonValue       | 0.000727 |
| explained_variance | 0.973    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 127900   |
| policy_entropy     | 1.55     |
| policy_loss        | 0.00346  |
| total_timesteps    | 10232000 |
| value_loss         | 0.00104  |
---------------------------------
---------------------------------
| avg reward         | 0.95     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.987    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 128000   |
| policy_entropy     | 1.58     |
| policy_loss        | -0.0216  |
| total_timesteps    | 10240000 |
| value_loss         | 0.000845 |
---------------------------------
---------------------------------
| avg reward         | 1.35     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.972    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 128100   |
| policy_entropy     | 1.41     |
| policy_loss        | 0.00678  |
| total_timesteps    | 10248000 |
| value_loss         | 0.000919 |
---------------------------------
---------------------------------
| avg reward         | 1.05     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.971    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 128200   |
| policy_entropy     | 1.59     |
| policy_loss        | 0.0211   |
| total_timesteps    | 10256000 |
| value_loss         | 0.0028   |
---------------------------------
---------------------------------
| avg reward         | 0.958    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.969    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 128300   |
| policy_entropy     | 1.61     |
| policy_loss        | -0.0265  |
| total_timesteps    | 10264000 |
| value_loss         | 0.0016   |
---------------------------------
---------------------------------
| avg reward         | 1.21     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.965    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 128400   |
| policy_entropy     | 1.53     |
| policy_loss        | 0.0116   |
| total_timesteps    | 10272000 |
| value_loss         | 0.00138  |
---------------------------------
---------------------------------
| avg reward         | 1.28     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.993    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 128500   |
| policy_entropy     | 1.69     |
| policy_loss        | -0.00529 |
| total_timesteps    | 10280000 |
| value_loss         | 0.000364 |
---------------------------------
---------------------------------
| avg reward         | 1.21     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.995    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 128600   |
| policy_entropy     | 1.71     |
| policy_loss        | 0.000511 |
| total_timesteps    | 10288000 |
| value_loss         | 0.000345 |
---------------------------------
---------------------------------
| avg reward         | 1.05     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.987    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 128700   |
| policy_entropy     | 1.51     |
| policy_loss        | -0.0257  |
| total_timesteps    | 10296000 |
| value_loss         | 0.00149  |
---------------------------------
---------------------------------
| avg reward         | 1.01     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.901    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 128800   |
| policy_entropy     | 1.55     |
| policy_loss        | 0.0104   |
| total_timesteps    | 10304000 |
| value_loss         | 0.00825  |
---------------------------------
---------------------------------
| avg reward         | 0.977    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.907    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 128900   |
| policy_entropy     | 1.66     |
| policy_loss        | -0.0375  |
| total_timesteps    | 10312000 |
| value_loss         | 0.0055   |
---------------------------------
---------------------------------
| avg reward         | 1.16     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.965    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 129000   |
| policy_entropy     | 1.58     |
| policy_loss        | 0.00838  |
| total_timesteps    | 10320000 |
| value_loss         | 0.00258  |
---------------------------------
---------------------------------
| avg reward         | 1.22     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.974    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 129100   |
| policy_entropy     | 1.62     |
| policy_loss        | 0.00393  |
| total_timesteps    | 10328000 |
| value_loss         | 0.00133  |
---------------------------------
---------------------------------
| avg reward         | 1.2      |
| epsilonValue       | 0.000727 |
| explained_variance | 0.989    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 129200   |
| policy_entropy     | 1.71     |
| policy_loss        | 0.00168  |
| total_timesteps    | 10336000 |
| value_loss         | 0.000675 |
---------------------------------
---------------------------------
| avg reward         | 1.18     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.981    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 129300   |
| policy_entropy     | 1.39     |
| policy_loss        | 0.0204   |
| total_timesteps    | 10344000 |
| value_loss         | 0.00123  |
---------------------------------
---------------------------------
| avg reward         | 0.955    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.906    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 129400   |
| policy_entropy     | 1.56     |
| policy_loss        | 0.0185   |
| total_timesteps    | 10352000 |
| value_loss         | 0.00776  |
---------------------------------
---------------------------------
| avg reward         | 1.06     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.986    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 129500   |
| policy_entropy     | 1.5      |
| policy_loss        | 0.00915  |
| total_timesteps    | 10360000 |
| value_loss         | 0.000781 |
---------------------------------
---------------------------------
| avg reward         | 1.08     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.984    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 129600   |
| policy_entropy     | 1.68     |
| policy_loss        | 0.000376 |
| total_timesteps    | 10368000 |
| value_loss         | 0.00125  |
---------------------------------
---------------------------------
| avg reward         | 1.19     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.971    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 129700   |
| policy_entropy     | 1.72     |
| policy_loss        | 0.0222   |
| total_timesteps    | 10376000 |
| value_loss         | 0.0025   |
---------------------------------
---------------------------------
| avg reward         | 1.21     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.98     |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 129800   |
| policy_entropy     | 1.68     |
| policy_loss        | 0.0103   |
| total_timesteps    | 10384000 |
| value_loss         | 0.00133  |
---------------------------------
---------------------------------
| avg reward         | 1.18     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.985    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 129900   |
| policy_entropy     | 1.51     |
| policy_loss        | 0.00163  |
| total_timesteps    | 10392000 |
| value_loss         | 0.000348 |
---------------------------------
----------------------------------
| avg reward         | 1.2       |
| epsilonValue       | 0.000727  |
| explained_variance | 0.984     |
| fps                | 2137      |
| learning rate      | 0.001     |
| nupdates           | 130000    |
| policy_entropy     | 1.52      |
| policy_loss        | -0.000657 |
| total_timesteps    | 10400000  |
| value_loss         | 0.00148   |
----------------------------------
---------------------------------
| avg reward         | 1.04     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.978    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 130100   |
| policy_entropy     | 1.52     |
| policy_loss        | 0.0371   |
| total_timesteps    | 10408000 |
| value_loss         | 0.00216  |
---------------------------------
---------------------------------
| avg reward         | 1.07     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.986    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 130200   |
| policy_entropy     | 1.56     |
| policy_loss        | 0.00369  |
| total_timesteps    | 10416000 |
| value_loss         | 0.000665 |
---------------------------------
----------------------------------
| avg reward         | 1.04      |
| epsilonValue       | 0.000727  |
| explained_variance | 0.997     |
| fps                | 2137      |
| learning rate      | 0.001     |
| nupdates           | 130300    |
| policy_entropy     | 1.52      |
| policy_loss        | -0.000405 |
| total_timesteps    | 10424000  |
| value_loss         | 0.000323  |
----------------------------------
---------------------------------
| avg reward         | 1.17     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.988    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 130400   |
| policy_entropy     | 1.73     |
| policy_loss        | -0.0185  |
| total_timesteps    | 10432000 |
| value_loss         | 0.000996 |
---------------------------------
---------------------------------
| avg reward         | 0.782    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.99     |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 130500   |
| policy_entropy     | 1.72     |
| policy_loss        | -0.0206  |
| total_timesteps    | 10440000 |
| value_loss         | 0.00151  |
---------------------------------
---------------------------------
| avg reward         | 1.31     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.943    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 130600   |
| policy_entropy     | 1.43     |
| policy_loss        | -0.0066  |
| total_timesteps    | 10448000 |
| value_loss         | 0.00181  |
---------------------------------
----------------------------------
| avg reward         | 1.12      |
| epsilonValue       | 0.000727  |
| explained_variance | 0.988     |
| fps                | 2137      |
| learning rate      | 0.001     |
| nupdates           | 130700    |
| policy_entropy     | 1.59      |
| policy_loss        | -0.000654 |
| total_timesteps    | 10456000  |
| value_loss         | 0.00123   |
----------------------------------
---------------------------------
| avg reward         | 1.09     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.988    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 130800   |
| policy_entropy     | 1.6      |
| policy_loss        | 0.0135   |
| total_timesteps    | 10464000 |
| value_loss         | 0.000842 |
---------------------------------
---------------------------------
| avg reward         | 1.04     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.984    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 130900   |
| policy_entropy     | 1.63     |
| policy_loss        | 0.00174  |
| total_timesteps    | 10472000 |
| value_loss         | 0.00105  |
---------------------------------
---------------------------------
| avg reward         | 1.28     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.987    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 131000   |
| policy_entropy     | 1.7      |
| policy_loss        | 0.0195   |
| total_timesteps    | 10480000 |
| value_loss         | 0.000804 |
---------------------------------
---------------------------------
| avg reward         | 1.14     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.994    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 131100   |
| policy_entropy     | 1.76     |
| policy_loss        | -0.00392 |
| total_timesteps    | 10488000 |
| value_loss         | 0.000506 |
---------------------------------
---------------------------------
| avg reward         | 1.13     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.992    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 131200   |
| policy_entropy     | 1.62     |
| policy_loss        | 0.0168   |
| total_timesteps    | 10496000 |
| value_loss         | 0.000656 |
---------------------------------
---------------------------------
| avg reward         | 1.31     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.983    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 131300   |
| policy_entropy     | 1.37     |
| policy_loss        | -0.00857 |
| total_timesteps    | 10504000 |
| value_loss         | 0.000775 |
---------------------------------
---------------------------------
| avg reward         | 1.15     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.985    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 131400   |
| policy_entropy     | 1.56     |
| policy_loss        | -0.0139  |
| total_timesteps    | 10512000 |
| value_loss         | 0.000822 |
---------------------------------
---------------------------------
| avg reward         | 1.17     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.987    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 131500   |
| policy_entropy     | 1.49     |
| policy_loss        | -0.00796 |
| total_timesteps    | 10520000 |
| value_loss         | 0.00087  |
---------------------------------
---------------------------------
| avg reward         | 0.98     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.972    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 131600   |
| policy_entropy     | 1.72     |
| policy_loss        | -0.015   |
| total_timesteps    | 10528000 |
| value_loss         | 0.00169  |
---------------------------------
---------------------------------
| avg reward         | 1.16     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.973    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 131700   |
| policy_entropy     | 1.34     |
| policy_loss        | -0.0069  |
| total_timesteps    | 10536000 |
| value_loss         | 0.0025   |
---------------------------------
---------------------------------
| avg reward         | 1.24     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.993    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 131800   |
| policy_entropy     | 1.72     |
| policy_loss        | 0.00214  |
| total_timesteps    | 10544000 |
| value_loss         | 0.000441 |
---------------------------------
---------------------------------
| avg reward         | 1.25     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.984    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 131900   |
| policy_entropy     | 1.53     |
| policy_loss        | 0.00145  |
| total_timesteps    | 10552000 |
| value_loss         | 0.00191  |
---------------------------------
---------------------------------
| avg reward         | 0.969    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.989    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 132000   |
| policy_entropy     | 1.68     |
| policy_loss        | 0.00862  |
| total_timesteps    | 10560000 |
| value_loss         | 0.000414 |
---------------------------------
---------------------------------
| avg reward         | 1.25     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.993    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 132100   |
| policy_entropy     | 1.69     |
| policy_loss        | -0.0029  |
| total_timesteps    | 10568000 |
| value_loss         | 0.000473 |
---------------------------------
---------------------------------
| avg reward         | 1.19     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.981    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 132200   |
| policy_entropy     | 1.73     |
| policy_loss        | -0.00741 |
| total_timesteps    | 10576000 |
| value_loss         | 0.00193  |
---------------------------------
---------------------------------
| avg reward         | 0.912    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.985    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 132300   |
| policy_entropy     | 1.75     |
| policy_loss        | 0.00589  |
| total_timesteps    | 10584000 |
| value_loss         | 0.00171  |
---------------------------------
---------------------------------
| avg reward         | 1.2      |
| epsilonValue       | 0.000727 |
| explained_variance | 0.984    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 132400   |
| policy_entropy     | 1.64     |
| policy_loss        | 0.00898  |
| total_timesteps    | 10592000 |
| value_loss         | 0.000825 |
---------------------------------
---------------------------------
| avg reward         | 1.21     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.985    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 132500   |
| policy_entropy     | 1.62     |
| policy_loss        | 0.0103   |
| total_timesteps    | 10600000 |
| value_loss         | 0.000693 |
---------------------------------
---------------------------------
| avg reward         | 1.08     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.993    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 132600   |
| policy_entropy     | 1.61     |
| policy_loss        | 0.00992  |
| total_timesteps    | 10608000 |
| value_loss         | 0.000512 |
---------------------------------
---------------------------------
| avg reward         | 1.1      |
| epsilonValue       | 0.000727 |
| explained_variance | 0.966    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 132700   |
| policy_entropy     | 1.55     |
| policy_loss        | -0.00657 |
| total_timesteps    | 10616000 |
| value_loss         | 0.00488  |
---------------------------------
---------------------------------
| avg reward         | 1.18     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.971    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 132800   |
| policy_entropy     | 1.55     |
| policy_loss        | -0.0041  |
| total_timesteps    | 10624000 |
| value_loss         | 0.00167  |
---------------------------------
---------------------------------
| avg reward         | 1.26     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.968    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 132900   |
| policy_entropy     | 1.5      |
| policy_loss        | -0.0151  |
| total_timesteps    | 10632000 |
| value_loss         | 0.00101  |
---------------------------------
---------------------------------
| avg reward         | 1.09     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.99     |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 133000   |
| policy_entropy     | 1.7      |
| policy_loss        | -0.0239  |
| total_timesteps    | 10640000 |
| value_loss         | 0.00112  |
---------------------------------
---------------------------------
| avg reward         | 1.16     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.936    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 133100   |
| policy_entropy     | 1.61     |
| policy_loss        | -0.0449  |
| total_timesteps    | 10648000 |
| value_loss         | 0.00536  |
---------------------------------
---------------------------------
| avg reward         | 1.12     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.979    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 133200   |
| policy_entropy     | 1.72     |
| policy_loss        | 0.0112   |
| total_timesteps    | 10656000 |
| value_loss         | 0.00102  |
---------------------------------
---------------------------------
| avg reward         | 1.17     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.979    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 133300   |
| policy_entropy     | 1.61     |
| policy_loss        | 0.00146  |
| total_timesteps    | 10664000 |
| value_loss         | 0.00164  |
---------------------------------
---------------------------------
| avg reward         | 1.13     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.988    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 133400   |
| policy_entropy     | 1.66     |
| policy_loss        | -0.0126  |
| total_timesteps    | 10672000 |
| value_loss         | 0.000633 |
---------------------------------
---------------------------------
| avg reward         | 1.17     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.964    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 133500   |
| policy_entropy     | 1.57     |
| policy_loss        | 0.015    |
| total_timesteps    | 10680000 |
| value_loss         | 0.00203  |
---------------------------------
---------------------------------
| avg reward         | 1.04     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.953    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 133600   |
| policy_entropy     | 1.46     |
| policy_loss        | -0.0119  |
| total_timesteps    | 10688000 |
| value_loss         | 0.00581  |
---------------------------------
---------------------------------
| avg reward         | 1.06     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.992    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 133700   |
| policy_entropy     | 1.65     |
| policy_loss        | 0.00102  |
| total_timesteps    | 10696000 |
| value_loss         | 0.000595 |
---------------------------------
---------------------------------
| avg reward         | 1.13     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.986    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 133800   |
| policy_entropy     | 1.54     |
| policy_loss        | 0.00759  |
| total_timesteps    | 10704000 |
| value_loss         | 0.000546 |
---------------------------------
---------------------------------
| avg reward         | 1.05     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.988    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 133900   |
| policy_entropy     | 1.64     |
| policy_loss        | 0.0188   |
| total_timesteps    | 10712000 |
| value_loss         | 0.000928 |
---------------------------------
---------------------------------
| avg reward         | 1.19     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.976    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 134000   |
| policy_entropy     | 1.46     |
| policy_loss        | -0.00561 |
| total_timesteps    | 10720000 |
| value_loss         | 0.0012   |
---------------------------------
---------------------------------
| avg reward         | 0.98     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.99     |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 134100   |
| policy_entropy     | 1.56     |
| policy_loss        | 0.00552  |
| total_timesteps    | 10728000 |
| value_loss         | 0.00097  |
---------------------------------
---------------------------------
| avg reward         | 1.02     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.917    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 134200   |
| policy_entropy     | 1.59     |
| policy_loss        | -0.0195  |
| total_timesteps    | 10736000 |
| value_loss         | 0.00666  |
---------------------------------
---------------------------------
| avg reward         | 1.28     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.977    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 134300   |
| policy_entropy     | 1.61     |
| policy_loss        | -0.00388 |
| total_timesteps    | 10744000 |
| value_loss         | 0.000876 |
---------------------------------
---------------------------------
| avg reward         | 0.927    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.913    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 134400   |
| policy_entropy     | 1.44     |
| policy_loss        | -0.00286 |
| total_timesteps    | 10752000 |
| value_loss         | 0.00695  |
---------------------------------
---------------------------------
| avg reward         | 1.04     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.912    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 134500   |
| policy_entropy     | 1.57     |
| policy_loss        | 0.00241  |
| total_timesteps    | 10760000 |
| value_loss         | 0.00136  |
---------------------------------
---------------------------------
| avg reward         | 1.18     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.865    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 134600   |
| policy_entropy     | 1.52     |
| policy_loss        | -0.013   |
| total_timesteps    | 10768000 |
| value_loss         | 0.0103   |
---------------------------------
---------------------------------
| avg reward         | 1.05     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.876    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 134700   |
| policy_entropy     | 1.69     |
| policy_loss        | 0.0496   |
| total_timesteps    | 10776000 |
| value_loss         | 0.00735  |
---------------------------------
---------------------------------
| avg reward         | 1.04     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.983    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 134800   |
| policy_entropy     | 1.59     |
| policy_loss        | 0.00896  |
| total_timesteps    | 10784000 |
| value_loss         | 0.00103  |
---------------------------------
---------------------------------
| avg reward         | 1.17     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.952    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 134900   |
| policy_entropy     | 1.65     |
| policy_loss        | -0.00244 |
| total_timesteps    | 10792000 |
| value_loss         | 0.00296  |
---------------------------------
---------------------------------
| avg reward         | 1.16     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.975    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 135000   |
| policy_entropy     | 1.48     |
| policy_loss        | 0.0146   |
| total_timesteps    | 10800000 |
| value_loss         | 0.00173  |
---------------------------------
----------------------------------
| avg reward         | 1.13      |
| epsilonValue       | 0.000727  |
| explained_variance | 0.99      |
| fps                | 2137      |
| learning rate      | 0.001     |
| nupdates           | 135100    |
| policy_entropy     | 1.54      |
| policy_loss        | -0.000175 |
| total_timesteps    | 10808000  |
| value_loss         | 0.000749  |
----------------------------------
---------------------------------
| avg reward         | 1.09     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.989    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 135200   |
| policy_entropy     | 1.56     |
| policy_loss        | 0.00469  |
| total_timesteps    | 10816000 |
| value_loss         | 0.000824 |
---------------------------------
----------------------------------
| avg reward         | 1.11      |
| epsilonValue       | 0.000727  |
| explained_variance | 0.926     |
| fps                | 2137      |
| learning rate      | 0.001     |
| nupdates           | 135300    |
| policy_entropy     | 1.66      |
| policy_loss        | -0.000151 |
| total_timesteps    | 10824000  |
| value_loss         | 0.00642   |
----------------------------------
---------------------------------
| avg reward         | 0.916    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.941    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 135400   |
| policy_entropy     | 1.7      |
| policy_loss        | 0.00482  |
| total_timesteps    | 10832000 |
| value_loss         | 0.00698  |
---------------------------------
---------------------------------
| avg reward         | 1.06     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.989    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 135500   |
| policy_entropy     | 1.65     |
| policy_loss        | 0.00678  |
| total_timesteps    | 10840000 |
| value_loss         | 0.000541 |
---------------------------------
---------------------------------
| avg reward         | 1.24     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.993    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 135600   |
| policy_entropy     | 1.64     |
| policy_loss        | 0.0128   |
| total_timesteps    | 10848000 |
| value_loss         | 0.000516 |
---------------------------------
---------------------------------
| avg reward         | 1.17     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.981    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 135700   |
| policy_entropy     | 1.62     |
| policy_loss        | -0.00341 |
| total_timesteps    | 10856000 |
| value_loss         | 0.00206  |
---------------------------------
---------------------------------
| avg reward         | 1.08     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.987    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 135800   |
| policy_entropy     | 1.76     |
| policy_loss        | 0.0149   |
| total_timesteps    | 10864000 |
| value_loss         | 0.000541 |
---------------------------------
---------------------------------
| avg reward         | 1.07     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.978    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 135900   |
| policy_entropy     | 1.62     |
| policy_loss        | -0.0103  |
| total_timesteps    | 10872000 |
| value_loss         | 0.000993 |
---------------------------------
---------------------------------
| avg reward         | 1.18     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.989    |
| fps                | 2138     |
| learning rate      | 0.001    |
| nupdates           | 136000   |
| policy_entropy     | 1.48     |
| policy_loss        | -0.00552 |
| total_timesteps    | 10880000 |
| value_loss         | 0.00093  |
---------------------------------
---------------------------------
| avg reward         | 0.924    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.977    |
| fps                | 2138     |
| learning rate      | 0.001    |
| nupdates           | 136100   |
| policy_entropy     | 1.66     |
| policy_loss        | 0.0187   |
| total_timesteps    | 10888000 |
| value_loss         | 0.00241  |
---------------------------------
---------------------------------
| avg reward         | 0.982    |
| epsilonValue       | 0.000727 |
| explained_variance | 0.975    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 136200   |
| policy_entropy     | 1.56     |
| policy_loss        | -0.00953 |
| total_timesteps    | 10896000 |
| value_loss         | 0.00126  |
---------------------------------
---------------------------------
| avg reward         | 1.18     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.989    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 136300   |
| policy_entropy     | 1.67     |
| policy_loss        | 0.0242   |
| total_timesteps    | 10904000 |
| value_loss         | 0.000651 |
---------------------------------
---------------------------------
| avg reward         | 1.17     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.989    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 136400   |
| policy_entropy     | 1.62     |
| policy_loss        | 0.00391  |
| total_timesteps    | 10912000 |
| value_loss         | 0.000913 |
---------------------------------
---------------------------------
| avg reward         | 1.14     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.983    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 136500   |
| policy_entropy     | 1.59     |
| policy_loss        | 0.00322  |
| total_timesteps    | 10920000 |
| value_loss         | 0.00127  |
---------------------------------
---------------------------------
| avg reward         | 1.17     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.985    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 136600   |
| policy_entropy     | 1.6      |
| policy_loss        | 0.0118   |
| total_timesteps    | 10928000 |
| value_loss         | 0.000988 |
---------------------------------
---------------------------------
| avg reward         | 1.07     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.943    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 136700   |
| policy_entropy     | 1.53     |
| policy_loss        | -0.042   |
| total_timesteps    | 10936000 |
| value_loss         | 0.00764  |
---------------------------------
---------------------------------
| avg reward         | 1.16     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.961    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 136800   |
| policy_entropy     | 1.46     |
| policy_loss        | -0.0204  |
| total_timesteps    | 10944000 |
| value_loss         | 0.00215  |
---------------------------------
---------------------------------
| avg reward         | 1.03     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.979    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 136900   |
| policy_entropy     | 1.61     |
| policy_loss        | 0.0278   |
| total_timesteps    | 10952000 |
| value_loss         | 0.00215  |
---------------------------------
---------------------------------
| avg reward         | 1.07     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.915    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 137000   |
| policy_entropy     | 1.72     |
| policy_loss        | 0.0117   |
| total_timesteps    | 10960000 |
| value_loss         | 0.00533  |
---------------------------------
---------------------------------
| avg reward         | 1.19     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.988    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 137100   |
| policy_entropy     | 1.32     |
| policy_loss        | -0.0103  |
| total_timesteps    | 10968000 |
| value_loss         | 0.00081  |
---------------------------------
---------------------------------
| avg reward         | 1.03     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.815    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 137200   |
| policy_entropy     | 1.61     |
| policy_loss        | 0.00486  |
| total_timesteps    | 10976000 |
| value_loss         | 0.015    |
---------------------------------
---------------------------------
| avg reward         | 1.19     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.988    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 137300   |
| policy_entropy     | 1.56     |
| policy_loss        | -0.0161  |
| total_timesteps    | 10984000 |
| value_loss         | 0.000737 |
---------------------------------
---------------------------------
| avg reward         | 1.07     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.992    |
| fps                | 2138     |
| learning rate      | 0.001    |
| nupdates           | 137400   |
| policy_entropy     | 1.68     |
| policy_loss        | 0.00941  |
| total_timesteps    | 10992000 |
| value_loss         | 0.000695 |
---------------------------------
---------------------------------
| avg reward         | 1.01     |
| epsilonValue       | 0.000727 |
| explained_variance | 0.958    |
| fps                | 2137     |
| learning rate      | 0.001    |
| nupdates           | 137500   |
| policy_entropy     | 1.58     |
| policy_loss        | 0.00763  |
| total_timesteps    | 11000000 |
| value_loss         | 0.00264  |
---------------------------------
